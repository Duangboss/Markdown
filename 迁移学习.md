# Driving Tasks Transfer Using Deep Reinforcement Learning for Decision-Making of Autonomous Vehicles in Unsignalized Intersection

# 基于深度强化学习的无信号交叉口自动驾驶车辆决策任务迁移

[toc]

## 摘要

- 知识迁移是实现自动驾驶车辆实时决策的一个有前景的概念。本文构建了一个迁移深度强化学习（RL）框架，用于转化交叉路口环境中的驾驶任务。在无信号灯交叉路口的驾驶任务被划分为自动驾驶车辆的左转、右转和直行。自动驾驶主车（AEV）的目标是高效、安全地通过交叉路口情况。这一目标促使研究中的车辆提高速度并避免与其他车辆相撞。从一个驾驶任务中学习到的决策策略通过三个迁移规则转移到另一个驾驶任务中并进行评估。仿真结果显示，与相似任务相关的决策策略是可迁移的，并且具有很高的成功率。这表明所提出的控制框架可以减少时间消耗并实现在线实施。因此，迁移强化学习概念对于建立自动驾驶车辆的实时决策策略是有帮助的。
- 关键词：迁移学习，深度强化学习，驾驶任务，决策，自动驾驶汽车，无信号交叉口。
- 期刊：IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY(IEEE TVT). SCIE Q1. peer-reviewed
- 作者：
  1. [舒红 重庆大学 副教授](https://faculty.cqu.edu.cn/HongShu/zh_CN/index/231307/list/)
  2. [刘腾飞 东北大学 教授 IEEE自动控制汇刊编委 PhD of ANU](http://www.ise.neu.edu.cn/2024/0603/c213a256317/pagem.htm)
  3. [曹东璞 清华大学 教授](https://cmce.szu.edu.cn/info/1607/4341.htm)



***

## Dueling Deep Q_Learning ALgorithm

- Dueling DQN 是一种强化学习算法，是对DQN的改进。Dueling DQN 的核心思想是将 **状态价值函数** 和 **动作优势函数** 分离，从而更好地估计每个动作的 Q 值。

- 传统 DQN 的不足：

  - 在传统的 DQN 中，Q 网络直接输出Q 值Q Q(s, a) $。但是，这种方法在某些情况下效率较低，尤其是在某些状态下，动作的选择对最终的奖励影响不大。例如，在视频游戏中，如果角色处于一个安全的角落，执行什么动作可能没有显著的区别，但 DQN 仍然需要逐个计算每个动作的 Q 值，这样的估计可能会导致训练缓慢。

- Dueling DQN 的核心思想是将 Q 值函数分解为两个部分：

  - **状态价值函数** $ V(s) $：表示在给定状态下，不考虑具体动作的情况下，该状态本身的“好坏”。
  - **动作优势函数** $ A(s, a) $：表示在给定状态下，选择某个特定动作相对于其他动作的优势。

  - 通过这种分解，网络可以更有效地评估每个动作的价值，尤其是在那些大部分动作没有显著效果的状态下，状态价值函数可以帮助更快地确定该状态的重要性。

- 公式表示
  $Q(s, a) = V(s) + \left( A(s, a) - \frac{1}{|\mathcal{A}|} \sum_{a' \in \mathcal{A}} A(s, a') \right)$

  - $Q(s, a)$：表示给定状态 $ s $ 和动作 $ a $ 的 Q 值。
  - $ V(s) $：状态 $ s $ 的价值函数，表示该状态本身的“好坏”。
  - $ A(s, a) $：动作 $ a $ 在状态 $ s $ 中的优势函数，表示在状态 $ s $ 下选择动作 $ a $ 相对于其他动作的优势。
  - $ \mathcal{A} $：动作空间的大小。
  - 减去 $ \frac{1}{|\mathcal{A}|} \sum_{a' \in \mathcal{A}} A(s, a') $ 是为了保证动作优势函数的均值为零，防止 $ A(s, a) $ 和 $ V(s) $ 的值互相偏移。

- Dueling DQN 网络结构

  - 输出层被设计为两个分支：
    - **状态价值分支**：输出状态价值 $ V(s) $。
    - **动作优势分支**：输出每个动作的优势值 $ A(s, a) $。

  - 这两个分支最终结合，计算得到 Q 值 $ Q(s, a) $。这种分离的结构允许网络在某些情况下更快地学习状态的价值，而不必完全依赖于动作的选择。

- Dueling DQN 的好处

  1. **更快的学习效率**：在一些状态下，动作选择对总奖励的影响不大，传统 DQN 需要逐个计算 Q 值，而 Dueling DQN 可以通过状态价值函数来快速确定状态的好坏。
  2. **更好的性能**：由于 Dueling DQN 的网络结构能够更好地学习状态与动作的区别，理论上它能够在复杂的环境中表现得更好，尤其是那些对动作不敏感的状态。
  3. **稳定性提升**：通过分离状态价值和动作优势，Dueling DQN 能够更稳定地学习估计 Q 值，从而减少训练过程中的不稳定性。


***

## 迁移学习

- 在RL中智能体对控制动作的选择是一个不断试错的过程。这总是很耗时的。对于不同的研究问题，训练过程需要重复。此外，深度RL方法的性能非常依赖于超 参数的设置。如何提高训练效率，保证控制效果，仍然是深度RL研究面临的挑战。
- 回顾之前见到的知识蒸馏
  ![](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/SQTEXZSX.png)
  - 指将一个较大的、复杂的模型（“教师模型”）的知识转移给一个较小、更简单的模型（“学生模型”），以便学生模型可以在相同或更快的速度内达到类似的性能。
  - 软目标知识蒸馏是指将教师模型的概率分布作为学生模型的目标输出，并使用温度参数 T（通常为 1 或小于 1）来调整概率分布的形状。
  - 硬目标知识蒸馏是指将真实标签作为学生模型的目标输出。这种方法可以帮助学生模型更快地收敛，并产生更具针对性的结果。

- 迁移学习是将从一个任务（源任务）中学到的知识应用到另一个相关任务（目标任务）中，以提高新任务的学习效率和性能。源任务中的网络被称为专家网络，目标任务中的网络称为学生网络
- 比较
  - 目标：
    - 知识蒸馏：主要用于模型压缩和效率提升。
    - 迁移学习：主要用于提高新任务的学习效率和性能。
  - 知识来源：
    - 知识蒸馏：从同一任务的大模型中获取知识。
    - 迁移学习：从相关但可能不同的任务中获取知识。
  - 模型结构：
    - 知识蒸馏：通常学生模型结构不同于教师模型。
    - 迁移学习：通常保持基础结构，可能修改输出层。
  - 应用场景：
    - 知识蒸馏：适用于需要轻量化模型的场景。
    - 迁移学习：适用于数据有限或需要快速适应新任务的场景。

- 在本文中决策问题的目标是在不发生碰撞的情况下尽可能快地行驶。相关的任务是在无信号路口左转、直行、右转。因此，从一个源任务训练的控制策略可以应用于另一个目标任务
- 在本文中对于迁移学习使用方式为：改变控制动作选择的贪婪策略。定义了三个规则来确定目标任务中的控制动作：
  ![image-20240924114718230](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240924114718230.png)
  ![image-20240924173929864](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240924173929864.png)
  - 在概率为p1的情况下，选择专家网络建议的动作：
    $p_1=\beta_0\left(1-t/T_{tran}\right)$
    - $T_{tran}$是转移周期，在此期间，目标网络中的代理受到专家网络的影响。$T_{exp}$是目标任务中的初始探索期。

  - 在概率为 p2 的情况下，随机选择动作：
    $p_2=\epsilon (1-p_1)$
    - $\epsilon=\begin{cases}\epsilon_{start}-\frac{\epsilon_{start}-\epsilon_{end}}{0-T}t,t\leq T\\\epsilon_{end},t>T\end{cases}$

  - 在概率为 p3 的情况下，从学生网络中选择最佳控制动作：
    $p_3=(1-\epsilon)(1-p_1)$


***

## 仿真环境设置

- 无信号四路交叉口
  ![image-20240924164237736](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240924164237736.png)

  - 当AEV接近交叉口时，它必须决定是继续沿着计划路线行驶还是在交叉口前停车

  - 在无信号交叉路口，车辆是否应该转弯或其他车辆是否有优先权取决于通行权。优先控制交叉口和右侧优先交叉口是常见的无信号交叉口类型。

    - 优先控制交叉口：

      - 定义：在优先控制交叉口，一部分道路上的车辆具有优先通行权，而另一部分道路上的车辆必须让行。这种交叉口通常通过路标或路面标线来标明哪条道路是主路（优先通行权）以及哪条是次路（需要让行）。
      - 规则：
        - 主路上的车辆具有优先通行权，它们可以不减速或停车直接通过。
        - 次路上的车辆必须让行，通常在进入交叉路口前需要减速或停车，等待主路上的车辆通过后才能进入路口。
      - 应用：优先控制交叉口在交通流量较大的主干道与次级道路交汇处较为常见，通过明确主次道路关系来提高交叉口的通行效率。

    - 右侧优先交叉口：

      - 定义：在右侧优先交叉口，没有明确的主次道路标识，车辆的通行顺序完全取决于右侧优先规则，即来自右侧的车辆享有优先通行权。

      - 规则：
        - 如果一辆车的右侧有其它车辆接近交叉口，则该车辆必须让行，等待右侧车辆先行通过。
        - 如果右侧没有车，车辆可以直接通过交叉口。

      - 应用：这种规则在一些交通流量较小的地区或乡村道路上较为常见，因为交通流量低，右侧优先可以有效避免在无信号灯的情况下的冲突。

  - 本文选择优先控制交叉口作为交叉口类型，考虑水平道路为主干道，垂直道路为次干道。在优先控制的交叉路口，次要道路的司机必须给主要道路的司机让路。

- 状态空间
  $S=\{ S_a,S_1,S_2, ...,S_n\}\\
  S_i=\{s_x^i,s_y^i,v_x^i,v_y^i,\}$

  - 

- 行为控制器

  - 采用双层框架来控制周围车辆和AEV的行为，上框架（深度强化学习）管理纵向行为，下框架（低层控制器）管理横向行为
  - 横向控制器是一个简单的比例微分控制器，结合一些非线性。横向控制器由位置控制和航向控制两部分组成。
    - 航向控制：
      $\begin{aligned}&\psi_{\mathrm{t}}=\psi_{L}+\Delta\psi_{t}\\&\dot{\psi}_{\mathrm{t}}=K_{p,\psi}(\psi_{t}-\psi)\\&\delta=\arcsin\left(\frac{1}{2}\frac{l}{v}\dot{\psi}_{t}\right)\end{aligned}$
      - $\psi_{t}$​：车辆当前的目标航向角；$\psi_{L}$​表示车道的航向角；$\Delta\psi_{t}$：表示车辆当前的航向偏差；
      - $\psi$是车辆当前的实际航向角；$\dot{\psi}_{t}$：这是目标航向角的变化速率，它由航向角误差（$\psi_{t} - \psi$）乘以比例增益系数 $K_{p,\psi}$ 得到。$K_{p,\psi}$ 是航向控制的比例增益，决定了航向角误差的修正速度。
      - $\delta$：车辆的转向角（前轮的转向角），由目标航向角的变化速率 $\dot{\psi}_t$ 、车辆的前进速度 $v$和车辆的长度$l$得到
    - 位置控制：
      $v_{l}= - K_{p,l}\Delta d_{l}\\\Delta\psi_{t}=\arcsin\left(\frac{v_{l}}{v}\right)$
      - $v_l$：这是车辆所需的横向速度，表示车辆在车道中的横向移动。通过车辆与车道中心线的偏差 $\Delta d_l$ 乘以比例增益系数 $K_{p,l}$ 得出的。$K_{p,l}$ 是位置控制的比例增益，决定了车辆横向位置偏差的修正速度。
      - $\Delta\psi_t$：为航向角修正量。它表示由于横向位置修正所需的航向角变化。这个修正量会加到航向控制部分的 $\psi_t$ 中，最终用于控制车辆的实际转向角 $\delta$​。
  - 周围车辆纵向控制器，使用智能驾驶员模型，这有助于生成道路状况的真实特征。
    - 跟车距离$d_d$
      $d_d=d_0+T\cdot v+\frac{v\cdot\Delta v}{2\sqrt{a_{\max}\cdot b}_{\max}}$
      - 其中$d_0$是与前车之间的最小距离，T 是期望的时间间隔。根据舒适目的，$a_{max}$是最大加速度，$b_{max}$是最大减速度。$\Delta v$​是与前车的相对速度
    - 车辆加速度a
      $a=a_{\max}\cdot\left(1-\left(\frac{v}{v_d}\right)^\lambda-\left(\frac{d_d}{d}\right)^2\right)$
      - 其中$v_d$是期望速度，λ是速度项的常数指数。d 是与前车之间的距离
  - 本车纵向控制器：动作的选择由深度强化学习决定，只影响 AEV 的纵向加速度
    $a_t\in[-5,0,5]\text{m/s}^2$

- 奖励函数
  $r=\begin{cases}1\cdot highest\_speed-5\cdot collision,&\text{not reach}\\1 ,&\text{reach the ending point}\end{cases}$​

  - 如果AEV发生碰撞，则 collision = 1，否则为0。同 样，如果AEV的速度最高，则highest\_speed= 1，否则为0。

***

## 实验结果

- DQN、Dueling DQN 源任务的训练
  ![image-20240927153926758](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240927153926758.png)

  - 在左转和直行的驾驶任务中，DQN的奖励具有波动的轨迹。这意味着AEV很难很好地了解驾驶环境并选择适当的动作。
  - Dueling DQN在前两个任务中有明显的上升趋势，可以获得更大的奖励。相关决策策略优于DQN
  - 此外， 在右转的情况下，这两种技术能够实现相同的回报。 这是因为右转任务相对简单。周围的车辆通常不会阻 挡AEV，因此碰撞可以很容易地避免。

- 迁移学习评估

  - 将左转、直行和右转三个驾驶任务分别索引为场景1、场景2和场景3

  - 源任务设置为左转和直行

  - 对比同一源任务对不同目标任务进行迁移学习的效果

  - 奖励对比
    ![image-20240927161052095](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240927161052095.png)

    - 当**源任务**和**目标任务**相同时，迁移学习带来的奖励较高，控制性能也较好。
    - 当源任务与目标任务不同，奖励会有所下降，但迁移学习仍然能够提供一定的帮助。
    - **右转任务**相对简单，因为右转时的优先权较高，因此即使在不同源任务之间进行迁移，右转任务的奖励也始终较高。

  - 成功率对比
    ![image-20240927161308881](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240927161308881.png)

    - 成功率指的是在没有发生碰撞的情况下，自动驾驶车辆成功完成任务的比例。

    - 当源任务和目标任务相同时，成功率通常较高，接近或达到100%。
    - 当源任务和目标任务不同时，成功率会有所下降，但即使在这种情况下，迁移学习依然可以保持较高的成功率（通常高于70%）。
    - Dueling DQL与迁移学习结合后，成功率显著高于DQN

- 迁移学习和非迁移学习对比训练

  - 源任务和目标任务设置为左转

    ![image-20240927155947290](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240927155947290.png)![image-20240927155921028](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240927155921028.png)

    - 加入迁移学习后，模型在训练过程中能够更好地了解驾驶环境，进而使生成的决策策略更适合于交通情况，实现更高的回报。
    - 加入迁移学习后，误差的值低于单独的Dueling DQN，表明前者评估的网络更接近实际网络，提高学习速率并保证控制性能。
