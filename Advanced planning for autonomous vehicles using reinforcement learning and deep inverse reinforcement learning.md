# Advanced planning for autonomous vehicles using reinforcement learning and deep inverse reinforcement learning

# 使用强化学习和深度逆强化学习的自动驾驶车辆高级规划

[toc]

## 文献摘要

- 摘要：本文主要研究自动驾驶车辆在交通中的规划问题。我们将自动驾驶车辆与环境之间的相互作用建模为随机马尔可夫决策过程（MDP），并将专家驾驶员的驾驶风格作为学习目标。在 MDP 模型中考虑了道路的几何形状，以便纳入更多不同的驾驶风格。自动驾驶汽车所需的专家级驾驶行为可通过以下方式获得：首先，我们设计相应 MDP 的奖励函数，并利用强化学习技术确定自动驾驶汽车的最佳驾驶策略。其次，我们收集专家驾驶员的大量演示，并利用反强化学习技术根据数据学习最优驾驶策略。使用深度神经网络（DNN）对专家驾驶员的未知奖励函数进行近似。我们阐明并验证了应用最大熵原理（MEP）学习 DNN 奖励函数的方法，并提供了使用最大熵原理学习参数化特征（奖励）函数的必要推导。模拟结果表明，使用强化学习和反强化学习技术，自动驾驶汽车可以实现理想的驾驶行为。
- 关键词：强化学习，逆强化学习，深度神经网络，最大熵，路径规划，自动驾驶汽车

***

## 问题建模

- 我们将要控制的自动驾驶车辆指定为主车辆（HV），并且将交通中的所有剩余车辆指定为环境车辆（EV）。我们假设不同车辆的驾驶员彼此不通信，并且车辆彼此不共享数据。
- 从当前状态 $s_t$ 到下一状态 $s_{t+1}$ 的状态转换过程分两步给出：
  - 首先，HV 观察当前状态 $s_t$ 并选择遵循其当前策略的动作 $π(st)$。
  - 随后，EV 响应 HV 的动作，并且按照随机序列采取遵循它们自己的策略的动作。
- 环境中每辆车的可用动作集为{“保持”、“加速”、“刹车”、“左转”、“右转”}
- 状态空间：
  - 使用白色虚线将道路划分为多个小单元，当 HV 位于道路中间车道，使用 9 个单元格表示状态； 当 HV 紧邻道路边界，使用 6 个单元格表示当前状态。
    ![image-20241014151455474](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20241014151455474.png)
    - 每辆车被视为一个质点，保证车辆不会跨越多个单元格
    - 文章中使用了单元格来表示车道上的每个位置，并通过标记每个单元格是否被车辆占据来表示状态。
  - 为了研究道路集合形状对不同驾驶员驾驶行为的影响，在本研究中将道路曲率考虑在内，并考虑左转、右转和直行三种道路
  - 使用这种方法状态空间大小为：
    - 内部车道有 9 个单元格（$2^8 = 256$ 种组合）。
    - 左边界和右边界车道各有 6 个单元格（$2^5 = 32$ 种组合）。
    - 三种道路几何形状（直道、左转、右转），将状态空间 = 960 = 3 x (256 + 32 + 32)
  - 动作空间
    - 环境中每辆车的可用动作集为 {“保持”、“加速”、“刹车”、“左转”、“右转”} 
    - EV 采取不会引起碰撞的随机动作，每个车辆在每个时间步采取单个动作。

***

## 使用强化学习和深度逆强化学习的自动驾驶汽车高级规划

- 强化学习

  - 设计奖励函数，使用 RL 求解使用 MDP 建模后的问题
  - 强化学习方法中主要有两种方法来表示和更新状态-动作值函数（Q 值函数）：表解法和函数逼近法
    - 表解法
      - 表解法适用于有限的离散状态和动作空间。在这种方法中，Q 值通过一个表格来存储。
      - 每个状态-动作对都有一个单独的 Q 值。强化学习算法会在每次与环境交互后更新这个 Q 表，直到 Q 值收敛到最优值。
    - 函数逼近法
      - 当状态或动作空间过大或连续时，表解法变得不可行。
      - 此时，可以使用函数逼近法来近似表示 Q 值函数。函数逼近法通过使用参数化模型（如神经网络、线性回归等）来逼近状态-动作值函数
    - 因为本文中的环境的建模方式使得状态数量可以维持在较小值，所以本文采用表解法来更新 Q 值函数。（Q-learning）
- 使用深度逆强化学习设计奖励函数

  - 前文对奖励函数进行了设计，然而在对奖励函数的先验知识不充分的情况下，很难设计所需的奖励函数来实现期望的驾驶行为。
  - 在逆强化学习问题中，目标是从专家的行为演示中推断出他们的奖励函数。专家的行为通常是通过最大化某个未知的奖励函数而获得的。
  - 传统的逆强化学习方法可能会导致多个可能的奖励函数与专家行为一致，在这种情况下，最大熵原理（Maximum Entropy Principle）被引入，它通过选择具有最大熵的策略来消除不必要的偏差。
  - 最大熵原理指出，在所有满足已知条件（如边缘分布、期望值等）的概率模型中，熵最大的模型是最优的。这是因为最大熵模型在没有更多额外信息的情况下，避免对未知数据做过度的具体假设。
  - 直观地假设离散随机变量 X 的概率分布式 P(X)，则他的熵为：
    $H(P)=-\sum_xP(x)log(p(x))$
  
    - 当且仅当 X 的分布是均匀分布时，熵最大。
    - 最大熵原理认为，要选择的概率模型首先必须满足已有约束条件，而在没有更多信息的情况下，那些不确定的部分都是“等可能的”。
  - 最大熵原理推导

    - 假设一组样本 $ x_i \in \mathcal{X} $， $ i = 1, 2, \dots, n $ 是从某个目标分布 $ p^*(x) $ 中独立同分布（i.i.d.）抽取的。我们还知道一组关于这些样本的期望约束：
      $$\hat{\mu}_j = \frac{1}{n} \sum_{i=1}^{n} f_j(x_i), \quad j = 1, 2, \dots, m,$$

      - 其中 $f_j(x) $ 是样本的特征函数。
    - 我们希望找到一个满足这些约束的概率分布 $ p(x) $，并使它的熵最大化：
      $$p = \arg \max_p \int_{\mathcal{X}} - p(x) \log p(x) \, \nu(dx),$$
      - 同时满足以下约束条件：
      - 期望约束： $\mathbb{E}[f_j(x)] = \int_{\mathcal{X}} p(x) f_j(x) \, \nu(dx) = \hat{\mu}_j, \quad j = 1, 2, \dots, m $。
      - 概率分布的归一化约束： $\int_{\mathcal{X}} p(x) \, \nu(dx) = 1$。
    - 为了同时处理这些约束和最大化熵，使用拉格朗日乘子法，分别引入拉格朗日乘子 $ \theta_j $（对应期望约束）和 $ \lambda $（对应归一化约束），构造拉格朗日函数：
      $\mathcal{L}(p, \theta, \lambda) = - \int_{\mathcal{X}} p(x) \log p(x) \, \nu(dx) + \sum_{j=1}^{m} \theta_j \left( \int_{\mathcal{X}} p(x) f_j(x) \, \nu(dx) - \hat{\mu}_j \right) + \lambda \left( \int_{\mathcal{X}} p(x) \, \nu(dx) - 1 \right)$
      - 对拉格朗日函数关于 $ p(x) $ 求偏导，找到最优概率分布 p(x)：
        $p(x) = \exp\left( -1 + \lambda + \sum_{j=1}^{m} \theta_j f_j(x) \right)$
    - 由于 $ p(x) $ 需要满足归一化约束 $ \int_{\mathcal{X}} p(x) \, \nu(dx) = 1 $，将 $ -1 + \lambda $ 吸收到归一化常数中，得到：
      $p(x) = \frac{1}{Z(\theta)} \exp\left( \sum_{j=1}^{m} \theta_j f_j(x) \right)$
      - 其中 $ Z(\theta) $ 是配分函数，定义为：
            $Z(\theta) = \int_{\mathcal{X}} \exp\left( \sum_{j=1}^{m} \theta_j f_j(x) \right) \, \nu(dx)。$
  
    - 使用期望约束，解得 $ \theta_j $ 的值
      $\hat{\mu}_j=\int_\mathcal{X}p(x)f_j(x)\:\nu(dx),\quad j=1,2,\ldots,m$
  - Ziebart et. al. (2008) 首次将应用最大熵原理来解决逆强化学习问题，在这种情况下，奖励函数只依赖于当前状态，它通过特征函数的线性组合来表示，即：
    $R(s)=\sum_{i}w_{i}\phi_{i}(s)=w^{\intercal}\Phi(s),$
    - 在文章中使用 $\zeta \triangleq \{s_{0},a_{0},\ldots,s_{T},a_{T}\}$ 表示一个演示的状态动作序列，则该演示的概率为：
      - 确定性 MDP （s + a 可以唯一确定 s'）：$\mathbb{P}(\zeta|w)=\frac{1}{Z(w)}e^{\sum_{s\in\zeta}w^{\intercal}\Phi(s)}$
      - 随机性 MDP：$\mathbb{P}(\zeta|w)=\frac{1}{Z(w)}e^{\sum_{s\in\zeta}w^{\intercal}\Phi(s)}\prod_{(s,a,s^{\prime})\in\zeta}\mathbb{P}(s^{\prime}|s,a)$​
    - 一个演示的概率与其总奖励的指数成正比，代表代理更喜欢奖励更高的路径
    - 逆强化学习问题的目标是找到最优权重 $w^∗$，从而使观察到的演示的可能性在使用最大熵计算的分布下达到最大。
    - 下面，我们用不同的奖励结构来表述逆强化学习问题。我们将为满足以下要求的 IRL 问题提供必要的推导：(1) 我们不使用 [45,49] 中的状态奖励 R(s) 和状态特征 Φ(s)，而是使用 R(s, a) 和 Φ(s, a)；(2) 我们只关注随机 MDP。我们使用以下符号：D 表示演示集合，N 表示 D 中演示的数量，Ω ⊇ D 表示完整路径空间，Φζ 表示沿路径 ζ∈ D 的特征计数，其值为 Φζ = ∑ (s, a)∈ζ Φ(s, a)。
  - 奖励函数：
    - 三种奖励函数的定义方式：
  
      - 状态奖励 R(s)：S→R
        - 当 agent 想要通过采取任何行动达到某个目标状态或避免某些危险状态时，使用 R(s)。这个定义表明代理对现有的行为没有特定的偏好。
      - 状态-行动 奖励 R(s, a)：S × A→R
        - R(s, a) 考虑了行为，可以用来表示 agent 对某一特定行为的偏好。 
      - 状态-行动-状态奖励 R(s, a, s’)：S × A × S→R。  
        - R（s, a, s ‘）考虑了 agent 在当前状态 s 下采取行动 a 后的结果状态 s ’。
        - 然而，由于结果状态 s‘ 取决于 agent 采取行动 a 后环境的反应，因此 agent 只能根据采取行动 a 的预期回报做出决策，而不知道未来的状态 s ’。 因此，R(s, a, s') 和 R(s, a) 在学习相同的策略方面应该是相等的。 
    - 本文使用状态-行动 奖励 R(s, a) 来重现对可用动作具有不同偏好的驾驶行为内在奖励
    - 使用特征的线性组合来表示奖励函数：$R(s,a)=w^{\mathrm{T}}\Phi(s,a),$
      - $w$ 是权重向量
      - $\Phi(s,a)$ 是特征向量，每个分量表示状态-动作空间中的单个特征点，包括：
        - 动作特征。如果驾驶员从某些动作中获得更高的奖励，则他可能更喜欢采取这些动作
        - HV 位置特征。它指示 HV 是否在道路边界附近行驶。根据道路几何形状，驾驶员可能更喜欢在不同的车道上驾驶。
        - 超车策略特征：该功能用于实现驾驶员在转弯时的不同超车行为。驾驶员可以具有关于从左侧或从右侧超车前面的汽车的不同偏好。
        - 跟随特征：该特征用于表示主车是否在跟随前车。主车可能会在某些情况下选择尾随前车，尤其是在车流密集或没有超车机会的情况下。尾随行为可以获得一定的正奖励。
        - 碰撞特征：如果 HV 和 EV 出现在同一单元格中，则发生碰撞。
    - 参数化特征和非参数化特征
      - 非参数化特征：$R(w;s,a)=w^{\intercal}\Phi(s,a).$
        - $\Phi(s, a)$ 是从状态 s 和动作 a 中提取的固定特征（非参数化）。
        - 优化目标：找到最优权重 $w^∗$​，从而使观察到的演示的可能性在使用最大熵计算的分布下达到最大（最大化似然）
          $\begin{aligned}
          w^{*}& =\arg\max_{w} \mathscr{L}_{\mathrm{D}}(w)=\arg\max_{w} \frac{1}{N}\sum_{\zeta\in\mathscr{D}}\log\mathbb{P}(\zeta |w) \\
          &=\arg\max_{w} \frac{1}{N}\Big(\sum_{\zeta\in\mathscr{D}}\Big(w^{\intercal}\Phi_{\zeta} + \sum_{(s, a, s^{\prime})\in\zeta}\mathbb{P}(s^{\prime}|s, a)\Big)\Big)-\log Z(w).
          \end{aligned}$
          - $\mathscr{L}_D(w)$ 表示在权重 $w$ 下，观察到的数据集 $\mathscr{D}$ 的概率（对数似然函数）
          - 正向梯度更新 $\frac{\partial\mathscr{L}_{\mathrm{D}}}{\partial w}$  得 $w^{*}$
      - 参数化特征：$R(w,\theta;s,a)=w^{\intercal}\Phi(\theta;s,a).$
        - $\Phi(\theta; s, a)$ 是从根据参数 $\theta$ 状态 s 和动作 a 中提取的特征（参数化）。
        - 优化目标：找到最优权重 $w^∗$ 以及 最优参数 $\theta^*$，从而使观察到的演示的可能性在使用最大熵计算的分布下达到最大（最大化似然）
          $\begin{aligned}
          w^{*},\theta^{*}& =\arg\max_{w,\theta} \mathcal{L}_{\mathrm{D}}(w,\theta)=\arg\max_{w,\theta} \frac{1}{N}\sum_{\zeta\in\mathcal{D}}\log\mathbb{P}(\zeta|w,\theta) \\
          &=\arg\max_{w,\theta} \frac{1}{N}\Big(\sum_{\zeta\in\mathscr{D}}\Big(w^{\intercal}\Phi_{\zeta}(\theta)+\sum_{(s, a, s^{\prime})\in\zeta}\mathbb{P}(s^{\prime}|s, a)\Big)\Big) \\
          &- \log Z(w,\theta).
          \end{aligned}$
          - 使用梯度上升法更新权重 $w$ 以及参数 $\theta*$
    - 在本文中使用参数化特征构建奖励函数，并设置提前设置权重 $w$，单独使用一个 DNN 更新参数 $\theta$。
  - 最大熵逆强化学习
    - 我们考虑一个完全无模型的情况，其中不知道状态转移模型 P(s’|s, a)。
    - 模型学习的思想是分析每个状态-动作-状态三元组的访问次数，并计算状态转换的每个可能结果的概率
      $\mathbb{P}(s^{\prime}|s,a)={\frac{\nu(s,a,s^{\prime})}{\sum_{s^{\prime}\in S}\nu(s,a,s^{\prime})}},\;\;\nu(s,a,s^{\prime})=count(s,a,s^{\prime})$
    - 基于模型学习的 Q-learning 算法
      ![image-20241014215723342](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20241014215723342.png)
    - 最大熵逆强化学习
      ![image-20241014231309445](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20241014231309445.png)
  - 逆强化学习改进

    -  改进的原因
      - 在标准的最大熵逆强化学习（MaxEnt IRL）算法中，通常需要依赖状态转移模型来计算状态-动作对的期望访问次数。然而，在某些情况下，这种假设并不成立，或者系统的随机性较强，难以准确预测未来状态的转移。此外，长轨迹数据可能会带来以下问题：
        - 长期预测误差累积：在高度随机的环境中，预测未来状态的误差会随着时间的推移而累积。尤其是在轨迹长度较长的情况下，这种误差可能会严重影响最大熵 IRL 的性能。
        - 专家演示数据不足：在一些情况下，收集到的专家演示数据可能不足以覆盖系统的所有可能状态，导致对状态转移模型的学习不够充分。
        - 模型学习难度大：对于复杂的随机系统，模型学习本身也是一个挑战，尤其是当系统的动态行为较为复杂时，难以准确地学习状态转移模型。
  
    - 避免模型学习（Single-Step Joint Maximum-Entropy Deep IRL）

      - 问题：在标准 IRL 算法中，期望状态-动作访问次数的计算依赖于状态转移模型 $ P(s' | s, a) $。然而，在某些情况下，状态转移模型可能无法准确获取或预测。
      - 解决方法：作者提出了一种只依赖于单步期望值的学习方法，即使用单步轨迹来避免模型学习的依赖。这种方法通过比较单步状态-动作对的访问次数来更新奖励函数的参数，而不需要完整的状态转移模型。
      - 具体方法：
        - 只使用单步轨迹片段，即每个轨迹片段的长度 $ \Delta T = 1 $。
        - 在每个状态下，只考虑当前状态和动作对的访问次数 $ \mu(s, a) $，而不需要考虑未来状态的转移概率。
        - 通过最大化单步轨迹的对数似然来更新奖励函数的参数。
  
      - 这种方法的优势在于：

        - 避免对未来状态的预测：由于不依赖于状态转移模型，可以减少模型不准确带来的偏差。

        - 适用于高度随机的系统：在系统随机性较强、状态转移模型难以学习的情况下，这种方法更加稳健。

      - 算法伪代码
        
        ![image-20241014234050456](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20241014234050456.png)![
    
    - 分段轨迹学习（Multiple-Step Joint Maximum-Entropy Deep IRL）
    
      - 问题：当轨迹长度较长时，系统的随机性可能会导致长期预测误差的累积。专家演示数据可能不足以全面覆盖系统的所有可能状态，导致学习过程中的误差较大。
      - 解决方法：为了避免长轨迹带来的误差累积，作者提出将轨迹分段，将长轨迹分解为多个较短的轨迹片段进行学习。这种方式可以减少长期预测误差的影响，并使得算法能够更好地处理系统的随机性
      - 具体方法：
        - 将专家演示数据中的每条轨迹 $ \zeta $ 切分为多个较短的子轨迹 $ \zeta_{\tau} $，每个子轨迹的长度为 $ \Delta T $。
        - 对每个子轨迹进行最大熵逆强化学习，最大化每个子轨迹的对数似然。
        - 对所有子轨迹进行联合学习，确保整体数据满足最大熵分布。
      - 算法伪代码
        ![image-20241014234126521](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20241014234126521.png)

## 实验分析

- 仿真环境
  - 本文交通仿真使用 Pygame 开发，Pygame 是一个免费的开源 python 编程语言库，用于开发多媒体应用程序。
  - 高速公路由一系列相连的直道和弯道组成，每段有五车道。
  - 由于每辆电动汽车都被视为一个质点，并且不区分模拟器上车辆的类型（即卡车，轿车）。
  - HV 用绿色汽车表示，电动汽车用其他颜色的汽车表示。模拟器只提供 HV 上方摄像机的顶视图。
  - 模拟器中的每个 EV 实现一个随机策略。 对于每辆 EV，我们使用周围所有车辆（HV 和 EVs）的状态 $s^{EV}$，为该 EV 找到不会导致碰撞的安全动作。然后，为安全动作集中的每个动作分配随机概率，为该 EV 生成随机策略。 
    ![image-20241014235055411](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20241014235055411.png)
    - 对于每个 EV，我们只实现第 1-3 行一次来初始化其策略 $π^{EV}_0$。
    - 由于每个 EV 的安全动作集随着模拟过程而变化，$π^{EV}_0$​ 需要根据新的驾驶环境而变化，以避免碰撞。为此，当安全操作集和状态发生变化时，我们使用第 4-8 行更新策略。通过采取车道切换动作，HV 和 EV 都能够在模拟期间访问 9 单元内部车道状态和 6 单元边界状态。
- 强化学习算法实验分析
  - 使用强化学习学习超车和跟车两种不同的驾驶行为。使用前文定义奖励函数特征，并设计权重 w1 和 w2 以分别实现两个期望的驾驶行为。
    ![image-20241015232716908](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20241015232716908.png)
    - 设计权重 w1 的学习超车决策方法，具体描述如下：
      - 如果前方有空位，HV 会加速占据前方空位；
      - 如果前方有EV，且无法超车，HV 会保持速度；
      - 如果只有一侧可以超车，HV 会超越前方EV，方法是先换线，然后加速并保持恒速；
      - 如果左右两侧均可超车，则小型货车从弯道内侧超越前方电动车； 
      - 除非为了超车，否则小型货车不变更车道；
      - 小型货车不制动以占用后方车位。
      - 不允许发生碰撞。
    - 设计权重 w2 的学习跟车决策方法，具体描述如下：
      - 如果前方有EV，HV 保持速度；
      - 如果前方有空位，HV 加速占据前方空位，不会通过变道发生追尾；
      - 如果前方没有EV，HV 变道跟随EV；
      - 在拐弯处，HV 优先跟随更靠近道路内侧车道上的车辆； 
      - 除非为了跟随前车，否则 HV 不变更车道；
      - HV 不为占用后部单元而制动； 
      - 不允许发生碰撞。
  - 模型收敛情况
    ![image-20241015234514915](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20241015234514915.png)
  - 通过强化学习实验得到超车和跟车策略 $\pi_1^*$ 和 $\pi_2^*$
- 逆强化学习算法实验分析
  - DNN网络设计输入为当前状态，输出为该状态下采取每个行动对应的奖励
  - 在模拟环境中使用上文得到的策略 $\pi_1^*$ 和 $\pi_2^*$ 收集模拟数据，作为逆强化学习的专家驾驶数据
  - 对上文中提到的传统最大熵DIRL(AL 5)、单步联合最大熵DIRL（AL 6）、多步联合最大熵DIRL（AL 7）进行实验对比
    ![image-20241015235905724](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20241015235905724.png)
    - 由于数据长度过大和系统状态转移的随机性，算法4没有收敛收敛。
    - 所提出的改进算法（算法5和6）提供了更好的收敛性能，并有效地学习策略。
    - 与算法6相比，算法5因为跳过了模型的学习，并且避免了计算预期的状态-动作访问计数，算法训练速度更快。
    - 我们还在模拟中实现了学习到的策略<$π <$1和<$π <$2。3我们只显示了通过实现<$π <$2的尾随行为（见图12），因为通过实现<$π <$​1的结果与图10中已经显示的π <$1类似。图12示出了四种驾驶场景。第一行示出了在HV前方具有空闲空间并且HV不能通过改变车道来尾随任何EV的驾驶场景。HV加速以占据其前方灰色车辆后方的空间。第二个驾驶场景显示HV向左变道，

- 实验结论：
  - 该模型没有区分不同的车辆速度，并且它将每个车辆视为一个质点。
  - 奖励函数中每个特征值都为离散的0、1值

