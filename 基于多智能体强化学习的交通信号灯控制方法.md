# 基于多智能体强化学习的交通信号灯控制方法

[toc]

## 论文摘要

- 题目：基于多智能体强化学习的交通信号灯控制方法
- 摘要：该论文研究了一种基于多智能体强化学习的交通信号灯控制方法，首先利用SUMO仿真系统对交通环境建模，并采用MADDPG算法优化小规模路网的信号灯控制策略，证明了其优于固定配时和IDQN策略。随后，论文提出了一种基于全息系统的多智能体交通控制方法，通过将大规模路网划分为多个子区域，由全息系统进行分层控制，从而实现更有效的交通疏导。实验结果表明，该方法能够显著减少车辆的平均延迟时间和排队长度，提高路网的通行能力。
- 关键词：多智能体系统，交通信号灯控制，深度强化学习，多智能体强化学习，交通仿真系统，全息系统

***

## 研究背景和目的

- 城市交通信号灯是协调路网和交通流的重要自动化工具。
- 交通信号灯的控制策略主要是一个信号周期的长短、信号切换的时机和一个信号周期内绿灯和红灯的时间比例。现有的控制策略主要有固定配时，传感器探测和自动控制
  - 固定配时
    - 是指交通信号灯的信号周期和周期内的相位持续时间都是固定的，无法根据车流量的变化更改自己的策略，策略远未达到最优，
    - 但因其部署简单，现在仍是绝大多数城市的交通信号控制的主流方法
  - 传感器探测方法
    - 利用物理手段检测十字路口等待的车辆数量，并根据检测数据进行决策。结合路口车辆等待数量和设定阈值的方式对交通信号灯的相
      位进行选择。
    - 这种方法在一定程度上解决了路段拥堵，相比于固定配时有一定优势，但仍非常简单，无法适应动态变化。
  - 自动控制
    - 动态捕捉城市交通的车流，路口排队长度，等待市场，然后通过控制算法调节现有的交通情况，动态可变化的调整交通信号灯的周期长度和周期内相位时间
    - 这类方法具有一定的智能性和自主性，采取的策略可能会达到次优，甚至可以达到最优的控制效果。
- 强化学习智能交通信号灯控制
  - 单智能体强化算法仅优化单个交叉路口的策略，而忽略了各交叉路口之间的联系。路网中信号灯的行为会直接影响相邻交叉路口的交通状况，所采取的决策会在路网中产生连锁反应。因此单智能体强化学习算法在优化路网信号灯策略时，具有一定的局限性。
  - 多智能体强化学习在交通信号灯的研究，主要是要让多个智能体之间学会相互合作的策略，联动的控制，进而产生联合全局最优动作将会给全局路网流通带来收益。不同于单智能体强化学习，各个智能体不能单独追求自己利益最大化，盲目的尝试增大自己交叉路口的车辆通行率。因为环境的变化是所有智能体共同造成的，单方面的追求自己通行最大化往往会带来相反的效果。
  - 但是在路网中部署多智能体系统困难重重。路网规模庞大，信号灯数量会根据路网规模和路网复杂程度增加，组织数量庞大的智能体将变得困难。而要求智能体学会协同控制车流的策略要求智能体具备和其他智能体进行通信的功能。当每个智能体使用强化学习算法优化策略时，还会遇到维度爆炸、难以收敛和环境不稳定等问题。
- 多智能体强化学习理论和算法
  - 单智能体深度强化学习算法关注的仅是一个智能体和环境的沟通，让单个智能体去完成一个独立的静态任务。
  - 可是在现实生活中，大多数问题并不是孤立的问题，它是由许多子任务共同构成的。而完成全局任务中的各种子任务需要智能体各司其职，这些智能体可能处于完全不同的工作环境，可能关注环境中的不同方面，它们都需要根据自己的任务需求，根据自身观察，做出独立的决定，进而为全局任务做出贡献。
  - 由于智能体数量不止一个，可以对环境造成影响的单位也变多了，所有的智能体在某一时刻做出的动作的好坏无法仅仅根据自身的动作和环境来做评价，还需要结合所有的智能体的联合动作考虑。这就要求智能体不能只关注环境的变化，也要学到智能体之间的相互协作，甚至相互竞争。
  - 多智能体强化学习（MARL）是通过多个智能体相互协调，自底向上的解决一个大问题的方法。在多智能体强化学习中，多智能体系统中所有的智能体都通过深度强化学习来·优化自己策略
  - 多智能体遵循随机博弈过程（SG），SG可由多元组$(S, A_1, A_2, · · · , A_n, R_1, · · · , R_n, f, R, \pi)$​来表示
    - 
    - S为环境的状态空间
    - n 为智能体的数量，$A_i(i = 1, 2, · · · , n)$ 是每一个智能体可以采取到的动作空间，$A = A_1 × A_2 × · · · × A_n$ 则是整个多智能体系统的联合动作空间
    - 整个多智能体系统的联合状态转移函数为 $f:S\times A\times S^{^{\prime}}\to[0,1],$
    - 所有智能体的回报 $R_i$ 不仅从局部环境环境得到反馈，有时也会考虑全局奖励。所以多智能体系统的联合回报由多智能体系统的联合策略的好坏决定。
    - 所有智能体的策略共同组成了整个多智能体系统系统的联合策略 π。
  - 按照每一个智能体任务的不同，随机博弈的MARL可以被分为三种类型：完全合作，完全竞争和混合型。
    - 完全合作：所有的智能体都具有相同的目标，所有的智能体的任务就是最大化共同奖励
    - 完全竞争：完全竞争型关系多智能体系统中的智能体的奖励函数具有敌对性，环境中的智能体彼此之间的关系纯粹是竞争关系。
    - 混合型：在智能体关系是混合型关系系统中，智能体之间的奖励函数不存在明确的关系，该环境中的智能体可以理解为自利型智能体
  - 完全合作多智能体强化学习方法
    - Value Decomposition Network，VDN 算法
      - VDN 算法尝试找到全局奖励和所有智能体局部奖励的关系。环境中的每一个智能体拥有自己的局部观测，智能体 i 的值函数 $\hat{Q}(h_i, a_i)$ 依赖于它的局部观测和做出的决策。
      - 综合所有智能体存在一个多智能体团队价值函数$Q_{total}$，VDN算法采取贪心思想，认为当每一个智能体都可以选取使自己价值函数最大的动作时，全局价值函数也会取到最大值，

