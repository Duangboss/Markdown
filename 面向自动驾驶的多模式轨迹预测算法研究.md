---
tags: []
parent: ""
collections:
    - 研0基础阅读
$version: 2040
$libraryID: 1
$itemKey: 4NU78WBF
marp: true

---
# 面向自动驾驶的多模式轨迹预测算法研究

## 研究背景与目的

*   自动驾驶典型解决方案

![](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/UFEHKA54.png)

*   轨迹预测模块主要基于上游环境感知模块的结果实现对周围动态智能体未来轨迹的预测，并将预测结果输出到下游路径规划模块，作为环境理解的额外信息，指导进行安全合理的路径规划与策略选择。由于未来存在固有不确定性，需要轨迹预测模块预测多条轨迹来拟合智能体的多种可能运动模式，以涵盖未来可能突发的小概率事件，保证规控决策的安全性。

    *   单模式轨迹预测任务往往基于运动学物理模型8等基本方法，其预测结果简单且预测范围较短，无法有效表征未来的不确定性，难以适用于高等级自动驾驶应用。
    *   多模式轨迹预测任务旨在面向高等级自动驾驶的复杂场景提出适配的多模式轨迹预测算法，具体分为面向城市快速道路场景的单智能体多模式轨迹预测算法与面向城市拥挤道路场景的多智能体多模式轨迹预测算法，通常可有效理解环境的不确定性，全面表征未来风险，保证轨迹预测模块的功能实现。

*   多模式轨迹预测
![](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/XWIY4C3R.png)
    *   在自动驾驶汽车的多模式轨迹预测任务中，周围动态智能体未来的多模式轨迹往往由多种因素决定

        *   一方面由自身运动状态等客观物理因素约束
        *   另一方面受到目标意图、潜在交互、交通规则等主观因素影响

    *   多模式轨迹预测任务的输入数据有目标智能体的历史轨迹、周围智能体的历史轨迹以及地图数据，通常上述语义层面的决定因素需要从这些输入数据中隐式地挖掘建模。

        *   对于自身运动状态等物理因素，往往可以通过目标智能体的历史轨迹等数据拟合得到：
        *   对于目标的意图，其往往代表了未来的多种模式，可以根据地图中的道路拓扑结构及自身运动状态建模；
        *   对于潜在交互，可以根据目标智能体及周围智能体的历史轨迹数据分别在时空维度进行挖掘并建模；
        *   对于交通规则，往往可以在地图数据提供的语义信息中体现。

    *   传统统计学方法

        *   传统统计学方法往往利用卡尔曼滤波器（KF）、隐马尔可夫模型（HMM）、动态贝叶斯网络（DBN）等数学统计模型对先前的运动状态进行建模，实现对未来状态的传播。还可细分为基于物理模型和基于传统机器学习的方法。

            *   基于物理模型的方法将预测过程看作物理过程，假定智能体通过如恒加速度、恒转角速率与加速等运动状态行驶，根据对应运动学模型或动力学模型得到当前时刻与下一时刻的运动状态关系，并结合卡尔曼滤波、隐马尔可夫等模型实现轨迹预测。

            *   基于传统机器学习的方法往往考虑到不同的驾驶行为，主要基于支持向量机（SVM）、隐马尔可夫模型、动态贝叶斯网络等机器学习方法

                *   首先预测驾驶人的一系列驾驶行为如直行或左右变道等，并假设未来的运动遵循这些驾驶行为组成的状态空间，然后针对不同的驾驶行为预测相应的轨迹。
                *   这类方法相较基于物理运动学模型的方法可考虑更多输入信息，如引入简单地图数据或交互建模等，但相关驾驶行为的启发式设计往往仅适用于某个场景比如高速公路中，普适性较差，另外当输入信息较多时此类方法计算复杂度较高，无法满足实时性要求。

        *   传统统计学方法适合较简单场景中对单一目标的短期预测，在复杂交通场景中难以实现对周围智能体的潜在交互建模，可能导致不合理的预测结果；其次，此类方法无法解决针对多个不同类型智能体的轨迹预测问题，需额外针对不同类型分别设计模型；最重要一点在于此类方法往往有效预测范围较短，适合短期预测，难以表征出有效的多模式轨迹，无法满足下游规划任务的需求。

    *   基于深度学习的方法

        *   基于深度学习的方法往往针对不同的输入信息采用适合的神经网络模型进行特征提取与建模，最后将各模型组合为完整的深度神经网络模型，实现多模式轨迹预测。

        *   时序信息挖掘

            *   由于轨迹预测任务的主要输入与输出均为轨迹点序列，本质上是一个时间序列预测问题，通过编码器充分挖掘时序信息，并将输入序列建模为隐层特征向量再通过解码器将隐层特征向量映射为未来序列，

                *   时序编解码器的相关模型包括循环神经网络（RNN）、一维卷积神经网络（1D-CNN)、Transformer 等。

            *   时序信息挖掘模型通常作为轨迹特征提取、预测的编解码框架，还需在其基础上考虑交互特征的建模、地图数据的表征，融合各部分特征，形成完整的轨迹预测模型。

        *   交互特征建模

            *   动态智能体之间的潜在交互是未来轨迹的重要影响因素，特别是在密集交通场景，不同的交互和博弈结果会直接引发不同的行为，导致不同的未来轨迹，所以在轨迹预测任务中对交互特征的有效建模往往至关重要。

                *   基于空间网格图

                    *   基于空间网格图的方法往往按照欧氏距离将动态智能体周围的空间划分为二维网格，并应用CNN、社会力（SocialForce）模型、社会池化层（SocialPooling）等方法，将网格内相关智能体的特征聚合到一起，以模拟智能体之间的交互。能够直观地表示智能体之间的相对空间关系。
                    *   一般情况下网格的大部分区域为空，但仍会进行无效计算，导致较高的计算成本，需额外考虑网格的稀疏性以提高计算效率。

                *   基于图神经网络（GNN）

                    *   基于图神经网络的方法往往采用有向全连接图，将各智能体当前时刻的轨迹信息作为图上节点特征，根据节点间的欧氏距离等空间信息生成邻接矩阵，并应用图卷积网络（GCN）、图注意力机制（GAT）等方法，根据节点和边的特征在图上进行消息传递，以捕获节点之间的相互依赖性，并通过叠加消息传递的层数增大各节点的感受野。
                    *   基于图神经网络的方法将规则的空间网格泛化为具有不规则拓扑的任意图，根据图上节点之间的边自然地建模交互信息，在现有方法中表现出较好的性能。

                *   基于Transformer的方法

                    *   与基于图神经网络的方法类似，同样将场景中各时刻的智能体看作图上节点，不同点在于基于Transformer的方法无需建图和考虑边的连通性，而是直接针对所有智能体节点应用注意力机制进行交互建模，其效果可类似于无向全连接图的GAT，但依赖于Transformer较强的表征能力，若有充足训练数据往往可得到更好的性能。
                    *   基于Transformer的方法往往具有更大的模型容量和学习能力，但缺点在于计算复杂度较高，且需要充足的训练数据。

            *   现有轨迹预测模型中大多通过图神经网络和Transformer实现交互建模，通常和时序特征提取的相关方法结合，在每个历史时刻建-一个空间图，得到时空图，然后交替使用RNN、GNN或Transformer在时空维度进行交互的建模，充分挖掘时序信息和空间交互信息。

        *   地图数据表征

            *   对于多模式轨迹预测任务，对地图数据的有效表征是获取智能体可执行操作并理解其意图的关键，轨迹预测中输入的高精度地图存储了车道的拓扑结构及车道间的连通性，蕴含着丰富的交通状态与道路结构信息，通常可分为栅格化地图与矢量化地图两种输入形式，对应着不同的处理方法

                *   对于栅格化地图，输入格式为图像，不同的图层中代表着不同的道路元素如可行驶区域、人行横道等。

                    *   通常采用计算机视觉领域的CNN骨于网络如ResNet-50l或Transformer骨于网络SwinTransformer等进行图像特征提取，得到全局特征图（featuremap）
                    *   然后根据各智能体所处位置的邻域截取特征图上的局部区域，得到各智能体感兴趣区域内（ROl）的特征图
                    *   最后可展平后与轨迹特征、交互特征等各智能体相关特征拼接融合，实现地图信息的表征，
                    *   栅格化地图对于轨迹数据来说是一种过于复杂的表示，且往往包含了大部分无用区域，这样直接融合比较简单粗暴，具有额外的计算开销，导致计算复杂度较高。

                *   对于矢量化地图，输入为地图上的语义采样点，不同采样点有不同的语义类型如车道中心线、交通信号灯、人行横道等，与栅格化地图相比根据人为先验进行了有效道路元素的采样与筛选，其存储和计算成本要小得多，且大多现有的真实世界轨迹预测数据集如Argoverse、INTERACTION、NuScenes中均提供了矢量化地图

                    *   此类地图输入的采样点与轨迹点均为二维坐标点，同样可通过GNN相关方法建图并根据连通性确定交互边，然后基于GCN、GAT等方法实现图上消息传递，表征为道路节点图，并通过在图上考虑智能体轨迹点进行消息传递，将地图特征融合入智能体特征，实现地图数据的表征。
                    *   若将轨迹点与稀疏的地图采样点看作点云图，还可通过点云图相关方法挖掘时空维度的依赖性。
                    *   以上两类方法均直接将矢量图上的采样点看作单独的点进行特征提取与融合，未从车道中心线等语义集合层面考虑，而对于车辆来说车道中心线通常具有很强的先验信息，能显式表征不同的意图，故可以将地图采样点集合看作不同的车道中心线，根据节点连通性提取车道中心线上的节点特征后，通过注意力机制、MLP等方法将某些关键车道中心线上的节点特征融合入智能体特征，实现地图数据表征

                *   在多模式轨迹预测任务中，不论是轨迹时序特征、空间交互特征还是地图提供的场景特征，均或显式或隐式地影响着智能体未来的轨迹，综合考虑这些输入信息会使模型对当前场景表征的更全面，在本质上会使预测结果更准确、合理，所以完整的多模式轨迹预测算法通常为以上模型的灵活组合。为深入研究自动驾驶中的多模式轨迹预测算法，本文将由浅入深，依次分析总结针对车辆目标的单智能体多模式轨迹预测算法研究现状，及针对异构目标的多智能体多模式轨迹预测算法研究现状。

*   单智能体多模式轨迹预测算法

    *   车辆和行人的运动规律有较大区别，现有单智能体多模式轨迹预测算法往往区分二者分别展开研究。

        *   在本文中，主要面向高速公路等仅包含稀疏车辆的城市快速道路场景，针对自动驾驶汽车周围某一关键智能体展开单智能体多模式轨迹预测算法的研究，由于在这类场景中行人极少且常位于边缘，导致其感知效果较差、数据质量不佳，故本文中不考虑行人仅针对车辆。

    *   单智能体多模式轨迹预测算法大多围绕如何生成多模式轨迹，综合考虑上目标意图、潜在交互、交通规则等轨迹预测问题中的关键因素展开研究。

    *   \*\*由于直接回归多模式轨迹容易出现模式崩溃现象，即模型为了尽可能地拟合训练数据，收敛到所有可能模式的轨迹均值附近，而多模式轨迹的均值并不一定是有效的轨迹。\*\*为避免模式崩溃，预测有效的多模式轨迹，现有研究提供了不同的多模式表征思路，大致可分为基于固定锚点、基于生成式模型和基于地图采样点的方法，![](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/CQEIN8CB.png)  

        *   基于固定锚点的方法

            *   首先根据经验，从道路结构或驾驶习惯中生成固定的轨迹锚点集实现常见行为模式的解耦，然后在模型中考虑当前运动状态、所处区域的道路结构、与周围智能体的交互等可变因素，对固定锚点集进行概率预测，启发式地选择当前情景下的可能锚点，最后基于各可能锚点实现不同行为模式的轨迹预测。

                *   启发式锚点往往针对高速公路这种行为模型较为固定的场景，根据统计先验启发式地预设左右换道、保存车道等几类轨迹锚；
                *   聚类锚点则是根据某一数据集中采集的运动轨迹数据，通过K-均值（K-means）、最近邻等方法将未来轨迹聚类为多个轨迹锚，与启发式锚点相比这类方法的锚点数选择更灵活，覆盖的轨迹空间更全面，但往往仅适用于单-一数据集。

            *   由于固定锚点大多针对某一固定场景人为设置或基于某一数据集聚类生成，对应的行为模式集固定于某一场景或数据集，导致这类方法生成的多模式轨迹普适性较差，较难迁移到其他场景或数据集中应用。

        *   基于生成式模型的方法

            *   大多基于生成对抗网络（GAN）或条件变分自动编码器（CVAE）等生成式模型，引入一个已知分布的隐变量如正态分布噪声，在训练阶段通过神经网络拟合未来轨迹空间的分布，将隐变量的分布空问映射到输出轨迹的多模式空间，推理时对潜在变量进行多次采样实现多模式轨迹的输出。

        *   基于车道采样点的方法

            *   **由于车辆总是将某条车道作为参考路径并倾向于沿着该车道的中心线行驶，其行驶轨迹终点及参考路径往往可通过对车道中心线采样得到，而不同的轨迹终点及参考路径通常对应着不同的潜在意图，同时也代表了未来轨迹的不同运动模式。**

            *   现有基于车道采样点的多模式轨迹预测方法，首先对高精度地图中的车道中心线进行采样，根据采样点得到轨迹终点候选空间或候选路径集合输入模型，然后设计分类任务筛选终点或路径，得到对应多个运动模式的轨迹终点或参考路径并通过轨迹终点或参考路径指导该运动模式下的轨迹预测，最后将各预测轨迹组合得到多模式轨迹。

                *   基于轨迹终点的方法，首先基于地图上的车道中心线采样点预测终点概率热图，并设计不同的采样策略对概率热图进行采样得到多个终点，最后基于终点进行轨迹补全，得到多模式轨迹，此类方法由于直接对轨迹终点回归，可达到较低的终点位移误差。
                *   基于参考路径的方法，主要通过启发式方法检索得到目标车辆可能行驶的候选车道线，将候选车道线视为候选路径，并针对候选车道线设计车道注意力机制以指导进行轨迹预测，综合多条候选车道线得到多模式轨迹。由于参考路径与轨迹终点相比增加了对中间轨迹的修正，可保证中间轨迹的合理性与准确率，故基于参考路径的方法可达到更低的轨迹平均位移误差且预测结果更符合道路拓扑。

    *   以上三类方法中：

        *   基于周定锚点的方法易适配于单一场景，但较难迁移到其他场景或数据集中，泛化能方较差；

        *   基于生成式模型的方法通过有效训练可实现较高的预测精度与合理的多模式，但存在可解释性差且推理时无法保证实时性等固有缺点：

        *   基于车道采样点的方法引入人为经验，将车道线上的采样点作为轨迹终点或参考路径等语义实体，一方面可根据车辆所处道路情况实现自适应的多模式，另一方面可参照终点或路径实现误差较小的轨迹预测，在车辆多模式轨迹预测算法中有较大优势。**参考路径与轨迹终点相比可提供更多的信息，且以车道中心线作为参考路径进行预测可使中间轨迹更合理，更符合道路拓扑，故本文中主要考虑基于车道中心线的车辆多模式轨迹预测方法**

    *   关键问题分析

        *   车辆未来的多模式轨迹通常由自身运动状态、临近车辆运动状态、道路拓扑结构等信息决定

        *   车辆信息与车道信息之间往往存在较强的相互依赖性：对于临近车辆，其当前所处的车道与后续倾向于选择的车道可在很大程度上表征其未来运动趋势，反过来其位置分布也可体现出车道的拥塞、占用情况；对于目标车辆，一方面不同的候选车道往往代表着其不同模式的未来轨迹，另一方面，已选定车道的拓扑结构与局部占用情况可为该模式下的未来轨迹提供很强的先验信息。

        *   现有基于车道中心线的方法往往

            *   首先基于地图检索出如直行、左转、右转共四条可能的候选车道

            *   然后依次使用RNN等网络提取运动特征、通过GNN等网络建模交互特征、基于提出的车道注意力机制挖掘原始候选车道的拓扑等地图信息

            *   最后或直接或间接地将这三类特征融合，并基于融合特征实现轨迹预测

            *   这些方法中大部分均可在捕获多个运动模式的同时挖掘道路拓扑信息，取得较高的预测精度，但明显**忽略了上述车辆与车道信息间的两点依赖性，仍将交互信息建模与车道信息挖掘分开考虑，这样无法体现不同临近车辆及局部车道区域的重要性**

        *   **处在不同位置的临近车辆对目标车辆的交互作用有明显差距，仅仅是那些相关车道与目标候选车道存在冲突的临近车辆才有可能影响目标车辆的选择与判断，需考虑临近车辆与对应车道的相关性以突出不同的临近车辆及不同的车道局部区域，融合体现有效交互。**

*   多智能体多模式轨迹预测算法

    *   在城市拥挤道路、十字路口等密集交互场景中，自动驾驶汽车往往需要同时预测周围多个智能体的多模式轨迹，以确保对未来可能风险的预判能力

    *   若直接采用单智能体多模式轨迹预测算法并行预测多个智能体的未来轨迹，往往会导致预测结果冲突，且容易产生过高的计算复杂度

    *   **在多智能体多模式轨迹预测问题中，除了考虑单智能体多模式轨迹预测相关要点外，还需联合不同智能体的预测结果，避免轨迹间可能的冲突，以确保未来轨迹的一致性**。为现有研究思路可大致分为条件预测、在编码时建模智能体间全局交互、在解码时考虑轨迹一致性三类
    ![](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/8WNJWCDP.png)
        *   条件预测相关方法

            *   条件预测相关方法首先分类判断智能体间是否存在交互以及在交互中所处的角色，然后针对不同交互角色依次进行条件预测，最后通过多轮条件预测实现协同一致的多智能体多模式轨迹预测。
            *   此类方法将多智能体轨迹预测问题分解为基于单智能体轨迹预测的条件预测，精细化地通过条件预测保证轨迹的一致性，可达到较高的联合预测精度
            *   但也正因为过于精细地考虑两两交互，需进行多轮串行预测，导致在密集交通场景中推理时延过高，难以落地应用。

        *   编码时建模全局交互

            *   由于多智能体未来轨迹的一致性本质上仍为智能体间交互的体现，可参照单智能体轨迹预测相关方法在网络的编码阶段对交互进行建模，不同点在于需针对多个智能体未来可能存在的全局交互建模，而非单智能体中的局部交互
            *   此类方法在编码阶段通过分层图或全局图间接或直接地对智能体间的全局交互建模，以此达到提高多智能体未来轨迹一致性的目的，由于可参照单智能体轨迹预测中局部交互建模相关方法，逐渐成为研究热点
            *   但此类方法在编码阶段往往需考虑输入视角的敏感性，如LaneRCNN、HEAT、HDGT等方法中的输入以各智能体为中心，将重复编码重叠部分区域的特征，造成冗余计算。

        *   在解码阶段考虑预测结果的一致性

            *   该方法通常能更有效地实现多智能体系统预测，直接保证轨迹一致性。
            *   此类方法在解码阶段直接针对预测结果的一致性进行优化，在理论上可保证协同一致的多智能体轨迹预测

    *   以上三种思路中，除了条件预测受到时延过高的固有限制难以落地外，另外两种思路均可在一定程度上提高预测轨迹的一致性。总的来说，**若要实现多智能体协同一致的多模式轨迹预测，在算法模型中需综合考虑编码阶段的全局交互建模以及解码阶段的轨迹一致性优化，二者相结合以达到最优效果。**

    *   关键问题分析

        *   输入数据的视角敏感![](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/R99J76KS.png)

            *   现有多智能体多模式轨迹预测算法大多由单智能体相关方法拓展得到，为便于模型拟合数据分布，往往将每个智能体邻域内的空间上下文数据分别转换到各自视角后再输入模型，以达到数据分布正则化的目的，但此做法需针对不同智能体多次前向传播，导致模型在编码时对各智能体重叠部分区域的上下文数据需重复编码，进行额外的冗余计算，并且按此类数据训练出的模型往往对输入视角敏感，仅能学习到固定范式下的轨迹预测，若自动驾驶实际感知条件较差会使视角转换有偏差，将导致极差的预测性能，对输入数据较苛刻
            *   故针对密集场景的多智能体轨迹预测算法需研究视角自适应的模型，输入数据以自动驾驶汽车或场景中某一智能体为中心，无需转换视角，仅单次前向通过模型即可实现多个智能体的多模式轨迹预测，

        *   智能体的异构性

            *   多智能体多模式轨迹预测算法应用场景中往往存在多种的智能体类型，具有显著不同的运动特点，不同类型的智能体之间同样具有不同性质的复杂交互，在多智能体多模式轨迹预测算法中，对不同类型的智能体分别进行与运动表征，并在建模异构智能体间的交互式考虑异构性

        *   多智能体联合多模式轨迹的实现

            *   现有基于单智能轨迹预测实现的多智能体多模式轨迹预测方法中，其预测的多模式轨迹为独立于每个智能体的边缘分布，对于各智能体边缘预测结果，将有智能体数目的指数幂种可能的模式组合，需按照启发式规则进行复杂的后处理筛选才得以应用到下游规划任务。
            *   对多智能体的多模式轨迹，需研究端到端的场景级联合模式预测算法，可根据场景的输入数据直接输出多智能体的联合多模式轨迹，并在每个场景级模式内考虑轨迹的协同一致性，有效避免模式内冲突，以真正实现协同一致的联合多模式轨迹预测。

***

## 联合学习车辆与车道信息的车辆多模式轨迹预测算法

### 问题描述

*   输入：目标车辆的上下文信息 $\mathbb{C}$
![](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/N9JVI9SP.png)
    *   $A^{tar}$表示目标车辆在历史$T_h$个时间步的轨迹序列

    *   $\mathbb{S}$表示N个临近车辆在历史$T_h$个时间步的轨迹序列集合，$a_t,s_t^i\in\mathbb{R}^2$,为目标车辆和临近车辆$i$在t时刻鸟瞰视角下的二维坐标

    *   $\mathbb{L}$表示当前场景中的M条车道中心线集合，每条车道中心线由D个车道节点组成，$l_d^i\in\mathbb{R}^{2}$为第j条车道中心线的第d个车道节点在鸟瞰视角下的二维坐标。

*   输出：目标车辆的预测轨迹序列$\mathbb{Y}$和可能概率P
    ![](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/84AWFY4K.png)
    *   $Y^k$表示第k个模式的预测轨迹序列，$\hat{y}_t^k\in\mathbb{R}^2$为目标车辆第k个模式的预测轨迹在t时刻的二维坐标，总共用K个模式来表征未来运动模式的多种可能性

    *   $p^k$表示第k个模式的预测概率

### 算法整体框架

![](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/U429G58E.png)

*   算法输入可分为目标特征、交互特征和车道特征三部分，交互特征和车道特征分别从邻近车辆的轨迹数据及高精度地图数据中提取，并通过S2L模块表征为场景上下文，用于RLA编解码器中辅助进行基于目标特征的轨迹预测。

*   具体地

    *   对于临近车辆的历史轨迹$S^{1:N}$，先后经过ID-CNN和GRU进行时序特征提取，取当前时刻隐层作为临近车辆的交互特征$S^{1:N}$输入到S2L模块；
    *   对于高精度地图的车道中心线采样点，首先基于启发式方法检索目标车辆可行驶的候选车道线$L^{1:M}$，通过多尺度1D-CNN进行特征提取，作为车道特征$L^{1:M}$输入到S2L模块：
    *   对于目标车辆历史轨迹$A^{tar}$，通过1D-CNN提取运动趋势，作为目标特征$H=\{ht,t\in[-T+1,0]\}$输入RLA编码器。

### 基于S2L模块的语义车道线联合表征

*   首先基于各临近车辆的历史运动状态，应用恒加速度运动模型，得到其在短时间内的轨迹推出，然后根据推出的轨迹检索欧式距离10米内的相关车道节点$\dot{l}_d^j$*，并针对各临近车辆节点*$\dot{S}^{1:N}$及相关车道节点$\dot{l}_d^j$应用如下空间注意力机制： ![](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/ZMS6E2Z5.png)

    *   其中，$\varphi_{res}(.)$和$\varphi_{agg}(.)$均为单层MLP 并对所有节点权重共享，|| 表示在特征维度的拼接，$dist= MLP(V_{\dot{l}_d^j}－V_{s^j})$蕴含着临近车辆节点与相关车道节点的空间位置关系，其中V为T=0时刻的节点位置坐标。

*   对于临近车辆更新后的交互特征$\tilde{S}^{1:N}$，将与目标车辆节点的隐藏层$\dot{H}^{1:M}$​一同作为节点输入特征，根据T=0时刻各车辆的位置构建如图3-3右侧所示的交互图为保证潜在交互的学习，图上交互边为全连接单向边，并应用如下图注意力机制![](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/GWFWEFED.png)

    *   其中，其中l为层数，f为节点输入特征，z为经映射后的节点特征，$dist_{p,q}=MLP(V_p-V_q)$表示节点p和节点q的空间坐标差，$e_{p,q}^{(l)}$表示节点q到节点q的边特征，$a_{p,q}^{(l)}$对应其注意力得分，（目标车辆$f_p^{(0)}$为最初节点输入特征$\dot{H}^j$，其余节点输入特征$f_q^{0}$为临近车辆特征$\tilde{S}^{1:N}$

### 基于RLA模块的多模式轨迹自回归预测

*   RLA编码器
    ![](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/2V2U4BDV.png)
    *   首先将目标车辆当前时刻的隐层特征$h_t^j$与位置信息$a_t^i$，输入到一个单层GRU中更新隐层特征$h_{t+1}^j$并进行下一时刻粗略关键点$b_{t+1}^j$的位置预测：![](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/LGR2FHJZ.png)

        *   $\varphi_{waypoint}(\cdot)$为两层MLP

    *   驾驶员更关注于他们当前所处车道的临近区域以及下一秒将要到达位置附近的车道状况。故基于当前时刻位置点$a_t^j$和下一时刻位置点$b_{t+1}^j$可检索到参考车道线上欧式距离最近的车道节点A和B，并通过车道注意力机制，以A到B的局部车道节点序列为条件进行特征更新：
    ![](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/RZMQ2P5Q.png)
        *   $W^K,W^V$针对所有车道节点权重共享，$\sqrt{d_k}$表示缩放系数，$d_k$为隐层张量的维度，$E_d^j$表示车道节点结合与目标车辆位置、曲率相关关系的特征，$D_d^j$表示目标车辆当前位置$a_t^j$与车道节点d的相对位置关系，$\Delta\theta_d^j$表示目标车辆当前时刻朝向与车道节点d处曲率的角度差

        *   在该注意力机制中，$h_t^j$作为query向量，为体现A到B段车道线的曲率变化等相对空间信息，将$E_d^j$作为key、value向量

    *   最后，更新后的隐层$\tilde{h}_j$通过一个两层GRU串联预测器来得到下一时刻的预测点$a_{t+1}^j$和$h_{t+1}^j$，并将其作为下一轮的输入，不断循环迭代知道获得完整序列 ![](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/28AN9IUJ.png)

        *   $\varphi_{waypoint}(\cdot)$为两层MLP

    *   对于RLA编码器，其输入$H=\{h_t,t\in[-T_h+1,-1]\}$需要复制M次后分别与M条候选车道线并行应用RLA模块，取最终的$h_0^{1:M}$作为输出

    *   **编码阶段RLA模块主要学习如何将不同参考车道线的节点特征编码到历史运动状态中，并得到不同的隐层向量。**
*   RLA解码器

    *   输入$\tilde{H}^{1:M}$本身对应了不同候选车道线，考虑到每条参考车道线上仍可能存在不同的运动模式（加速、匀速、减速等），在解码器输入阶段将$\tilde{H}^{1:M}$复制K次以学习不同的可能性，最后取$t\in[0,T_f-1]$，对$\tilde{H}^{1:M}_{1:K}$并行应用RLA模块得到完整的M\*K条轨迹
*   算法的完整流程为：
    ![](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/73YYR6Z5.png)
    *   针对RLA编码器输出的M条参考车道线对应的隐层$h_0^{1:M}$，通过车道选择器，得到M条参考车道线的可行概率估计值；
    *   针对RLA解码器输出的M\*K条轨迹，取$t=T_f$的各轨迹终点隐层代表轨迹特征，通过轨迹选择器，得到M\*K条轨迹的概率估计；
    *   将M条参考车道线的可行概率与M\*K条轨迹的概率相乘，并取前K条最高概率的轨迹，将概率重新归一化，得到最终K条轨迹$Y^{1:K}={\hat{y}_t^{1:K}，t\in[1,T_f]}$及概率$p^{1:K}$"，作为模型的多模式预测结果。
    *   车道选择器与轨迹选择器均为单层MLP，对应了两个分类子任务。
*   模型训练及推理

    *   模型共包括一个回归子任务和两个分类子任务，三者相互独立![](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/AE8JM6PX.png)
    *   轨迹预测回归子任务采用赢者通吃（WTA）策略，即在训练时仅计算K条预测轨迹中重点距离真值最近的轨迹的损失![](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/U7E3EQKJ.png)
    *   对于车道选择和轨迹选择的两个分类子任务，在训练时分别考虑参考车道、预测轨迹与真实轨迹之间的距离，采用自监督方式生成：

        *   车道选择子任务
            ![](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/XE9H3WHS.png)
            *   $D_1(L_j,Y_{GT})$表示真实轨迹在各时刻与候选车道最近节点的欧式距离之和，$\beta(t)=t$为缩放权重体现不同时刻的重要性
        *   轨迹选择子任务
            ![](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/BGHNPYFD.png)
            *   $D_2(Y^k,Y_{GT})$表示真实轨迹与预测轨迹在各时刻的欧式距离之和
        *   最后使用交叉熵损失函数计算分类损失

## 基于异构图Transformer的多智能体联合多模式轨迹预测算法

### 问题描述

*   在城市拥挤道路等较普遍的困难驾驶场景中，通常包含数量密集、种类繁多的动态智能体，自动驾驶汽车往往需要综合场景中**所有智能体的历史轨迹、类别等相关数据A**以及**当前区域对应的车道中心线、人行横道、红绿灯等地图数据M**，以**预测**场景中**所有智能体在未来几秒的轨迹集合Y**

*   本文仅考虑车辆、行人和非机动车这三种较普遍的智能体类型，并且将AV（自动驾驶车辆）视为场景中的一个智能体同样输入到模型中进行预测

*   输入：整个场景的上下文信息$\mathbb{C}$
![](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/NW4HJTHZ.png)
    *   $\mathbb{A}$表示场景中N个智能体在历史$T_h$个时间步的输入数据集合，智能体i在t时刻的输入特征$a_t^i\in\mathbb{R}^5$包含二维坐标、朝向、类别（1、2、3；车辆、行人、非机动）、填充掩码；

    *   $\mathbb{M}$为t=0时刻当前场景所在区域的地图数据，并包含矢量图上道路元素的M个采样点，第j个地图节点的输入特征$m^j\in\mathbb{R}^6$包括采样点的二维坐标、所属道路元素的类别（one-hot编码；车道中心线、红绿灯、人行横道、停车线）

*   输出：为确保输出的多模式轨迹可直接用于下游规划任务，多智能体多模式轨迹预测算法的输出为场景中所有智能体未来轨迹的联合多模式分布$\mathbb{Y}$
    ![](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/HTNSSU6V.png)

*   其中，$Y^k$表示第k个联合模式的预测轨迹集合，包含整个场景中所有智能体的一种轨迹模式，$\hat{y}^{k,i}_t\in\mathbb{R}^3$​为第k个联合模式中第i个智能体在t时刻的预测轨迹点，包含二维坐标及朝向，总共用K个联合模式来表征整个场景未来可能出现的多种轨迹分布情况。

### 算法整体架构

### ![](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/S2RVX6PL.png)

*   首先围绕整个场景根据当前时刻各节点所处位置建立全局异构图，图上节点包括场景中的异构智能体节点及所在区域异构道路元素的采样点，通过欧氏距离判断节点间是否存在交互边，并基于节点之间的距离、夹角等相对位置信息以及节点类别计算交互边的初始特征，得到初始全局异构图。

*   其次分别通过特定网络对图上异构节点进行表征

    *   针对不同类型的智能体采用与类型绑定的1DCNN+FPN网络进行时序特征提取，并通过时间维度的Transformer保留关键时序信息，得到各智能体节点的初始特征；

    *   针对道路节点，采用LaneGCN进行空间特征提取，多尺度地表征道路的拓扑连通性，得到各道路节点的初始特征

        *   由于对道路节点的映射过程不涉及动态智能体且自适应于输入视角，可以通过离线处理并将特征缓存以在线使用，避免重复计算。

*   然后基于初始的全局异构图，通过异构图Transformer进行消息传递与特征更新，建模各节点间全局交互，经过多轮更新后，得到各节点最终特征，并根据各智能体在道路图中的位置检索其邻域内的道路节点，作为候选路径节点进行各智能体基于路径的边缘多模式轨迹预测。

*   以上过程对应了基于异构图Transformer的边缘多模式轨迹预测编码器，对该编码器进行充分训练后即可针对场景中的各智能体实现有效的边缘多模式轨迹预测，

*   最后为了得到场景协同一致的联合多模式轨迹，需基于各智能体的边缘预测结果，应用联合多模式轨迹解码器，在可学习的场景级特征空间中进行边缘模式的特征聚合与重组，实现多智能体联合多模式轨迹的协同预测。

### 基于异构图Transformer的边缘多模式轨迹编码器

* 全局异构图的建立与初始化

  * 针对t=0时刻的场景，将场景中包含的所有智能体与道路节点作为图上节点v，根据各节点所处空间位置$v^{pos}$​建立全局有向图， 由于图中存在多种类型的节点及交互，需将该图建为**异构图**，具体体现为**节点的异构性与边的异构性**
    ![](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/PYEB74BA.png)

    *   $\mathbb{V}$为图上节点的集合，$v_i$包含节点 i 的节点特征$v_i^{att}$与空间位置信息$v_i^{pos}$，节点特征通过特定网络对节点的输入数据进行向量映射得到，空间位置信息主要包含二维坐标$x_i^{pos}$与方向$h_i^{pos}$；

    *   $\mathbb{E}$为图上边的集合，$e_{j\rightarrow i}$表示源节点 j 到目的节点 i 的有向边，包含相对特征$e_{j\rightarrow i}^{attr}$与交互类型$v_{j\rightarrow i}^{cis}$，需根据$v_i^{pos}$，$v_j^{pos}$计算节点i，j之间欧氏距离来判断交互边$e_{j\rightarrow i}$与$e_{i\rightarrow j}$是否存在，并基于节点间相对位置信息及节点类别计算边的初始特征。

  *   节点初始特征提取

      *   智能体节点主要包含时序运动信息，而道路节点主要包含空间连通性信息，所以需要采用不同的网络进行特征提取

      * 智能体节点初始特征提取
        ![](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/PHZSRN8H.png)
      
        *   将绝对坐标序列转换为残差向量$v_i^{att}=A_{-T_h+2:0}^{i}-A_{-T_h+1:-1}^{i}$（错位相减），表示智能体的相对运动状态
      
        *   不同类别智能体运动特性差异较大，单独训练网络
      
        *   首先将智能体历史轨迹输入到由残差块组成的FPN网络中提取多层次的短期运动特性，得到运动特征序列，主要由1DCNN、层归一化、激活函数堆叠而成以有效提取序列化特征；
      
            *   FPN网络
      
                *   **底层特征提取（Bottom-up Pathway）**：
      
                    *   通过一个主干网络（如ResNet）提取输入图像的底层特征。在不同的卷积层，主干网络生成一系列特征图，这些特征图的空间分辨率逐层降低，而语义信息逐层丰富。
      
                    **顶层特征整合（Top-down Pathway）**：
      
                    *   从最高层特征图（语义信息最丰富但空间分辨率最低）开始，逐步向下采样。通过上采样操作（如反卷积或双线性插值）将特征图的空间分辨率逐步提高，同时结合来自底层特征图的特征信息。
      
                    **横向连接（Lateral Connections）**：
      
                    *   在每一层上采样的过程中，通过横向连接将顶层特征图与对应的底层特征图进行融合。具体操作是对底层特征图进行1x1卷积，以匹配顶层特征图的通道数，然后两者相加。这样做的目的是结合低层的细节信息和高层的语义信息。
      
                    **输出多尺度特征图（Output Feature Maps）**：
      
                    *   经过上述步骤，FPN生成了一组具有相同通道数但空间分辨率不同的多尺度特征图。这些特征图可以用于后续的目标检测或其他任务，以提高对不同尺度物体的检测能力。
      
        *   然后引入可学习的嵌入向量对特征序列应用时序Transformer，学习序列的全局特征，保留关键时序信息，最后将输出的可学习嵌入向量作为智能体节点的初始特征。
      
      *   道路元素初始特征提取
      
          *   针对道路元素的空间采样点数据M，同样需要将绝对坐标向量化，参照LaneGCN中的思路，根据道路节点的前后连通关系，取两道路节点的中点为新的道路节点，节点坐标更新为$x^{pos}_i=(m^{i,end}+m^{i,start})/2$，方向$h^{pos}_i$为起点指向终点的单位向量，对应的残差向量特征更新为$v^{att}_i=m^{i,end}-m^{i,start}$。
      
          *   预先表征道路节点的连通性，采用LaneGCN在道路图上进行道路节点间的消息传递，通过1、2、4、8四个尺度的LaneConv算子聚合不同范围内节点之间的相互依赖关系，充分挖掘道路图的拓扑连通性，得到全局图上各道路节点的初始特征
      
              *   LaneGCN是一种专门用于道路网络的图卷积网络（GCN），其主要目的是通过捕捉道路节点之间的空间关系和连通性，从而为自动驾驶中的路径规划、轨迹预测等任务提供有效的特征。LaneGCN将道路网络被表示为一个图，节点表示道路的采样点，边表示道路的连通关系。每个道路节点初始化为一些初始特征，如节点的位置、方向和残差向量特征，LaneGCN通过多个尺度的LaneConv操作来捕捉不同范围内的节点关系，全面挖掘道路网络的拓扑结构和节点之间的连通性。
      
                  *   多个尺度的LaneConv：
      
                      *   **1尺度**：捕捉相邻节点的关系。
      
                      *   **2尺度**：捕捉两个跳跃节点的关系。
      
                      *   **4尺度**：捕捉四个跳跃节点的关系。
      
                      *   **8尺度**：捕捉八个跳跃节点的关系。
      
          *   边初始特征
      
              *   首先需要判断边是否存在：根据节点i、j的绝对坐标计算欧式距离，考虑到智能体的不同运动特性，针对车辆、行人、非机动车分别取距离值为50m、15m、30m，筛选掉超出距离值的节点，假定各智能体与运动范围外的节点不存在交互；
      
              *   针对具有连通性的节点i、j，考虑到有向边$e_{j\rightarrow i}$是为了在全局视角下建模源节点 j 对目的节点 i 的潜在影响，那么边的相对特征$e^{att}_{j\rightarrow i}$需要表征出各源节点 j 在目的节点 i 的视角下的空间位置分布，为充分体现这一点，本文在初始化$e^{att}_{j\rightarrow i}$时考虑 j 到 i 相对视角转换，基于节点间相对位置信息及朝向进行编码得到边的初始特征$e^{0,att}_{j\rightarrow i}$
              ![](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/CNIPMAZB.png)
                  *   其中$\varphi_{edge}(\cdot)$和$\varphi_{theta}(\cdot)$为两个单层MLP，$[\cdot,\cdot]$表示在特征维度拼接
      
                  *   $R_i\in\mathbb{R}^{2x2}$为根据节点 i 的朝向角$h_i^{pos}$​计算得到的视角旋转矩阵，
      
                      *   当一个点 (x, y) 旋转角度$\theta$后的新坐标 ((x', y')) 可以通过矩阵乘法表示为：$\begin{pmatrix}x'\\y'\end{pmatrix}=R\begin{pmatrix}x\\y\end{pmatrix}=\begin{pmatrix}\cos(\theta)&-\sin(\theta)\\\sin(\theta)&\cos(\theta)\end{pmatrix}\begin{pmatrix}x\\y\end{pmatrix}$
                      *   通过$R_{i}^{T}\big(x_{j}^{pos}-x_{i}^{pos}\big)$将节点 j 相对于节点 i 的全局位置差异转换到节点 i 的局部视角中
      
                  *   $H_{j\rightarrow i}$代表了两节点朝向角的相对偏转情况。
      
              *   不同类型节点之间的交互存在较大差异，例如行人通常走人行横道，在少数情况下才会和车道中心线产生联系，而车辆的停车行为往往与人行横道有关，但在正常行驶时会沿着车道中心线。考虑到交互边的这种异构性，本文根据离散的节点类别生成对应的交互边类别$e_{j\rightarrow i}^{cls}\in[0,I_{cts}]$，实际取预设的离散值，后续消息传递时将基于边的类别进行门控判断，体现异构性。

* 基于异构图Transformer的全局消息传递
  ![](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/EIY5WUTD.png)

  *   节点特征更新

      *   首先基于边的连通性，在图上检索与该节点存在交互边得相关节点$j\in\mathcal{N}(i)$

      *   根据交互边类型$e_{j\rightarrow i}^{cls}$通过门控机制选择对应网络进行节点特征更新
      ![](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/AQR54FWH.png)
          *   其中$W^Q_c，W^K_c，W^V_c，W^{gate}_c，W^{pre}_c$均为与不同类型边绑定的可学习参数，仅针对同一类型的边进行参数共享，$g_{i,c}^l$为类似RNN中的门控机制，旨在有选择地根据当前消息传递结果进行节点特征的更新，$\odot$​为张量之间的逐元素相乘操作。

      *   根据交互边类型进行消息传递后，最终节点特征需考虑所有类型的交互边实现特征更新，仍采用Transformer中的注意力机制学习对不同类型交互边的权重并完成特征聚合
          ![](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/35QCBKAP.png)

  *   边特征更新

      *   基于当前层中边的源节点特征与目的节点特征进行边的特征更新：
          ![](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/YD87E4FU.png)
          *   其中$\varphi_{update}(\cdot)$为单层MLP，不同类别间不共享参数

* 基于路径的边缘多模式轨迹预测

  *   首先在异构图上检索该智能体领域内的相关道路节点$j\in\mathcal{N}(i)$，由于这些道路节点指向智能体的交互边$e_{j\rightarrow i}^{attr}$​可在一定程度上体现出其各自的重要性，可用于估计智能体将其作为自身路径节点的概率：
  ![](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/R6UL24CA.png)
      *   $\varphi_{path}(\cdot)$​为输出单个概率值的双层MLP，训练时此处引入了--个自监督分类子任务用于学习真实路径的概率分布。

  *   基于得到的$p_{j\rightarrow i}^{path}$​进行S次模拟采样，得到S种可能的路径组合。为拟合智能体在各细化路径下的轨迹情况，针对不同路径上的道路节点应用如下attention机制：
      ![](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/M9WA2WTQ.png)
      *   其中，$\varphi_{rela}(\cdot)$为单层MLP，$\varphi_{path}(\cdot)$为双层MLP组成的单模式轨迹预测器，区分智能体类型共享参数，$W^Q、W^K、W^V$为对所有节点均共享的可学习参数；

      *   为突出路径上的拓扑、曲率等空间信息，在key和value中引入了与交互边特征初始化时类似的相对位置信息，如旋转矩阵$R_i$、相对朝向角等；

      *   $\dot{Y}_s\in\mathbb{R}^{T\times2}$为第s条路径下的单模式预测结果。

      *   基于S条路径下预测的轨迹集合$\dot{Y}_i^{1:S}$，通过K-means聚类，可得到智能体i的K条边缘多模式轨迹$\tilde{Y}_i^K$：
          ![](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/TPU4YPRA.png)
      
      *   最终通过对各智能体并行应用如上流程，可得到场景中所有智能体的边缘多模式轨迹集合$\tilde{Y}^K$

*   针对场景协同一致性的联合多模式轨迹解码器

    *   假设边缘多模式轨迹编码器的输出$\tilde{Y}^K$可以全面表征每个智能体的未来轨迹若想得到能直接用于下游规划任务的联合多模式轨迹，通常需要将所有N个智能体的K个边缘模式进行排列组合得到$K^N$条联合多模式轨迹，再根据启发式规则进行后处理，筛选掉存在冲突的不可能组合后截取前K条最可能的的联合多模式轨迹输出，

    *   上述方法在一般驾驶场景中此类方法简单且有效，但在城市拥挤道路场景中往往智能体数目较多，其模式组合数将爆炸式增长，导致后处理的时空复杂度较高，需要更严苛的启发式筛选规则。

    *   可学习的联合多模式轨迹预测解码器 ![](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/G8QADMF6.png)

    *   针对场景的协同一致性进行边缘多模式的可学习组合，直接输出多模式轨迹的联合分布。

    *   该解码器的输入输出与后处理过程相同，基于编码器预测的场景中N个智能体各自K条边缘多模式轨迹$\tilde{Y}^{N,K}$，考虑场景中轨迹的协同一致性，筛选可能存在冲突的轨迹组合，输出最可能的K个联合多模式轨迹$Y^K$​，每个联合模式中均包含N个智能体在某一种可能情况下的轨迹。 
    ![](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/Z3QJDY57.png)
        *   其中$\phi_{agent}(\cdot)$为智能体轨迹特征提取网络，与编码器中的智能体节点特征初始化网络具有相同结构

        *   $S^K$为K个可学习的特征向量参数，用于表征K个联合模式的场景级特征空间。

        *   通过多头注意力机制有选择性地聚合N个智能体的所有N\*K个边缘多模式特征，此处注意力的头数设为16。

    *   由于每个联合模式的场景级特征$S^k$均包含了当前场景中所有智能体所有模式轨迹特征，可根据各智能体的边缘多模式特征与各场景级特征计算点积相似度并根据度量结果重组边缘模式：
        ![](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/QRLZEVF4.png)
        *   其中$argmax_k$表示仅取各智能体在场景级特征空间中相似度得分最高的那条边缘模式作为该联合模式下的轨迹特征，
        *   然后分别通过$\phi_{pred}(\cdot)$基于各智能体重组的轨迹特征进行预测，得到K个多智能体的轨迹组合$Y^K$作为最终的联合多模式轨迹，$\phi_{pred}(\cdot)$同样为智能体类型特定的双层MLP

### 模型训练及推理

*   边缘多模式轨迹编码器用于预测多智能体的边缘多模式轨迹，联合多模式轨迹解码器需基于前者的结果，并且需要在前者的输出可以全面表征每个智能体的未来轨迹前提下进行边缘模式的聚合重组，以生成场景协同一致的联合多模式轨迹。因此在训练时不能直接端到端训练整个模型，需针对编码器与解码器分阶段训练

*   首先通过边缘多模式轨迹预测损失函数训练编码器直至其收敛并可以针对场景中的多智能体生成边缘多模式轨迹后，冻结编码器的网络权重，再通过联合多模式轨迹预测损失函数训练解码器，进行边缘模式的特征聚合与重组，生成联合多模式轨迹。

*   边缘多模式轨迹编码器

    *   具体用于训练编码器的边缘多模式轨迹预测损失函数$L_{marginat}$​共包含路径选择与轨迹回归两部分：
        ![](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/CCQCE4U4.png)
        *   其中$x_j^{pos}$为道路节点 j 的位置坐标，$y^i_t$为智能体 i 在 t 时刻的真值轨迹点，$\hat{y}_t^{k,i}$为智能体 i 的第 k 个边缘模式的预测轨迹点；
        *   路径选择子任务为自监督学习任务，其标签$Label_{j\rightarrow i}$需根据道路节点 j 与智能体 i 未来轨迹点的最近欧氏距离在线生成，其损失函数$L_{path}$取预测概率值与生成标签的交叉熵；
        *   轨迹回归子任务同单智能体多模式轨迹预测算法中的回归损失，采用WTAI51策略计算各智能体最优预测轨迹与GT的L1损失函数，但此处需对场景中所有智能体求和，目的是在不影响多模式的前提下使场景中每个智能体自身的边缘预测结果更准确。

*   联合多模式轨迹预测损失函数

    *   使用上述损失函数训练编码器至收敛后，需冻结编码器部分的网络权重，采用如下联合多模式轨迹预测损失函数$L_{joint}$​仅针对解码器进行训练：
        ![](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/224NHYVG.png)
        *   其中$\hat{y}_t^{k,i}$​代表k个联合模式中智能体i的预测轨迹点，Ljoint同样采用WTA策略，但统计的是整个场景的最优联合模式中各智能体的预测轨迹与GT的L1损失函数，目的是在不影响模型组合出多种可能的联合模式前提下，尽可能地将最优轨迹组合到同-一个场景级联合模式中，这样可在一定程度上保证场景中轨迹的协同一致性。
