# Interaction-Aware Planning With Deep Inverse Reinforcement Learning for Human-Like Autonomous Driving in Merge Scenarios
# 汇入场景下基于深度逆强化学习的类人自动驾驶交互感知规划

[TOC]

## 摘要

- 在高速公路汇入场景中，自主驾驶车辆（ADV）与其他交通车辆的相互影响非常强烈，车辆需要选择适当的目标间隙并在加速车道结束前完成汇入，且存在多种不确定性。因此，如何让自主驾驶车辆表现得像人类司机一样，能够与周围的人类驾驶车辆进行有效的互动，成为了一个重要课题。这种互动不仅可以提高驾驶的安全性，还能减少乘客接管车辆的频率。针对以上问题，本文提出了一种基于深度逆强化学习的交互感知决策与规划方法，用于并线场景下的实现具有人类驾驶风格的自动驾驶，该方法通过学习人类驾驶员的奖励函数，提升了规划的可解释性和泛化能力。与传统的预测方法不同，本文模型不仅考虑了自主驾驶车辆的行为，还通过交互预测其他交通车辆的轨迹。通过决策模块选择合适的汇入间隙，进一步减少了规划的计算复杂度。
- 关键词：深度逆向强化学习、类人化的（Human-like）、交互、汇入场景、规划
- 期刊：IEEE Transactions on Intelligent Vehicles (IEEE TIV). SCI Q1
- 作者：
  1. 南江峰 北京航空航天大学 博士 IEEE TVT 审稿人。
  1. 邓伟文 北京航空航天大学 教授
  1. 张汝正 清华大学 博士
  1. 王英 吉林大学 副教授
  1. 赵睿 北京航空航天大学 助理教授
  1. 丁娟 吉林大学 博士

***

## 研究背景与目的

- 目前自动驾驶技术发展迅速，但在复杂的交互场景下行驶仍然是自动驾驶面临的主要挑战。
- 其中最具挑战性的场景之一是汇入场景，一般情况下，汇入车辆需要在目标车道上选择合适的间隙，并在当前车道结束前完成汇入行为。
- 由于强烈的交互作用，自主驾驶车辆需要表现得像人类驾驶员一样，以使其行为对人类驾驶的交通车辆具有可预测性。此外，类人驾驶风格的自主车辆可以显著减少乘客在这些具有挑战性的场景中接管车辆的频率。
- 汇入场景的决策和规划
  - 在早期的研究中，基于规则的方法、马尔可夫过程和效用理论模型被用来解决场景中的决策和规划问题。 这些方法没有考虑自我车辆与其他交通车辆之间的交互，这违背了人类驾驶员的驾驶习惯，不礼貌且不安全。
  - 为了反映这种相互作用，一些研究用自我车辆的固定轨迹来预测其他交通车辆的未来轨迹。然而，这些开环预测方法忽略了本车对其他交通车辆的影响。
  - 强化学习（RL）被用于通过设置考虑自我车辆对其他交通车辆的影响的训练环境来对交互进行建模。尽管 RL 潜力巨大 ，但在可预测性和安全性方面仍面临重大挑战。
  - 博弈论是另一种流行的方法，用于模拟变道和并线场景中的交互作用，然而，如何确保游戏参与者手工创建的奖励函数的准确性是博弈论的主要挑战之一。
  - 本文采用的反向强化学习可以通过从专家演示中学习专家使用的奖励函数来解决这一挑战。
- 具有人类驾驶风格的自动驾驶决策
  - 如果 ADV 表现出与人类相似的驾驶行为，人类驾驶的交通车辆就会更容易与之互动，乘客也会更加信任它。
  - 文献 [27] 结合深度自动编码器网络和 XGBoost 算法，提出了基于行为克隆的类人变道决策模型。
  - [28] 还采用了基于博弈论的行为克隆来模仿人类的变道行为。
  - 行为克隆
    - 行为克隆（Behavior Cloning, BC）是一种基于模仿学习（Imitation Learning）的算法，旨在通过模仿专家的行为来训练模型。
    - 具体来说，行为克隆通过从专家演示的行为数据中直接学习一个决策策略，将其视为一个标准的监督学习问题。其核心思想是使用专家的状态-动作对（state-action pairs）作为训练数据，学习一个能够在相似状态下做出与专家行为相同决策的策略模型。
    - 行为克隆将模仿学习问题转化为一个监督学习问题，并且只需要专家数据，无需明确定义奖励函数或动态模型
    - 但行为克隆从静态的专家数据中学习，它可能在测试环境中偏离专家的行为轨迹。一旦模型偏离轨迹，可能会在未来的决策中引入更多的错误，导致误差逐渐积累。并且行为克隆模型通常只能在训练数据分布内很好地工作，对于未见过的状态，它可能无法做出合理的决策。
- 逆强化学习（Inverse Reinforcement Learning, IRL）
  - 与行为克隆直接模仿专家行为以及传统强化学习假设奖励函数已知不同，逆强化学习的目标是通过观察专家行为逆向推导出隐含的奖励函数，并利用该奖励函数学习合理的策略。
  - IRL 能够自动从专家行为中推断出奖励函数，避免了手动设计奖励函数的复杂性。并且通过推断奖励函数，IRL 能够解释专家行为背后的动机或目标。这使得它比直接模仿专家行为的行为克隆更具解释力。
  - 逆强化学习的典型算法：
    - 最大边际 Max-margin IRL：最大化边际的逆强化学习，通过最大化专家演示与其他可能策略之间的差异来推断奖励函数。
    - 最大熵 Maximum entropy IRL：最大熵逆强化学习，通过最大化策略的熵，推导出更通用的奖励函数，能够处理奖励函数的多解性问题。
      - 最大熵 IRL 可以解决奖励函数模糊的问题，相对于最大边际方法得到了更广泛的应用
    - 基于特征 Feature-based IRL：基于特征的逆强化学习，通过定义状态的特征向量，学习奖励函数作为这些特征的加权和。
  - 近年来，**深度逆强化学习**（Deep Inverse Reinforcement Learning, DIRL）逐渐成为研究热点。DIRL 结合了深度神经网络的强大表示能力，用深度学习模型来表示复杂的奖励函数和策略，尤其在高维连续状态和动作空间中表现良好。使得逆强化学习能够应用于更加复杂的场景。
  - 但因为状态和行动空间庞大且连续，成为 DIRL 训练的主要难题
- 具体工作
  - 本文提出了一种基于样本的 DIRL，用于规划类似 人类的合并轨迹，以解决因状态和行动空间庞大且连续而造成的 DIRL 解决难题。 
  - 为了考虑轨迹规划过程中的交互作用，在规划时会通过奖励网络对自我车辆和交通车辆的联合轨迹进行评估。当 FIRL 预测交通车辆的轨迹时，会考虑自我车辆的行为。 
  - 为了缩小规划阶段的解空间，在规划并线轨迹之前，利用决策模块选择最合适的并线间隙。

***

## 问题陈述

- 汇入场景
  ![image-20240929203740642](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240929203740642.png)
- 加速车道上的小车需要并入主车道，并且必须在到达加速车道终点之前完成操作
- 为了在计算成本和性能之间取得平衡，将重点放在周围 5 辆交通车辆上，其中 V1、V2、V3 和 V4 构成三个间隙。
- 为了在描述并线场景状态时保持状态数量的一致性，我们为补充运载的车辆分配到了虚拟状态，分配虚拟状态的原则是，确保虚拟交通车辆的存在不会影响本车辆的汇入行为
  - 如果 V1 或 V2 不存在，则它们的虚拟相对距离和速度分别为-100m 和-30 m/2。
  - 如果不存在 V3、V4 或 V5，则将其虚拟相对距离和速度分别指定为 100 m 和 30 m/s
  - V2 和 V3 代表目标车道上相对于自我车辆最近的跟随车辆和领先车辆，V2 或 V3 不存在则 V1、V4 也一定不存在
- 自车汇入的间隙（gap）被称为“目标间隙（target gap）”。
- 为了减小规划轨迹的解空间，首先利用决策模块选择最合适的合并间隙作为目标间隙，然后在目标间隙的解空间内规划最优轨迹。通过这种方式，规划算法的解空间可以减少到其原始大小的三分之一。
- 当自车汇入主车道时，主要影响的是目标间隙中的后方车辆。因此，本文只考虑自车与目标间隙后方车辆之间的交互。

***

## 基于深度逆强化学习的交互感知决策与规划方法

- 基于采样的深度逆强化学习

  - 根据最大熵原理，选择轨迹的概率与其回报的自然指数成正比（分区函数）
    $P(\tau|\theta)=\frac{e^{R(\tau|\theta)}}{Z(\theta)}$
    $Z(\theta)=\int_{D}e^{R(\tau|\theta)}d\tau $
    - $P(\tau|\theta)$ 表示轨迹 $\tau$ 被选择的概率，R 是轨迹 $\tau$ 的奖励，$\theta $​ 为奖励函数测参数，D 是智能体可以选择的所有可能轨迹的集合
  - 连续且巨大的状态空间使得合并场景中的分区函数的计算变得困难。通过在状态空间中对候选轨迹进行采样来近似分区函数：$Z(\theta)\approx\sum_{\tau_i\in\phi}e^{R(\tau_i|\theta)},$
    - $\phi$​ 表示所有采样的候选轨迹。

  - 所以轨迹选择概率近似为
    $P(\tau|\theta)\approx\frac{e^{R(\tau|\theta)}}{\sum_{\tau_i\in\phi}e^{R(\tau_i|\theta)}}.$
    - 在集合 $\phi$ （即采样得到的候选轨迹集合）中的每一条轨迹，都是从与轨迹 $\tau$ 相同的状态开始的
  - 最大熵逆强化学习的目标是通过调整奖励函数的参数 θ 来最大化专家演示轨迹的可能性：
    $\theta^*=\arg\max_\theta\sum_{\tau_e\in E}\log P(\tau_e|\theta)$
    - E 表示所有专家演示轨迹
  - 则目标函数为
    $\begin{aligned}
    J(\theta)& =\sum_{\tau_e\in E}\log P(\tau_e|\theta) \\
    &=\sum_{\tau_e\in E}\left [R(\tau_e|\theta)-\log\sum_{\tau_i\in\phi_e}e^{R(\tau_i|\theta)}\right]
    \end{aligned}$
- 交互感知规划方法

  - 规划具有人类驾驶的轨迹，以便并入决策模块选定的目标间隙
  - 将目标间隙后面的车称为 FVTG，前面的车称为 LVTG
  - 规划算法遵循“抽、评估、选择”的框架
    ![image-20240930103624137](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240930103624137.png)
    - 采样：
      - 对满足安全约束的候选合并轨迹进行采样（框 2 所示）
      - 预测每个候选轨迹对应的交通车辆的轨迹（框 2 所示）
      - 在对候选轨迹进行采样时，首先采用基于“高阶驱动意向”的轨迹表示方法对合并轨迹进行降维处理，然后在低维轨迹空间中对候选轨迹进行采样。 
    - 评估：(框 3 所示)
      - 基于 DIRL 从自然驾驶数据中学习到的奖励网络，对自车和周围交互车辆的联合轨迹进行评估
      - 自车轨迹经过卷积层处理，交通车辆轨迹则经过线性层处理。它们在拼接后被输入到 MLP 层，最终输出联合奖励。
      - 在汇入场景中，所考虑的交通车辆是目标间隙后方车辆（FVTG）
    - 选择：从候选轨迹中选择最优轨迹（框 4 所示）
  - 汇入轨迹可以分为两个阶段
    ![image-20240930110610447](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240930110610447.png)
    - 第一阶段从 t0 到 t1（用绿线表示），为纵向调整阶段，小车根据目标间隙调整其纵向位置和速度。 
    - 第二阶段从 t1 到 t2（用红线表示），自车在第二阶段完成变道操作。
  - 人类驾驶员的驾驶行为是受其高层意图支配的。相比于对低级驾驶意图做出决策（比如每个时间 步的加速），对高级驾驶意图做出决策可以大大减少状态空间，使轨迹更加平滑。因此，合并轨迹是由高级驾驶意图生成的。
  - S_L 坐标系
    - S 轴：沿着道路的中心线的纵向距离，表示小车沿道路前进的距离。
    - L 轴：垂直于道路中心线的横向偏移，表示小车相对于道路中心线的偏离程度。
  - 第一阶段纵向调整中，高层驾驶意图包括自我车辆的目标纵向位置 $S_e(t_1)$ 和速度 $v_e(t_1)$，以及完成纵向调整机动所需的时间 $T_1=t_1 - t_0$​
    - 设置初始状态 $S_e(t_0)、v_e(t_0)、a_e(t_0)$ 以及目标状态 $S_e(t_1)、v_e(t_1)、a_e(t_1)=0$，利用五次多项式拟合自动驾驶车辆在 S_L 坐标系下第一阶段的纵向位置、速度、加速度（五次多项式能够保证轨迹的平滑性和连续性）
      $S_{e}(t)=b_5t^5+b_4t^4+b_3t^3+b_2t^2+b_1t+b_0\\v_{e}(t)=5b_5t^4+4b_4t^3+3b_3t^2+2b_2t+b_1\\a_{e}(t)=20b_{5}t^{3}+12b_{4}t^{2}+6b_{3}t+2b_{2}.$
      $\begin{cases}b_5=6\frac{S_e(t_1)-S_e(t_0)}{t_1^5}-3\frac{v_e(t_1)+v_e(t_0)}{t_1^4}+\frac{a_e(t_1)-a_e(t_0)}{2t_1^3}\\b_4=15\frac{S_e(t_0)-S_e(t_1)}{t_1^4}+\frac{7v_e(t_1)+8v_e(t_0)}{t_1^3}+\frac{3a_e(0)-2a_e(t_1)}{2t_1^2}\\b_3=10\frac{S_e(t_1)-S_e(t_0)}{t_1^3}-\frac{4v_e(t_1)+6v_e(t_0)}{t_1^2}+\frac{a_e(t_1)-3a_e(t_0)}{2t_1}\\b_2=\frac{a_e(t_0)}{2}\\b_1=v_e(t_0)\\b_0=S_e(t_0)\end{cases}$
  - 在第二阶段，五次多项式也被用来拟合车道变换的横向轨迹。S-L 坐标系中的 L 坐标表示为：
    $L(t)=c_5t^5+c_4t^4+c_3t^3+c_2t^2+c_1t+c_0.$
    - 给定边界条件 $(L(t0), \dot{L}(t0), \ddot{L}(t0),L(t1), \dot{L}(t1), \ddot{L}(t1))$ 计算用于车道变换的横向轨迹多项式的系数。
  - 通过采样高层驾驶意图 $\{T_1,S_e(t_1),v_e(t_1)\}$，通过计算两个阶段的轨迹方程来采样轨迹，采样中任何导致碰撞的候选轨迹将被移除。
    - 所需时间 T1 的采样范围为 [0, 10] 秒，采样间隔为 1 秒。
    - 目标速度 $v_e(t_1)$ 的采样范围为 $[v_g − 2, v_g + 2]m/s$ ，采样间隔为 0.5 m/s
      -  $v_g$ 是目标间隙前方车辆 FVTG 和后方车辆 LVTG 的速度中最接近自车的车辆的速度。
    - 目标纵向位置 $S_e(t_1)$ 的采样范围为 $[S_{gf}, S_{gl}]$ 米，采样间隔为 2 米
      - 其中 $S_{gf}$ 和 $S_{gl}$ 分别是 t1 时刻 FVTG 和 LVTG 的位置。
  - 为了提高逆强化学习的拟合能力，采用奖励网络来表示规划中的人类使用的奖励函数。
    ![image-20241002193824910](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20241002193824910.png)
    - 为了合理地与交通车辆进行交互，将 FVTG 轨迹的奖励与自我车辆的候选轨迹一起送入奖励网络。
    - 合并轨迹输入到卷积神经网络中提取器特征，合并轨迹可以由具有两个维度的矩阵表示：时间和状态。
    - 自我车辆轨迹的状态包括自我车辆的速度、加速度、横向位置以及与 FVTG、LVTG 和加速车道上的领先车辆（V5）的相对距离和速度
  - 基于采样的 DIRL 对自车与交通车辆的联合轨迹训练奖励函数
    ![image-20241002194715386](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20241002194715386.png)
    1. 以与专家演示轨迹相同的初始状态对自我车辆的候选轨迹集进行采样。（如图 5 中的框“1”和“2”所示）
    2. 本车的候选轨迹和 FVTG 的未来轨迹的奖励被馈送到奖励网络中。（如图 5 中的框“2”、“3”、“4”和“5”所示）
    3. 根据奖励网络输出的奖励，得到候选轨迹被选中的概率，使用梯度上升算法更新回报网络的参数（最大化专家轨迹的概率）。（如图 5 中的方框“6”和“7”所示）
    4. 为了快速收敛，线性层的参数通过基于特征的逆强化学习方法确定，而不与卷积层和 MLP 中的参数一起训练。
- 交互
  - 在汇入场景中，自我车辆主要意识到它们的行为对目标间隙（FVTG）的后续车辆的影响（是否会给 FVTG 带来额外的不便）。
  - 为了明确量化这种影响，一些研究人员通过 IDM 预测 FVTG 的轨迹，并基于其速度损失量化自我车辆对 FVTG 的影响。然而由于模型参数较少，IDM 模型驾驶行为的能力是有限的。
  - 同时，仅采用 FVTG 的速度损失来量化本车对 FVTG 的影响是不全面的。
  - 因此本文利用基于特征的逆强化学习（FIRL）来预测 FVTG 的轨迹。FIRL 从驾驶数据中学习 FVTG 的内部奖励函数，并找到最大化奖励函数的轨迹作为预测轨迹。
  - FIRL 学习的奖励函数是由多个驱动特征组成的线性函数。汇入场景被视为 FVTG 领先车辆突然变化的汽车跟随场景。FVTG 能够识别本车的换道意图，并在本车使用其转向信号或历史轨迹来发出换道信号时将其视为新的领先车辆。在这种情况下，FVTG 的驱动特征选择如下：
    - 加速度：$f_1(s)=acc^2$
    - 与自车的相对速度：$f_2(s)=v_r^2$
    - 与自车的相对距离：$f_3(s)=\left(d-d_{des}\right)^2$​
      - $d_{des}$ 表示到与自车的预期相对距离。FVTG 和 LVTG 之间的距离被认为是预期的相对距离（本车切入前的原始相对距离）。
    - 碰撞罚款：$f_4(s)=\left(\frac{1}{d}\right)^2$
    - 奖励函数：$r=-(\theta_1f_1+\theta_2f_2+\theta_3f_3+\theta_4f_4)$
  - 训练过程
    1. 初始化奖励函数 $\theta$ 的参数；
    2. 计算所有演示的经验特征向量平均值 $\tilde{\mathbf{f}}=\frac{1}{N}\sum_{\tau\in E}\mathbf{f}_{\tau}$；
    3. 基于当前奖励函数求解最优轨迹 $\tau$；
    4. 计算最优轨迹的特征 $\mathbf{f}_{\tau}$；
    5. 计算奖励函数 $\mathbf{g}=\mathbf{f}_{\tau}-\mathbf{\tilde{f}}$ 的梯度，根据梯度更新奖励函数的参数；
    6. 从 3 开始重复，直到收敛。
  - 在给定奖励函数的情况下，采用 MPC（模型预测控制）规划 FVTG 的最优轨迹。
- 主动决策方法
  - 为了减少规划阶段的采样空间，在规划阶段之前，决策模块被用来选择最合适的间隙进行合并。
  - 与通过放慢速度等待适当的合并间隙出现的被动决策方法相比，我们提出了一种主动决策方法。主动决策算法也遵循“采样、评估、选择”框架，如图 6 中的“Q 网络在线决策”所示。
  - 采样：
    ![image-20240929203740642](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240929203740642.png)
    - 自我车辆从三个附近的间隙中选择目标间隙。如果没有找到合适的间隙，自我车辆将加速或减速以搜索合适的合并间隙。
    - 因此，主动决策方法具有五个候选动作：“并入间隙 1”、“并入间隙 2”、“并入间隙 3”、“加速以搜索目标间隙”和“减速以等待目标间隙”。
  - 评估：采用 Q 网络对候选动作进行评估，输入 Q 网络的状态包括自我车辆的速度、加速度、到加速车道终点的距离，以及自我车辆与周围 5 辆交通车辆的相对距离和速度。
  - 选择：选择 Q 值最高的动作作为最优动作。
  - 在使用 DIRL 对 Q 网络进行离线训练期间，训练 Q 网络的目标是使 Q 网络选择的动作与专家在相同驾驶状态下选择的动作尽可能一致。

# 实验分析

- 数据集与预处理：

  - 用于决策的 Q 网络和用于规划的奖励网络都是用自然驾驶数据训练的
  - 使用 NGSIM 数据集中美国 101 号公路（NGSIM US 101）上午 7:50 - 8:35 的交通数据用于训练网络，数据集中车辆状态的采样频率为 10Hz。
  - NGSIM US 101 数据集的记录区域由五条主车道（车道 1-5）、一条加速车道（车道 6）和两条辅助车道（车道 7 和车道 8）组成。
    ![image-20241003100640101](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20241003100640101.png)
    - 图中的红色区域为汇入轨迹，加速车道上的汇入车辆首先选择合适的间隙，然后并入主车道。我们从 NGSIM US 101 数据集中提取了 143 个合并轨迹。
  - 采用 SavitzkyGolay 滤波器对 NGSIM 数据进行 2 秒窗口平滑
    - Savitzky-Golay 滤波器的原理是：在每个滑动窗口中，利用 **多项式回归** 来拟合窗口内的数据点，然后用拟合后的多项式值替代中心点的数据值

- 决策 Q 网络

  - 模型训练伪代码
    ![image-20241003101854301](C:\Users\28159\AppData\Roaming\Typora\typora-user-images\image-20241003102243316.png)

  - 对比实验
    ![image-20241003101854301](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20241003101854301.png)
    - 将决策模型的准确率定义为 5 个决策动作的平均召回率。
    - 使用监督学习的 Softmax 分类模型作为基线

- 规划奖励网络

  - 模型训练
    ![image-20241003102441496](C:\Users\28159\AppData\Roaming\Typora\typora-user-images\image-20241003102441496.png)
  - 训练过程
    ![image-20241003102557095](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20241003102557095.png)

- 总体模型评价

  - 专家示范轨迹的平均对数似然和最终位移误差（FDE）被选为模型的评价指标。
    - 由于我们的模型是一个概率模型，我们将最终位移误差定义为所有候选轨迹的最终位移误差的期望
  - 训练数据集和测试数据集的比例为 9：1
    ![image-20241003103951850](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20241003103951850.png)

- 案例研究
  - 从测试集中选取了三个案例进行测试，以直观地验证所提出模型的可行性和有效性。
  - 在每个案例中，自车由所提出的方法控制，而其他交通车辆则遵循它们在 NGSIM 数据中的原始轨迹。
  - 上述测试方法称为日志模拟测试方法（log-sim testing method），该方法能够将决策与人类驾驶员的决策进行比较，在学术界和工业界广泛应用。
  - 所有驾驶场景均在 Python 上建立和实现
  - 案例研究1：通过减速并入车道
    ![image-20241003105503335](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20241003105503335.png)
    - 橙子方框表示由所呈现的模型控制的自我载体。
    - 紫色框是存在于US101交通数据集中的自我车辆的位置，该位置由人类驾驶员控制。
    - 由蓝框表示的主要交通车辆构成了目标间隙。
    - 其他交通工具，称为次要交通工具，由淡黄色方框表示。所有的交通车辆都遵循它们在US101交通数据集中的原始轨迹。当交通车辆离开本车的视野时，它将从图中移除
    - 分析场景可见，在 t=0 时，自车非常接近交通车辆2，因此无法安全地并入目标间隙。随后，自车首先减速，以增加与车辆2的相对距离。最终，自车在与车辆1和车辆2保持适当的相对距离后，成功并入目标间隙。在整个并入过程中，模型控制下的自车轨迹与人类驾驶员的轨迹非常接近，FDE仅为2.56米。
  - 案例研究2：通过加速并入车道
  - 案例研究2：立即并入车道
- 相互作用因子的影响
  - 将本车轨迹对其他车辆的影响作为相互作用因子。
  - FIRL学习的奖励函数被用来量化自我车辆对FVTG的影响。自我车辆对FVTG的影响是奖励网络中线性层的输出。
  - 为了研究交互因子对模型精度的影响，比较了交互感知和交互不感知模型的平均FDE和平均似然。
  - 与从奖励网络中删除交互因子的交互感知模型进行对比
    ![image-20241003110732280](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20241003110732280.png)
    - 在奖励网络中删除交互因子会损害模型的准确性，这表明交互因子在规划类似人类的轨迹时很重要。
    - 由交互感知模型控制的自我车辆的行为比交互不感知模型更类似于人类驾驶员的行为。
  - 安全性指标对比
    ![image-20241003111030415](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20241003111030415.png)
    - 安全度量为自我车辆和FVTG之间的最小距离和最小碰撞时间（TTC）。
    - 实验结果表明，由交互感知模型控制的自我车辆的行为比交互不感知模型更安全。