# Interaction-Aware Planning With Deep Inverse Reinforcement Learning for Human-Like Autonomous Driving in Merge Scenarios
# 融合场景下基于深度逆强化学习的类人自动驾驶交互感知规划

[TOC]

## 摘要

- 在高速公路汇入场景中，车辆需要选择适当的目标间隙并在加速车道结束前完成汇入。这一过程中，自主驾驶车辆（ADV）与其他交通车辆的相互影响非常强烈，且存在多种不确定性。因此，如何让自主驾驶车辆表现得像人类司机一样，能够与周围的人类驾驶车辆进行有效的互动，成为了一个重要课题。这种互动不仅可以提高驾驶的安全性，还能减少乘客接管车辆的频率。针对以上问题，本文提出了一种基于深度逆强化学习的交互感知决策与规划方法，用于并线场景下的实现具有人类驾驶风格的自动驾驶，该方法通过学习人类驾驶员的奖励函数，提升了规划的可解释性和泛化能力。与传统的预测方法不同，本文模型不仅考虑了自主驾驶车辆的行为，还通过交互预测其他交通车辆的轨迹。通过决策模块选择合适的汇入间隙，进一步减少了规划的计算复杂度。
- 关键词：深度逆向强化学习、类人化的（Human-like）、交互、汇入场景、规划
- 期刊：IEEE Transactions on Intelligent Vehicles (IEEE TIV). SCI Q1
- 作者：
  1. 南江峰 北京航空航天大学 博士 IEEE TVT 审稿人。
  1. 邓伟文 北京航空航天大学 教授
  1. 张汝正 清华大学 博士
  1. 王英 吉林大学 副教授
  1. 赵睿 北京航空航天大学 助理教授
  1. 丁娟 吉林大学 博士

***

## 研究背景与目的

- 目前自动驾驶技术发展迅速，但在复杂的交互场景下行驶仍然是自动驾驶面临的主要挑战。
- 其中最具挑战性的场景之一是汇入场景，一般情况下，并道车辆需要在目标车道上选择合适的间隙，并在当前车道结束前完成并道行为。
- 由于强烈的交互作用，自主驾驶车辆需要表现得像人类驾驶员一样，以使其行为对人类驾驶的交通车辆具有可预测性。此外，类人驾驶风格的自主车辆可以显著减少乘客在这些具有挑战性的场景中接管车辆的频率。
- 汇入场景的决策和规划
  - 在早期的研究中，基于规则的方法、马尔可夫过程和效用理论模型被用来解决场景中的决策和规划问题。 这些方法没有考虑自我车辆与其他交通车辆之间的交互，这违背了人类驾驶员的驾驶习惯，不礼貌且不安全。
  - 为了反映这种相互作用，一些研究用自我车辆的固定轨迹来预测其他交通车辆的未来轨迹。然而，这些开环预测方法忽略了本车对其他交通车辆的影响。
  - 强化学习（RL）被用于通过设置考虑自我车辆对其他交通车辆的影响的训练环境来对交互进行建模。尽管 RL 潜力巨大 ，但在可预测性和安全性方面仍面临重大挑战。
  - 博弈论是另一种流行的方法，用于模拟变道和并线场景中的交互作用，然而，如何确保游戏参与者手工创建的奖励函数的准确性是博弈论的主要挑战之一。
  - 本文采用的反向强化学习可以通过从专家演示中学习专家使用的奖励函数来解决这一挑战。
- 具有人类驾驶风格的自动驾驶决策
  - 如果 ADV 表现出与人类相似的驾驶行为，人类驾驶的交通车辆就会更容易与之互动，乘客也会更加信任它。
  - 文献 [27] 结合深度自动编码器网络和 XGBoost 算法，提出了基于行为克隆的类人变道决策模型。
  - [28] 还采用了基于博弈论的行为克隆来模仿人类的变道行为。
  - 行为克隆
    - 行为克隆（Behavior Cloning, BC）是一种基于模仿学习（Imitation Learning）的算法，旨在通过模仿专家的行为来训练模型。
    - 具体来说，行为克隆通过从专家演示的行为数据中直接学习一个决策策略，将其视为一个标准的监督学习问题。其核心思想是使用专家的状态-动作对（state-action pairs）作为训练数据，学习一个能够在相似状态下做出与专家行为相同决策的策略模型。
    - 行为克隆将模仿学习问题转化为一个监督学习问题，并且只需要专家数据，无需明确定义奖励函数或动态模型
    - 但行为克隆从静态的专家数据中学习，它可能在测试环境中偏离专家的行为轨迹。一旦模型偏离轨迹，可能会在未来的决策中引入更多的错误，导致误差逐渐积累。并且行为克隆模型通常只能在训练数据分布内很好地工作，对于未见过的状态，它可能无法做出合理的决策。
- 逆强化学习（Inverse Reinforcement Learning, IRL）
  - 与行为克隆直接模仿专家行为以及传统强化学习假设奖励函数已知不同，逆强化学习的目标是通过观察专家行为逆向推导出隐含的奖励函数，并利用该奖励函数学习合理的策略。
  - IRL 能够自动从专家行为中推断出奖励函数，避免了手动设计奖励函数的复杂性。并且通过推断奖励函数，IRL 能够解释专家行为背后的动机或目标。这使得它比直接模仿专家行为的行为克隆更具解释力。
  - 逆强化学习的典型算法：
    - 最大边际 Max-margin IRL：最大化边际的逆强化学习，通过最大化专家演示与其他可能策略之间的差异来推断奖励函数。
    - 最大熵 Maximum entropy IRL：最大熵逆强化学习，通过最大化策略的熵，推导出更通用的奖励函数，能够处理奖励函数的多解性问题。
      - 最大熵 IRL 可以解决奖励函数模糊的问题，相对于最大边际方法得到了更广泛的应用
    - 基于特征 Feature-based IRL：基于特征的逆强化学习，通过定义状态的特征向量，学习奖励函数作为这些特征的加权和。
  - 近年来，**深度逆强化学习**（Deep Inverse Reinforcement Learning, DIRL）逐渐成为研究热点。DIRL 结合了深度神经网络的强大表示能力，用深度学习模型来表示复杂的奖励函数和策略，尤其在高维连续状态和动作空间中表现良好。使得逆强化学习能够应用于更加复杂的场景。
  - 但因为状态和行动空间庞大且连续，成为 DIRL 训练的主要难题
- 具体工作
  - 本文提出了一种基于样本的 DIRL，用于规划类似 人类的合并轨迹，以解决因状态和行动空间庞大且连续而造成的 DIRL 解决难题。 
  - 为了考虑轨迹规划过程中的交互作用，在规划时会通过奖励网络对自我车辆和交通车辆的联合轨迹进行评估。当 FIRL 预测交通车辆的轨迹时，会考虑自我车辆的行为。 
  - 为了缩小规划阶段的解空间，在规划并线轨迹之前，利用决策模块选择最合适的并线间隙。

***

# 问题陈述

- 汇入场景
  ![image-20240929203740642](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240929203740642.png)
- 加速车道上的小车需要并入主车道，并且必须在到达加速车道终点之前完成操作
- 为了在计算成本和性能之间取得平衡，将重点放在周围 5 辆交通车辆上，其中 V1、V2、V3 和 V4 构成三个间隙。
- 为了在描述并线场景状态时保持状态数量的一致性，我们为补充运载的车辆分配到了虚拟状态，分配虚拟状态的原则是，确保虚拟交通车辆的存在不会影响本车辆的并道行为
  - 如果 V1 或 V2 不存在，则它们的虚拟相对距离和速度分别为-100m 和-30 m/2。
  - 如果不存在 V3、V4 或 V5，则将其虚拟相对距离和速度分别指定为 100 m 和 30 m/s
  - V2 和 V3 代表目标车道上相对于自我车辆最近的跟随车辆和领先车辆，V2 或 V3 不存在则 V1、V4 也一定不存在
- 自车汇入的间隙（gap）被称为“目标间隙（target gap）”。
- 为了减小规划轨迹的解空间，首先利用决策模块选择最合适的合并间隙作为目标间隙，然后在目标间隙的解空间内规划最优轨迹。通过这种方式，规划算法的解空间可以减少到其原始大小的三分之一。
- 当自车汇入主车道时，主要影响的是目标间隙中的后方车辆。因此，本文只考虑自车与目标间隙后方车辆之间的交互。

# 基于深度逆强化学习的交互感知决策与规划方法

- 基于采样的深度逆强化学习

  - 根据最大熵原理，选择轨迹的概率与其回报的自然指数成正比（分区函数）
    $P(\tau|\theta)=\frac{e^{R(\tau|\theta)}}{Z(\theta)}$
    $Z(\theta)=\int_{D}e^{R(\tau|\theta)}d\tau $
    - $P(\tau|\theta)$ 表示轨迹 $\tau$ 被选择的概率，R 是轨迹 $\tau$ 的奖励，$\theta $​ 为奖励函数测参数，D 是智能体可以选择的所有可能轨迹的集合
  - 连续且巨大的状态空间使得合并场景中的分区函数的计算变得困难。通过在状态空间中对候选轨迹进行采样来近似分区函数：$Z(\theta)\approx\sum_{\tau_i\in\phi}e^{R(\tau_i|\theta)},$
    - $\phi$​ 表示所有采样的候选轨迹。

  - 所以轨迹选择概率近似为
    $P(\tau|\theta)\approx\frac{e^{R(\tau|\theta)}}{\sum_{\tau_i\in\phi}e^{R(\tau_i|\theta)}}.$
    - 在集合 $\phi$ （即采样得到的候选轨迹集合）中的每一条轨迹，都是从与轨迹 $\tau$ 相同的状态开始的
  - 最大熵逆强化学习的目标是通过调整奖励函数的参数 θ 来最大化专家演示轨迹的可能性：
    $\theta^*=\arg\max_\theta\sum_{\tau_e\in E}\log P(\tau_e|\theta)$
    - E 表示所有专家演示轨迹
  - 则目标函数为
    $\begin{aligned}
    J(\theta)& =\sum_{\tau_e\in E}\log P(\tau_e|\theta) \\
    &=\sum_{\tau_e\in E}\left [R(\tau_e|\theta)-\log\sum_{\tau_i\in\phi_e}e^{R(\tau_i|\theta)}\right]
    \end{aligned}$

- 交互感知规划方法

  - 规划具有人类驾驶的轨迹，以便并入决策模块选定的目标间隙
  - 将目标间隙后面的车称为 FVTG，前面的车称为 LVTG
  - 规划算法遵循“抽、评估、选择”的框架
    ![image-20240930103624137](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240930103624137.png)
    - 采样：
      - 对满足安全约束的候选合并轨迹进行采样（框 2 所示）
      - 预测每个候选轨迹对应的交通车辆的轨迹（框 2 所示）
      - 在对候选轨迹进行采样时，首先采用基于“高阶驱动意向”的轨迹表示方法对合并轨迹进行降维处理，然后在低维轨迹空间中对候选轨迹进行采样。 
    - 评估：(框 3 所示)
      - 基于 DIRL 从自然驾驶数据中学习到的奖励网络，对自车和周围交互车辆的联合轨迹进行评估
      - 自车轨迹经过卷积层处理，交通车辆轨迹则经过线性层处理。它们在拼接后被输入到 MLP 层，最终输出联合奖励。
      - 在汇入场景中，所考虑的交通车辆是目标间隙后方车辆（FVTG）
    - 选择：从候选轨迹中选择最优轨迹（框 4 所示）
  - 汇入轨迹可以分为两个阶段
    ![image-20240930110610447](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240930110610447.png)
    - 第一阶段从 t0 到 t1（用绿线表示），为纵向调整阶段，小车根据目标间隙调整其纵向位置和速度。 
    - 第二阶段从 t1 到 t2（用红线表示），自车在第二阶段完成变道操作。
  - 人类驾驶员的驾驶行为是受其高层意图支配的。相比于对低级驾驶意图做出决策（比如每个时间 步的加速），对高级驾驶意图做出决策可以大大减少状态空间，使轨迹更加平滑。因此，合并轨迹是由高级驾驶意图生成的。
  - 纵向调整中，高层驾驶意图包括自我车辆的目标纵向位置 $S_e(t_1)$ 和速度 $v_e(t_1)$，以及完成纵向调整机动所需的时间 $T_1=t_1 - t_0$

  