# DQN 尝试

- 使用 CartPole-v1 仿真环境

  - CartPole-v1 仿真环境
    ![image-20240914141110680](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240914141110680.png)

    - 状态空间

      - Cart Position：[-4.8, 4.8]
      - Cart Velocity：[-Inf, Inf]
      - Pole Angle：[-0.418 rad (-24°), 0.418 rad (24°)]
      - Pole Angular Velocity：[-Inf, Inf]

    - 动作空间：

      - 0: Push cart to the left
      - 1: Push cart to the right

    - 结束条件：

      - Termination：Pole Angle is greater than ±12°
      - Termination：Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)
      - Truncation：Episode length is greater than 500

    - 奖励函数

      - CartPole-v1 中默认的奖励为模拟一步获得奖励 1

      - 自定义奖励函数
        ```Python
        r = 1 - abs(s_[0]) / 2.4 + 1 - abs(s_[1]) / 1.2 - done * 10 + truncated * 10
        ```

        - 鼓励小车远离边界
        - 鼓励杆子角度小于 12
        - 游戏结束（角度或位置大于限制）获得惩罚
        - 完成游戏（模拟次数大于限制）获得奖励

  - DQN 实现

    - agent 类
      ```Python
      class Agent:
          def __init__(self, n_input, n_output, device, use_prirememo) -> None:
              self.n_input = n_input # 状态维度
              self.n_output = n_output # 动作维多
      
              self.GAMMA = 0.99
              self.lr = 0.001
      
              self.use_prirememo = use_prirememo # 是否使用优先经验回放
      
              if use_prirememo:
                  self.memo = PrioritizedReplayMemory(n_input, n_output, device)
              else:
                  self.memo = Replaymemory(n_input, n_output, device)
      
              self.online_net = DQN(n_input, n_output, device).to(device) # 在线网络
              self.target_net = DQN(n_input, n_output, device).to(device) # 目标网络
      
              self.optimizer = t.optim.Adam(self.online_net.parameters(), lr=self.lr)
      ```
      
    - Replaymemory 类
    
      ```python
      class Replaymemory:
          def __init__(self, n_s, n_a, device):
              self.n_s = n_s
              self.n_a = n_a
              self.device = device
              self.MEMORY_SIZE = 1000 # 经验池大小
              self.BATCH_SIZE = 64
              
              # 申请空间
              self.all_s = np.empty(shape=(self.MEMORY_SIZE, self.n_s), dtype=np.float32)
              self.all_a = np.empty(shape=self.MEMORY_SIZE, dtype=np.int64)
              self.all_r = np.empty(shape=self.MEMORY_SIZE, dtype=np.float32)
              self.all_done = np.empty(shape=self.MEMORY_SIZE, dtype=np.int64)
              self.all_s_ = np.empty(shape=(self.MEMORY_SIZE, self.n_s), dtype=np.float32)
      
              self.idx_memo = 0
              self.max_idx = 0
      
          def add(self, s, a, r, done, s_): # 添加新经验
              self.all_s[self.idx_memo] = s
              self.all_a[self.idx_memo] = a
              self.all_r[self.idx_memo] = r
              self.all_done [self.idx_memo] = done
              self.all_s_[self.idx_memo] = s_
      
              self.idx_memo = (self.idx_memo + 1) % self.MEMORY_SIZE # 循环添加
              self.max_idx = max(self.max_idx, self.idx_memo)
      
      
          def sample(self): # 随机抽样
              if self.max_idx < self.BATCH_SIZE:
                  batch_indices = range(self.max_idx)
              else:
                  batch_indices = np.random.choice(range(self.max_idx), self.BATCH_SIZE)
      
              batch_s = t.tensor(self.all_s[batch_indices], dtype=t.float32).to(self.device)
              batch_a = t.tensor(self.all_a[batch_indices], dtype=t.int64).unsqueeze(-1).to(self.device)
              batch_r = t.tensor(self.all_r[batch_indices], dtype=t.float32).unsqueeze(-1).to(self.device)
              batch_done = t.tensor(self.all_done[batch_indices], dtype=t.int64).unsqueeze(-1).to(self.device)
              batch_s_ = t.tensor(self.all_s_[batch_indices], dtype=t.float32).to(self.device)
      
              return batch_s, batch_a, batch_r, batch_done, batch_s_
      ```
    
    - DQN
      ```Python
      class DQN(nn.Module):
          def __init__(self, n_input, n_output, device):
              super().__init__()
      
              self.device = device
              self.net = nn.Sequential( # 使用两个线性层组成网络，输入为状态输出为动作id
                  nn.Linear(in_features=n_input, out_features=128),
                  nn.Tanh(),
                  nn.Linear(in_features=88, out_features = n_output)
              )
      
          def forward(self, x):
              return self.net(x)
          
          def act(self, obs): # 根据输入状态，求出所有动作的Q值，并选择Q值最大的动作返回
              obs_tensor = t.tensor(obs, dtype=t.float32).to(self.device)
              q_value = self(obs_tensor.unsqueeze(0))
              max_q_idx = t.argmax(q_value)
      
              action = max_q_idx.detach().item()
      
              return action
      ```
    
    - trian
      ```Python
      env = gym.make("CartPole-v1")
      s = env.reset()[0] # 初始化环境
      
      n_episode = 1000
      n_time_step = 501 # CartPole-v1 限制模拟次数为500
      n_input = len(s)
      n_output = env.action_space.n
      
      for episode in range(n_episode):
          episode_reward = 0
          for step in range(n_time_step):
              epsilon = np.interp(episode * n_time_step + step, [0, EPSILON_DECAY], [EPSILON_START, EPSILON_END]) # ε-Greedy探索环境，并随模拟次数递减
      
              if random.random() <= epsilon:
                  a = env.action_space.sample()
              else:
                  a = agent.online_net.act(s)
      
              s_, r, done, truncated, info = env.step(a) 
              # 执行动作获得 下一时间步状态、奖励、是否结束、是否完成、其他信息
      
              agent.memo.add(s, a, r, done, s_)
              s = s_
              episode_reward += r
      
              if done or truncated:
                  break
      		
              # 是否为优先经验回放
              if agent.use_prirememo:
                  batch_s, batch_a, batch_r, batch_done, batch_s_, batch_weights, batch_indices = agent.memo.sample()
              else:
                  batch_s, batch_a, batch_r, batch_done, batch_s_ = agent.memo.sample()
      
                  
              target_q_values = agent.target_net(batch_s_)
              max_target_q_values = target_q_values.max(dim=1, keepdim=True)[0]
              targets = batch_r + agent.GAMMA * (1 - batch_done) * max_target_q_values
      
              q_values = agent.online_net(batch_s) # 得到状态s下采取所有动作对应的q值
              a_q_values = t.gather(input=q_values, dim=1, index=batch_a) # 取出经验中对应的 Q(s,a)
      
              # 根据是否为优先经验回放，进行优先级更新和损失计算
              if agent.use_prirememo:
                  td_errors = (targets - a_q_values).detach().cpu().numpy()
      
                  agent.memo.update_priorities(batch_indices, td_errors)
      
                  loss = (batch_weights * nn.functional.smooth_l1_loss(targets, a_q_values)).mean() # 根据重要性采样权重，计算加权损失
              else:
                  loss = nn.functional.smooth_l1_loss(targets, a_q_values).mean()
      
              agent.optimizer.zero_grad()
              loss.backward()
              agent.optimizer.step()
      
          s = env.reset()[0]
          REWARD_BUFFER[episode] = episode_reward
          TRUNCATED_BUFFER[episode] = truncated
      
          if episode % TARGET_UPDATE_FRE == 0:
              agent.target_net.load_state_dict(agent.online_net.state_dict())
      
              print(f"Episode: {episode}, Avg Reward {np.mean(REWARD_BUFFER[min(episode-9, 0):episode+1])}, Trn Num {np.sum(TRUNCATED_BUFFER[max(episode-9, 0):episode+1])}")
      
          if np.sum(TRUNCATED_BUFFER[max(episode-99, 0):episode+1]) >= 95:
              break
      
      t.save(agent.online_net.state_dict(), 'DQN_OR_online_net.pth')
      print("Model parameters saved.")
      ```
    
    - 可视化
      ```Python
      env = gym.make("CartPole-v1", render_mode="human") # 图像化界面渲染
      s = env.reset()[0]
      i = 0
      while True:
          a = online_net.act(s)
          s_, r, done, truncated, info = env.step(a)
          s = s_
          i = i + 1
      
          env.render() 
          time.sleep(0.01)
      
          if done:
              print(i, "done")
              i = 0
              s = env.reset()[0]
          
          if truncated:
              print(i, "truncated")
              i = 0
              s = env.reset()[0]
      ```
    
  - 实验记录
  
    - 结束条件设为：`np.sum(TRUNCATED_BUFFER[max(episode-99, 0):episode+1]) >= 95:` 过去 100 轮中完成概率大于等于 95%
  
    - 训练：
  
      - DQN+OR：未达到终止条件
        ![image-20240914133530077](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240914133530077.png)
      - PRDQN+OR：未达到终止条件
        ![image-20240914135850876](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240914135850876.png)
  
      - DQN+CR 460Ep
        ![image-20240914133010579](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240914133010579.png)
  
      - PRDQN+CR 460Ep
        ![image-20240914132900579](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240914132900579.png)
        ![image-20240916214936737](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240916214936737.png)
    
    - 对比：使用保存的模型参数模拟 100 轮
      ![image-20240914142607167](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240914142607167.png)
  
- SUMO 环境尝试

  - 环境描述
    ![image-20240914154955197](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240914154955197.png)
    - 高速直行两车道，包含车辆 A 和车辆 B 两辆车，初始沿着同一车道（车道 0）直行。
    - 车辆 A 为前车，最高车速为 15m/s
    - 车辆 B 为后车，最高车速为 25m/s
    - 决策任务：车辆 B 通过换道至左侧车道超过 A 车。
    - 动作空间：{0：车道保持, 1：换道}
      - 底层采用 IDM 跟车模型与 SUMO 默认换道模型。
      - 变道指令 `traci.vehicle.changeLane(vehID, laneIndex, duration)`
        - 命令车辆尝试在指定的持续时间（duration）内改变到指定的车道。
        - 持续时间结束后，车辆会恢复正常的车道选择行为
    - 状态空间：车辆 B 和车辆 A 的状态 {x_a, y_a, speed_a, x_b, y_b, speed_b}。
    - 奖励函数：
      - 时间奖励：每个时间步给予对应惩罚
      - 目标车道奖励：鼓励车辆向目标车道靠近
        - 在该场景中，存在 0,1 两条车道，两车初始都位于车道 0，训练目标为完成超车
        - sumo 默认车道宽度为 3.2，使用 `-abs(-1.6-x)` 作为目标车道奖励
      - 速度奖励：
        - 设定车辆期望速度，鼓励车辆速度接近期望速度
      - 安全奖励： 
        - 发生碰撞添加惩罚
        - 未发生碰撞为 0
    - 终止条件：{1：完成超车, 2：任务超时, 3：发生碰撞}
  - 训练
    <img src="https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240916230032315.png" alt="image-20240916230032315" style="zoom:200%;" />
    - 完全跑不满？
    - sumo 不支持多线程
    - 尝试方式在 执行动作 -> 存储经验 时使用多线程，在 采样经验 -> 更新网络时单线程
  - 10000 episode 训练效果很差
    ![avg_reward_buffer](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/avg_reward_buffer.png)
    ![avg_loss_buffer](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/avg_loss_buffer.png)

- 论文复现-DQN

  - 环境描述
    ![\< img alt = "image-20240906180446991" data-attachment-key = "2JAJS6NL" width = "831" height = "121" src = "attachments/2JAJS6NL.png" ztype = "zimage" >](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/2JAJS6NL.png)
    - 高速三车道（0,1,2），包含不同驾驶风格的车辆
    - 状态空间：由本车状态和本车周围环境车辆状态两部分组成 $s_t=\begin{bmatrix}s_{te},s_{th}\end{bmatrix}$

      *   本车状态：$s_{te}=[loc_{xt},loc_{yt},v_{xt},v_{yt},acc_{t}]$

          *   横向位置、纵向位置、横向速度、纵向速度、加速度

      *   环境车辆状态：$s_{at}=[loc_{xt},loc_{yt},v_{xt},v_{yt},acc_{t},d_{i}]$

          *   周围第 i 辆车和本车的相对距离、周围第 i 辆车的纵向速度、周围第 i 辆车的加速度、周围第 i 辆车的横向位置
    - 动作空间：原问题为连续动作空间，这里先建模离散动作
      - $a=[a_1,a_2,a_3,a_4,a_5]$

        *   保持当前车道且匀速、保持当前车道且按加速度 $a_1$ 加速、保持当前车道且加速度 $a_2$ 减速、向左换道、向右换道
    - 奖励函数：
      - 安全奖励函数 $R_s$： $R_s=\begin{cases}R_{co}&\quad\text{发生碰撞}\\R_{nc}&\quad\text{当}d_i<d_{s_i}\end{cases}$
  
        *   碰撞惩罚值 $R_{co}=-\beta_1$
  
        *   临近碰撞奖励函数 $R_{nc}=-\delta / \left|d_{i\min}\right|$​
  
            *   $d_{S_i}$ 为安全距离，$d_{i\min}$ 表示自动驾驶车辆和周围环境车辆 i 之间的最小相对距离。
      - 速度奖励函数 $R_v=-\left|v_{y}-v_{\max}\right|$
      
        *   $v_y$ 表示车辆的纵向速度，$v_{\max}$​​ 表示道路允许的最大速度。
        *   修改：由于原奖励函数全为负值，导致 DQN 学习到的策略变为尽快寻求碰撞，将速度奖励调整为正值并加入到达终点奖励
  - DQN 学习到跟车策略，完全不变道，然而速度也并不高（其他车辆最高速度较低），说明奖励函数设置很有问题
    ![image-20241006222445703](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20241006222445703.png)

# 优先经验回放实现

- 使用 heapq

  - Python 的 `heapq` 模块提供了堆队列算法的实现。堆是一种特殊的树形数据结构，其中父节点的值总是小于或等于其子节点的值（最小堆）。通过列表来实现

  - 优先经验回放

    ```python
    class PrioritizedReplayMemory:
        def __init__(self, capacity, device, alpha=0.6, beta=0.4):
            self.MEMORY_SIZE = capacity
            self.device = device
            self.ALPHA = alpha  # α用于控制优先级对采样的影响
            self.BETA = beta
            self.EPSILON = 1e-5  # 防止优先级为零
    
            # 使用最小堆存储经验和优先级
            self.buffer = []
            self.max_priority = 1.0
            self.count = 0
    
        def add(self, s, a, r, done, s_):
            experience = (np.array(s), np.array(a), np.array(r), np.array(done), np.array(s_))
    
            if len(self.buffer) < self.MEMORY_SIZE:
                # 将经验和其优先级作为元组加入最小堆
                heapq.heappush(self.buffer, [self.max_priority, self.count, experience])
            else:
                # 更新已有最小优先级的经验
                heapq.heappushpop(self.buffer, [self.max_priority, self.count, experience])
    
            self.count += 1
    
        def sample(self, batch_size):
            # 根据优先级计算采样权重和概率
            priorities, _, experiences = zip(*self.buffer)
            self.max_priority = max(priorities)
            priorities = np.array(priorities, dtype=np.float32)
            priorities = priorities ** self.ALPHA
            sampling_probabilities = priorities / priorities.sum()
    
            batch_indices = np.random.choice(len(self.buffer), batch_size, p=sampling_probabilities, replace=False)
    
            # 获取样本和对应的采样权重
            experiences = [experiences[i] for i in batch_indices]
            weights = (len(self.buffer) * sampling_probabilities[batch_indices]) ** (-self.BETA)
            weights /= weights.max()  # 正规化
    
            batch_s, batch_a, batch_r, batch_done, batch_s_ = zip(*experiences)
            # 分解经验并转换为张量
            batch_s = t.tensor(np.array(batch_s), dtype=t.float32).to(self.device)
            batch_a = t.tensor(np.array(batch_a), dtype=t.float32).to(self.device)
            batch_r = t.tensor(np.array(batch_r), dtype=t.float32).unsqueeze(-1).to(self.device)
            batch_done = t.tensor(np.array(batch_done), dtype=t.int64).unsqueeze(-1).to(self.device)
            batch_s_ = t.tensor(np.array(batch_s_), dtype=t.float32).to(self.device)
            batch_weights = t.tensor(weights, dtype=t.float32).unsqueeze(-1).to(self.device)
    
            # print(batch_s.shape, batch_a.shape, batch_r.shape, batch_done.shape, batch_s_.shape, batch_weights.shape, batch_indices.shape)
    
            return batch_s, batch_a, batch_r, batch_done, batch_s_, batch_weights, batch_indices
    
        def update_priorities(self, batch_indices, td_errors):
            for i, idx in enumerate(batch_indices):
                # 重新计算并更新优先级，使用 heapq 的方法进行替换
                priority = abs(td_errors[i]) + self.EPSILON
                self.buffer[idx][0] = priority[0]
            heapq.heapify(self.buffer)  # 重新维护堆的结构
    
        def __len__(self):
            return len(self.buffer)
    ```

    

- 使用 sumTree

  - sumTree：

    - SumTree（求和树）是一种特殊的二叉树数据结构。它的主要特点是每个父节点的值等于其两个子节点值的和，而叶节点存储实际的优先级值。这种结构允许我们高效地执行基于优先级的采样和更新操作。

    ```python
    class SumTree:
        def __init__(self, capacity):
            self.capacity = capacity  # 样本的最大数量
            self.tree = np.zeros(2 * capacity - 1)  # 完全二叉树数组
            self.data = np.zeros(capacity, dtype=object)  # 存储经验
            self.write = 0  # 记录写入位置
            self.n_entries = 0  # 当前存储的经验数量
    
        def _propagate(self, idx, change):
            """递归更新树中的父节点"""
            parent = (idx - 1) // 2
            self.tree[parent] += change
    
            if parent != 0:
                self._propagate(parent, change)
    
        def _retrieve(self, idx, s):
            """递归查找叶子节点"""
            left = 2 * idx + 1
            right = left + 1
    
            if left >= len(self.tree):
                return idx
    
            if s <= self.tree[left]:
                return self._retrieve(left, s)
            else:
                return self._retrieve(right, s - self.tree[left])
    
        def total(self):
            """返回树的根节点值，即所有优先级的总和"""
            return self.tree[0]
    
        def add(self, p, data):
            """添加新的经验及其优先级"""
            idx = self.write + self.capacity - 1
    
            self.data[self.write] = data  # 存储经验
            self.update(idx, p)  # 更新叶子节点及其父节点
    
            self.write += 1
            if self.write >= self.capacity:
                self.write = 0  # 循环覆盖
    
            if self.n_entries < self.capacity:
                self.n_entries += 1
    
        def update(self, idx, p):
            """更新某个叶子节点的优先级，并递归更新父节点"""
            change = p - self.tree[idx]
            self.tree[idx] = p
            self._propagate(idx, change)
    
        def get(self, s):
            """根据给定的累积概率 s，找到对应的叶子节点"""
            idx = self._retrieve(0, s)
            dataIdx = idx - self.capacity + 1
    
            return (idx, self.tree[idx], self.data[dataIdx])
    ```

  - 优先经验回放
    ```python
    class PrioritizedReplayMemory_SumTree:
        def __init__(self, capacity, device, alpha=0.6, beta=0.4):
            self.MEMORY_SIZE = capacity
            self.device = device
            self.ALPHA = alpha  # α用于控制优先级对采样的影响
            self.BETA = beta
            self.EPSILON = 1e-5  # 防止优先级为零
    
            # 使用最小堆存储经验和优先级
            self.buffer = SumTree(capacity)
            self.max_priority = 1.0
            self.count = 0
    
        def add(self, s, a, r, done, s_):
            experience = (np.array(s), np.array(a), np.array(r), np.array(done), np.array(s_))
            priority = self.max_priority ** self.ALPHA  # 使用 alpha 调整优先级
    
            self.buffer.add(priority, experience)
    
        def sample(self, batch_size):
            batch = []
            idxs = []
            priorities = []
            segment = self.buffer.total() / batch_size
    
            for i in range(batch_size):
                s = random.uniform(segment * i, segment * (i + 1))
    
                idx, p, data = self.buffer.get(s)
    
                if data == 0:
                    continue
                batch.append(data)
                idxs.append(idx)
                priorities.append(p)
    
            sampling_probabilities = np.array(priorities) / self.buffer.total()
            # 计算重要性采样权重
            weights = (self.buffer.n_entries * sampling_probabilities) ** (-self.BETA)
            weights /= weights.max()  # 归一化
    
            # 分解经验并转换为张量
            batch_s, batch_a, batch_r, batch_done, batch_s_ = zip(*batch)
    
            batch_s = t.tensor(np.array(batch_s), dtype=t.float32).to(self.device)
            batch_a = t.tensor(np.array(batch_a), dtype=t.float32).to(self.device)
            batch_r = t.tensor(np.array(batch_r), dtype=t.float32).unsqueeze(-1).to(self.device)
            batch_done = t.tensor(np.array(batch_done), dtype=t.float32).unsqueeze(-1).to(self.device)
            batch_s_ = t.tensor(np.array(batch_s_), dtype=t.float32).to(self.device)
            batch_weights = t.tensor(weights, dtype=t.float32).unsqueeze(-1).to(self.device)
    
            return batch_s, batch_a, batch_r, batch_done, batch_s_, batch_weights, idxs
    
        def update_priorities(self, idxs, td_errors):
            for idx, td_error in zip(idxs, td_errors):
                priority = (abs(td_error) + self.EPSILON) ** self.ALPHA
                self.buffer.update(idx, priority)
                self.max_priority = max(self.max_priority, priority)
    
        def __len__(self):
            return self.buffer.n_entries
    ```
    
  - 示例：
    ![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/62638afd9dcc9f4226778de59a47f860.png)
  
- | 操作类型                             | heapq                       | SumTree                 |
  | :----------------------------------- | :-------------------------- | :---------------------- |
  | **添加经验 (`add`)**                 | `O(log N)`                  | `O(log N)`              |
  | **采样经验 (`sample`)**              | `O(N + batch_size * log N)` | `O(batch_size * log N)` |
  | **更新优先级 (`update_priorities`)** | `O(N)`                      | `O(batch_size * log N)` |

  - 问题是：**SumTree 没有实现加入新经验时替换已有最小优先级经验的**
    - 那为什么不直接用列表+random.sample
      - 添加经验：`O(1)`
      - 采样经验：`O(N + batch_size * log N1)`
      - 更新优先级：`O(batch_size)`
    - SumTree 可能会出现高优先级经验重复采样的问题
  - 训练时间对比：
    - 使用 deque 不使用优先经验回放（stable-baseline3 中的 DDPG 默认不使用，与自己实现的算法耗时接近）：
      ![image-20240928085325337](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240928085325337.png)
    - 使用 headq 实现优先经验回放（新经验替换最小优先级经验）
      ![image-20240928085446176](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240928085446176.png)
    - 使用 sumtree 实现优先经验回放（新经验循环加入）
      ![image-20240928085607381](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240928085607381.png)
    - 使用 deque 和 sample 方法实现优先经验回放（新经验循环加入）
      ![image-20241006190215611](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20241006190215611.png)
    - 使用 deque 实现是错误的选择，python deque 虽然支持索引访问但是底层仍是链表，随机访问非常耗时，在规模在 1e4 规模时差距可以忽略，在规模较大时其速度 << list，规模为 1e6 时随机索引一千万次，时间相差 60 倍
      ![image-20241006202606806](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20241006202606806.png)
    - 使用 list 和 sample 方法实现优先经验回放（新经验替换最小优先级经验）
      ![image-20241006215251980](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20241006215251980.png)


# DDPG 尝试

- 使用“Pendulum-v1”仿真
  - Pendulum-v1
    ![../../../_images/pendulum.gif](https://www.gymlibrary.dev/_images/pendulum.gif)

    - 状态空间：
      - cos(θ)、sin(θ)、$\omega$
      - 竖直为 0 度，顺时针方向来计算角度
    - 动作空间：
      - 施加在摆杆上的力矩（torque）。范围 [-2.0, 2.0]
    - 奖励函数：
      - $r=-(\theta^2 + 0.1 * \omega^2 + 0.001 * torque^2)$​
      - 当摆杆竖直静止，且不施加力矩时奖励函数达到最大值 0
      - 奖励函数的问题：对于角度奖励函数中计算方式为平方值，在角度较小时无法很好的指导 agent 进一步减小摆杆角度
    - 训练后的 agent 不断施加相同的力矩或来回施加不同方向的力矩来维持摆杆处于一个较小的角度
      ![a94ec745380efb1def19eed77e5315ec](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/a94ec745380efb1def19eed77e5315ec.png)

  - DDPG

    - actor 网络
      ```python
      class Actor_net(nn.Module):
          def __init__(self, state_dim, action_dim, hidden_dim=64):
              super().__init__()
              self.net = nn.Sequential(
                  nn.Linear(state_dim, hidden_dim),
                  nn.ReLU(),
                  nn.Linear(hidden_dim, hidden_dim),
                  nn.ReLU(),
                  nn.Linear(hidden_dim, action_dim),
                  nn.Tanh() # 动作区间为[-2,2]
              )
      
          def forward(self, x):
              return self.net(x) * 2
      ```

    - critic 网络
      ```python
      class Critic_net(nn.Module):
          def __init__(self, state_dim, action_dim, hidden_dim=64):
              super().__init__()
              self.net = nn.Sequential(
                  nn.Linear(state_dim + action_dim, hidden_dim),
                  nn.ReLU(),
                  nn.Linear(hidden_dim, hidden_dim),
                  nn.ReLU(),
                  nn.Linear(hidden_dim, 1)
              )
      
          def forward(self, x, a):
              return self.net(t.cat([x, a], dim=1))
      ```

    - 定义 agent
      ```python
      class Agent:
          def __init__(self, state_dim, action_dim, batch_size, use_pri_replay, device):
      	    ......
      
              self.online_actor_net = Actor_net(self.state_dim, self.action_dim).to(self.device)
              self.target_actor_net = Actor_net(self.state_dim, self.action_dim).to(self.device)
              self.target_actor_net.load_state_dict(self.online_actor_net.state_dict())
              self.target_actor_net.eval()
      
              self.online_critic_net = Critic_net(self.state_dim, self.action_dim).to(self.device)
              self.target_critic_net = Critic_net(self.state_dim, self.action_dim).to(self.device)
              self.target_critic_net.load_state_dict(self.online_critic_net.state_dict())
              self.target_critic_net.eval()
      
              self.optimizer_actor = t.optim.Adam(self.online_actor_net.parameters(), lr=self.lr_actor)
              self.optimizer_critic = t.optim.Adam(self.online_critic_net.parameters(), lr=self.lr_critic)
      
          def get_action(self, s, add_noise=True):
              self.online_actor_net.eval()
              with t.no_grad():
                  s = t.FloatTensor(s).unsqueeze(0).to(self.device)
      
                  a = self.online_actor_net(s).squeeze(0).cpu().detach().numpy()
                  a = a + add_noise * self.get_noise() # DDPG使用动作噪声代替ε-greedy来探索环境
              self.online_actor_net.train()
      
              a = np.clip(a, -2.0, 2.0)
      
              return a
      
          def get_noise(self):
              return np.random.normal(0.0, 0.2, size=self.action_dim)
      
          def update(self):
              if self.use_pri_replay:
                  batch_s, batch_a, batch_r, batch_done, batch_s_, batch_weights, batch_indices = self.replay_buffer.sample(self.batch_size)
              else:
                  batch_s, batch_a, batch_r, batch_done, batch_s_ = self.replay_buffer.sample(self.batch_size)
      
              # 更新 Critic 网络： 使用目标网络计算td_target。与在线网络输出值相减得到td_error
              next_actions = self.target_actor_net(batch_s_) # 使用目标网络获得下一时刻动作
      
              target_q = self.target_critic_net(batch_s_, next_actions.detach())  # 使用目标网络评估这些动作的Q值
              q_target = batch_r + self.GAMMA * target_q * (1 - batch_done)  # 计算td_target
      
              q_eval = self.online_critic_net(batch_s, batch_a) # 获取当前状态评估的Q值
      
              if self.use_pri_replay:
                  td_errors = (q_target - q_eval).detach().cpu().numpy()
      
                  self.replay_buffer.update_priorities(batch_indices, td_errors)
      
                  loss_critic = (batch_weights * nn.functional.huber_loss(q_eval, q_target, reduction='none')).mean()
              else:
                  loss_critic = nn.functional.huber_loss(q_eval, q_target, reduction='mean')
      
              self.optimizer_critic.zero_grad()
              loss_critic.backward()
              self.optimizer_critic.step()
      
              # 更新 Actor 网络：最大化输出动作的q值
              loss_actor = -self.online_critic_net(batch_s, self.online_actor_net(batch_s)).mean()
      
              self.optimizer_actor.zero_grad()
              loss_actor.backward()
              self.optimizer_actor.step()
      
              for target_param, param in zip(self.target_critic_net.parameters(), self.online_critic_net.parameters()):
                  target_param.data.copy_(self.TAU * param + (1 - self.TAU) * target_param)
      
              for target_param, param in zip(self.target_actor_net.parameters(), self.online_actor_net.parameters()):
                  target_param.data.copy_(self.TAU * param + (1 - self.TAU) * target_param)
      
              return loss_critic.item(), loss_actor.item()
      
          def save(self, path=""):
              t.save(self.online_actor_net.state_dict(), path)
      
              print("Models saved", path)
      ```

    - 使用 stablebaseline3

      - 训练
        ```python
        env = gym.make("Pendulum-v1")
        
        action_dim = env.action_space.shape[-1]
        
        action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(action_dim))
        
        model = DDPG(
            "MlpPolicy", # 网络类型
            env,
            verbose=0, # 训练日志
            learning_rate=1e-3,
            buffer_size=20000,
            learning_starts=100,
            batch_size=128,
            tau=0.005,
            gamma=0.99,
            action_noise=action_noise, # DDPG使用动作噪声代替ε-greedy来探索环境
        )
        
        # 训练模型
        N_TIMESTEPS = 200000  # 总时间步数
        model.learn(total_timesteps=N_TIMESTEPS)
        
        # 保存模型
        model.save("models/ddpg_SB3") # 压缩包文件
        ```

      - 调用
        ```python
        from stable_baselines3 import DDPG
        from stable_baselines3.common.evaluation import evaluate_policy
        
        model = DDPG.load("models/ddpg_SB3")
        
        env = gym.make(id="Pendulum-v1")
        
        for episode in range(10):
            s = env.reset()[0]
            episode_reward = 0
            
            for i in range(200):
                a, _ = model.predict(s, deterministic=True) # 调用predict方法输出确定性策略
        
                s_, r, done, _, info = env.step(a)
        
                episode_reward += r
                s = s_
        
            print(f"Episode: {episode}, Reward: {episode_reward}")
        
        env = gym.make(id="Pendulum-v1")
        mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10, deterministic=True)
        ```

- 论文复现-DDPG

  - 问题：
    - SUMO 无法设置直接设置转向角度 [sumo 官方手册](https://sumo.dlr.de/docs/TraCI/Change_Vehicle_State.html#move_to_xy_0xb4)
    - 间接方法：MoveToXY()方法
      - 可以指定移动到的目标位置，以及移动到位置后车辆的角度
      - 但是！该方法为直接传送，而不是模拟行驶路线
      - 直接计算是否会发生碰撞？
      - 缩小模拟步长？

- Carla 尝试

  - ……

  