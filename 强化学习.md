[toc]

# 强化学习基础

- 强化学习与其他机械学习
  - 有监督学习：利用一组已知类别的训练样本调整分类器的参数，使习得的分类器能对未知样本进行分类或预测
  - 无监督学习：从无标注的数据中学习隐含的结构或模式
  - 强化学习：就是学习“做什么才能使数值化的收益信号最大化”，是机器通过与环境交互来实现目标的一种计算方法
    - 没有监督数据只有奖励（reward）
    - 信号奖励信号不一定是实时的，可能存在延迟
    - 时间是一个重要因素
    - 智能体（Agent）当前的动作（Action 影响后续接收到的数据
- 强化学习基本要素
  - 奖励（reward）
    - 奖励 $R_t$ 是一个反馈信号，是一个标量
    - 反应智能体在时间步 t 工作得如何
    - 智能体的工作就是 **最大化累计奖励**
    - **强化学习主要基于奖励假设（Reward Hypothesis）：所有问题的目标都可以被描述成最大化期望的累积奖励。**
  - 序列决策
    - 目标：选择一定的动作序列以最大化未来的总体奖励
    - 智能体的行为可能是一个很长的动作序列
    - 大多数时候奖励是延迟的
    - 宁愿牺牲即时（短期）奖励以获取更多的长期奖励
  - 智能体：强化学习系统的决策者或学习者，智能体的目标是通过与环境的交互学习一个策略，以最大化其获得的累计奖励。
    - 智能体在每个时间步 t 接收观测 $O_t$
    - 接收标量奖励信号 $R_t$
    - 执行动作 $A_t$
  - 环境：智能体与之交互的外部系统。环境根据智能体的动作改变其状态，并提供相应的奖励。
    - 接收动作 $A_t$
    - 产生观测 $O_{t+1}$ 产生标量奖励信号 $R_{t+1}$
    - 完全可观测环境
      - 完全可观测：智能体可以直接观察到全部环境状态 $O_t=S_t^e=S_t^a$（围棋）
      - 智能体状态 = 环境状态 = 信息状态
      - 正式地说，这是马尔科夫决策过程（MDP） 
    - 部分可观测环境
      - 部分可观测：智能体可以观测到环境的部分（打麻将、斗地主）
      - 智能体状态 不等于 环境状态
      - 正式地说，这是部分可观测马尔科夫决策过程（POMDP）
  - 状态
    - 历史是观测、行动和奖励的序列
      $H_t=O_1, R_1,A_1,O_2, R_2,A_2, \dots, O_{t-1}, R_{t-1},A_{t-1},O_t, R_t$
      - 根据这个历史可以决定接下来会发生什么
        - 智能体选择行动
        - 环境选择观测及奖励
    - 状态是一种用于确定接下来会发生的事情（行动、观察、奖励）的信息
      - **状态是关于历史的函数** $S_t =f(H_t)$​
    - 环境状态 $S_t^e$ 是环境的内部状态
      - 用来确定下一个观测 / 奖励
      - 环境状态通常对智能体是不可见的
      - 即使 $S_t^e$​可见，大都包含大量不相关的信息
    - 智能体状态 $S_t^a$ 是智能体内部对信息的表达
      - 包括智能体可以使用的、决定未来动作的所有信息
      - **智能体状态是强化学习算法使用的信息**
      - **智能体状态是历史的函数** $S_t^a =f(H_t)$​
    - 信息状态（马尔可夫状态）包含了历史上所有有用的信息
      - 马尔可夫状态 $S_t$ 具有 **马尔可夫性**，当且仅当 $\mathbb{P}[S_{t+1}|S_t]=\mathbb{P}[S_{t+1}|S_1,...,S_t]$
      - **给定当前时刻的状态，将来与历史无关** $H_{1:t}\to S_t\to H_{t+1:\infty}$​
- 智能体的组成
  - 强化学习智能体由下述三个组件中的一个或多个组成
    - 策略（Policy）：智能体的行为函数。输入为状态输出为行动决策
    - 价值函数（Value function）：评估每个状态或行动有多好
    - 模型（Model）：智能体对环境的表示，是智能体眼里的环境

  - 策略(Policy)是学习智能体在特定时间的行为方式
    - 是从状态到行动的映射
    - 确定性策略（Deterministic Polic Policy）：在确定性策略下，智能体在每一个状态下总是选择一个特定的动作。这意味着策略函数 π 是状态到动作的一个映射
      -  $a=\pi(s)$

    - 随机第略（Stochastic Policy）： 在随机策略下，智能体在每一个状态下选择动作是有概率的。这意味着策略函数 π 是一个状态到动作概率分布的映射。
      - $\pi(a|s)=P(A_t=a\mid S_t=s)$​

  - 价值函数（Value Function）：是对于未来累积奖励的预测
    - 用于评估在给定的策略下状态的好坏
    - 可用于选择动作 $V_\pi(s)=\mathbb{E}_\pi[R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\cdots|S_t=s]$，在时间步 t 处于状态 s 时，智能体按照策略 π 行动后，未来所有时间步 $t+1, t+2, t+3, \ldots$ 所获得奖励的期望值。
      - 期望值符号 $\mathbb{E}_\pi[\cdot]$ 的含义是在策略 $\pi$ 下的期望值，在计算时要考虑策略 $\pi$ 规定的每个动作选择的概率。
      - 奖励序列 $R_{t+1}, R_{t+2}, R_{t+3}, \ldots$ 表示智能体在时间步 $t+1, t+2, t+3, \ldots$ 时执行策略 $\pi$ 并进行动作选择后获得的即时奖励。
      - 折扣因子 $\gamma$ 是一个在 0 到 1 之间的数，用来折扣未来的奖励。折扣因子表示未来的奖励相对于当前的价值有多重要，$\gamma$ 越接近 1，未来的奖励越重要；如果 $\gamma = 0$，则只关心当前的即时奖励。
    - 在强化学习中，状态价值函数 $V_\pi(s)$ 是一个核心概念。它帮助智能体评估一个状态的“好坏”，即在这个状态下能期望获得的长期回报。这种评估是策略改进和优化的基础，因为智能体会倾向于选择那些能够最大化 $V_\pi(s)$​ 的状态和行动序列。
  - 模型（Model）：用于模拟环境的行为，建模环境的动态特性
    - 解决下述两个问题：
      - 状态转移概率：用来预测环境的下一个状态
        - $\mathcal{P}_{ss^{\prime}}^{a}=\mathbb{P}[S_{t+1}=s^{\prime}|S_{t}=s,A_{t}=a]$

      - 奖励：预测环境给出的下一个即时奖励
        - $\mathcal{R}_{s}^{a}=\mathbb{E}[R_{t+1}|S_{t}=s,A_{t}=a]$

    - 环境真实的运行机制通常不称为模型，而称为环境动力学
    - 模型并不能立即给我们一个好的策略

- 智能体的分类
  - 根据智能体策略的更新与学习方式分为
    - 基于价值函数的
      - 基于值函数的算法利用和环境产生的交互数据不断逼近一个真实的状态-动作值函数，并选取每一个状态下值函数最大的动作作为最优动作，并没有直接学习策略。
      - 基于值函数的算法的主要缺点是一般只能用于低维度的离散动作空间，难以扩展到高维度动作空间以及连续的动作空间下。

    - 基于直接策略搜索的算法
      - 基于策略的算法并不去拟合一个值函数，而是直接拟合一个随机策略，策略输出的是在不同状态下选择各个动作的概率。
      - 基于策略的算法既能在离散动作空间下使用，也能在连续动作空间下使用，具有更加广泛的适用场景，但是具有收敛到局部最优的缺点。
    - 基于执行者-评论者（Actor-Critic）算法
      - Actor-Critic 框架用基于策略的方式来训练演员（Actor），Actor 是智能体的策略，用基于值函数的方法来训练一个评论家（Critic）来给策略进行评估。
      - 这种方式结合了前两者的优势，相比之下会有更好的性能表现。
  - 根据智能体是否使用环境模型来预测当前状态与动作下 下一个状态与奖励分为
    - 基于模型的算法：基于模型的算法需要我们预先知道环境模型或者训练智能体去学习环境模型，当获得环境模型之后，可以使用动态规划等算法来选择最优
    - 无模型算法：在一些复杂任务中，环境模型往往很难构建与学习，而无模型的算法不需要对环境进行建模，可以通过对价值函数进行估计等方式进而学习一个最优策略。无模型的强化学习算法因为摆脱了对环境模型的依赖，因此相比于基于模型的算法适用范围更广泛。
  - 根据环境返回的回报函数是否已知
    - 正向强化学习算法：在正向强化学习中，回报函数是已知的或者是可以通过与环境交互来估计的。学习算法学习如何在环境中采取行动以获得最大化的长期回报。
    - 逆向强化学习算法：在逆向强化学习中，算法不是直接学习策略，而是试图理解专家是如何做出决策的，从而推断出一个隐含的回报函数。
  - 根据智能体与环境交互的策略与智能体优化的策略是否为同一策略分为
    - 离线策略 off-policy 算法
      - 利用一个独立的探索策略在环境中进行探索获得经验，并用经验来训练一个不同目标策略。探索策略与环境交互的经验可以存放在一个经验池中，并且不断重复利用，经验数据的利用率大大提升。
      - 在机器人和自动驾驶等对于数据利用率要求较高的领域，off-policy 算法具有更强的适用性。

    - 在线策略 on-policy 算法
      - on-policy 算法探索策略与优化的目标策略是相同的策略，只能利用当前策略或者与当前策略相近的策略获取的经验数据进行训练，而不能利用在之前训练过程中获取的经验
      - 这也使得 on-policy 算法的经验数据利用率大大降低，往往需要多个环境进行交互采样，对于计算资源要求更高。
      - 相比于 off-policy 算法，on-policy 算法具有更强的稳定性
- 强化学习问题
  - 序列决策中的两个基础问题：学习（Learning）与规划（Planning）
    - 强化学习
      - 环境初始未知
      - 智能体不断与环境交互
      - 智能体提升它的策略

    - 规划
      - 环境模型已知
      - 智能体根据 Model 进行计算（不进行额外的交互）
      - 智能体提升它的策略

  - 探索（Exploration）和利用（Exploitation）
    - 强化学习类似于一个试错的学习，智能体从其与环境的交互中发现一个好的策略，在试错的过程中不会损失太多奖励
    - 探索：探索会发现有关环境的更多信息，有选择地放弃某些奖励
    - 利用：利用已知信息来最大化回报，强调开发利用已有的信息
    - 探索和利用是决策时需要平衡的两个方面
  - 预测（Prediction）与控制（Control）
    - 预测：评估未来。策略已经给定。（评估）
    - 控制：最大化未来。找到最优的策略。（优化）

***

# 马尔可夫决策过程

- 前言
  - 马尔可夫决策过程（MDP）是强化学习问题在数学上的理想化形式
  - MDP 中的环境是完全可观测的
  - 几乎所有的强化学习问题都可以在数学上表示为马尔可夫决策过程
  
- 马尔可夫过程：
  - 马尔可夫性质：给定当前时刻的状态，将来与历史无关 $\mathbb{P}[S_{t+1}|S_t]=\mathbb{P}[S_{t+1}|S_1,...,S_t]$
  - 状态：状态是对过去的充分统计
  - 状态转移矩阵：
    - 对于马尔可夫状态 s 与其后继状态 s’ ，它们的状态转移概率定义为：
      $\mathcal{P}_{\mathrm{ss}^{\prime}}=\mathbb{P}[\mathcal{S}_{t+1}=s^{\prime}|\mathcal{S}_t=s]$
    - 状态转移矩阵 P 定义了马尔可夫状态 s 到其所有后继状态 s' 的转移概率
      $\mathcal{P} = \mathrm{from}\begin{bmatrix}\mathcal{P}_{11}&...&\mathcal{P}_{1n}\\\vdots&&\vdots\\\mathcal{P}_{n1}&...&\mathcal{P}_{nn}\end{bmatrix}$
      - 矩阵的每一行总和为 1。

  - 马尔可夫过程
    - 马尔可夫过程是一种无记忆的随机过程
    - 马尔可夫过程可以分为三类
      - 时间、状态都离散 的马尔科夫过程 (**马尔科夫链**)
      - 时间连续、状态离散 的马尔可夫过程（**连续时间的马尔科夫链**）
      - 时间、状态都连续 的马尔可夫过程义

    - 马尔可夫过程（马尔可夫链）由元组 (S, P) 构成
      - $\mathcal{S}$ 是有限状态的集合
      - $\mathcal{P}$ 是状态转移矩阵 $\mathcal{P}_{\mathrm{ss}^{\prime}}=\mathbb{P}[S_{t+1}=s^{\prime}|S_t=s]$

    - 示例
      ![image-20240827185507376](C:\Users\aa\AppData\Roaming\Typora\typora-user-images\image-20240827185507376.png)
      - 状态
        ![image-20240827185528172](C:\Users\aa\AppData\Roaming\Typora\typora-user-images\image-20240827185528172.png)
        - 示例中的 Sleep 状态是终止状态（方框表示）：当一个马尔可夫链进入终止状态后，它将停留在该状态，不再转移到其他状态。
      - 状态转移矩阵
        ![image-20240827185705524](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240827185705524.png)
    
  - 分幕
    - 从初始状态 $S_1=C_1$ 开始，我们可以从马尔可夫链中采样一些子序列，每个子序列又称为幕（Episodes）$S_1,S_2,... ,S_T$​
      - 示例
        ![image-20240827185744773](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240827185744773.png)
  
- 马尔可夫奖励过程：
  - 马尔可夫奖励过程（MRP）是具有价值的马尔可夫链
  - 一个 MRP 由元组 $(\mathcal{S},\mathcal{P},\mathcal{R},\gamma)$ 组成
    - $\mathcal{S},\mathcal{P}$​ ……
    - $\mathcal{S}$ 是有限状态的集合
    - $\mathcal{P}$ 是状态转移矩阵 $\mathcal{P}_{\mathrm{ss}^{\prime}}=\mathbb{P}[\mathcal{S}_{t+1}=s^{\prime}|\mathcal{S}_t=s]$
    - $\mathcal{R}_S$ 是奖励函数，$\mathcal{R}_s=\mathbb{E}[R_{t+1}|\mathcal{S}_t=s]$​
      - 示例
        ![image-20240827190907423](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240827190907423.png)
    - $\gamma$ 是折扣因子，$\gamma \in [0,1]$​
      - 回报（Return）：在一个马尔可夫奖励过程中，从 t 时刻的状态 $S_t$ 开始，直至终止状态时，所有奖励的衰减之和 $G_t$ 称为回报
        $G_{t}=R_{t+1}+\gamma R_{t+2}+\cdots=\sum_{k=0}^{\infty}\gamma^{k}R_{t+k+1}$​
      - 折扣因子的作用
        - 避免有环的马尔可夫过程计算收益时出现无限循环
        - 从金融投资回报的角度讲，即时奖励（immediate rewards）比延时奖励（delayed rewards）更吸引人
        - 动物/人类行为中都表现出对即时奖励的偏好
        - 可以表达未来的不确定性
        -  $\gamma \in [0,1]$，$\gamma = 0$ 只看眼前收益
  - 价值函数
    - 在马尔可夫奖励过程中，一个状态的 **期望回报** 被称为这个状态的价值函数
      $v(s)=\mathbb{E}[G_{t}|S_t=s]$
    - 价值函数 v(s)给出状态 s 的长期价值（long-term value）
    - 价值函数输入为某个状态，输出为这个状态的价值
  - 贝尔曼方程（求解价值函数）
    - 价值函数可以分解为两部分
      - 即使奖励 $R_{t+1}$
      - 后继状态的折扣值 $rv(S_{t+1})$
    - 贝尔曼方程
      - 递推形式：$v(s)=\mathbb{E}[R_{t+1}+\gamma v(\mathcal{S}_{t+1})|\mathcal{S}_t=s]$
      - 还可以写成：$v(s)=\mathcal{R}_s+\gamma\sum_{s^{\prime}\in\mathcal{S}}\mathcal{P}_{ss^{\prime}}v(s^{\prime})$
        - 可以写成矩阵形式：$\begin{bmatrix}v(1)\\\vdots\\v(n)\end{bmatrix}=\begin{bmatrix}\mathcal R_1\\\vdots\\\mathcal R_n\end{bmatrix}+\gamma\begin{bmatrix}\mathcal P_{11}&\cdots&\mathcal P_{1n}\\\vdots&&\vdots\\\mathcal P_{n1}&\cdots&\mathcal P_{nn}\end{bmatrix}\begin{bmatrix}v(1)\\\vdots\\v(n)\end{bmatrix}$
      - 贝尔曼方程是线性方程，可以直接求解
      - 对于 $|\mathcal{S}|$ 个状态，计算复杂度为 $O(|\mathcal{S}|^3)$ 
      - 直接求解仅适用于小型 MRP
      - 对于大型 MRP，有很多迭代方法，例如：
        - 动态规划（Dynamic programming）
        - 蒙特卡洛评估（Monte-Carlo evaluation）
        - 时序差分学习（Temporal-Difference learning）
  
- **马尔可夫决策过程：**
  - 马尔可夫决策过程（MDP）是具有决策的马尔可夫奖励过程（MRP）
  
  - 一个 MDP 由元组 $(\mathcal{S},\mathcal{A},\mathcal{P},\mathcal{R},\gamma)$ 
    - $\mathcal{S}$ 是有限状态的集合
    - $\mathcal{A}$​ 是有限动作的集合
      - 示例
        ![image-20240827203639865](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240827203639865.png)
    - $\mathcal{P}$ 是状态转移矩阵 $\mathcal{P}_{\mathrm{ss}^{\prime}}^a=\mathbb{P}[\mathcal{S}_{t+1}=s^{\prime}|\mathcal{S}_t=s,\mathcal{A}_t=a]$
    - $\mathcal{R}_S$ 是奖励函数，$\mathcal{R}_s^a=\mathbb{E}[R_{t+1}|\mathcal{S}_t=s,\mathcal{A}_t=a]$
    - $\gamma$ 是折扣因子，$\gamma \in [0,1]$​
  
  - 策略 π
  
    - 策略 π 是一个函数，表示输入状态为 s 的情况下采取动作 a 的概率
      $π(a|s)= P[\mathcal{A}_t = a|\mathcal{S}_t =s]$
  
    - 策略完全定义了智能体的行为
  
    - 在马尔可夫决策过程中，策略仅取决于当前状态（而不是历史记录）
  
  - 给定一个 MDP：$\mathcal{M}=(\mathcal{S},\mathcal{A},\mathcal{P},\mathcal{R},\gamma)$ 和一个策略 $\pi$​ 
  
    - 状态序列 $S_1, S_2, \dots$ 是一个马尔可夫随机过程 $<\mathcal{S},\mathcal{P}^{\pi}>$
    - 状态和奖励序列 $S_1,R_1,S_2, \dots$ 是一个马尔可夫奖励过程 $<\mathcal{S},\mathcal{P}^{\pi},\mathcal{R}^{\pi}, \gamma>$​
    - $\begin{aligned}&\mathcal{P}_{ss^{\prime}}^{\pi}=\sum_{a\in\mathcal{A}}\pi(a|s)\mathcal{P}_{ss^{\prime}}^{a}\\&\mathcal{R}_{s}^{\pi}=\sum_{a\in\mathcal{A}}\pi(a|s)\mathcal{R}_{s}^{a}\end{aligned}$
  
  - 价值函数
  
    - 状态价值函数（State-Value）
      - 在马尔可夫决策过程中，一个状态价值函数 V(s) 是从状态 s 出发，遵循策略 π 得到的期望回报。
        $v_{\pi}(s)=\mathbb{E}_{\pi}[G_{t}|S_{t}=s]$
    - 动作价值函数（Action-Value）
      - 在马尔可夫决策过程中，一个动作价值函数 $q_\pi(s,a)$ 是从状态 s 开始，遵循策略 π ，对当前状态 s 执行动作 a 得到的期望回报:
        $q_\pi(s,a)=\mathbb{E}_\pi[G_t|S_t=s,A_t=a]$​
  
  - **贝尔曼期望方程**
  
    - 状态价值函数：即时奖励 + 后继状态的折扣值
      $v(s)=\mathbb{E}[R_{t+1}+\gamma v(\mathcal{S}_{t+1})|\mathcal{S}_t=s]$
    - 动作价值函数：即时奖励 + 后继状态的折扣值
      $q_\pi(s,a)=\mathbb{E}_\pi[R_{t+1}+\gamma q_\pi(S_{t+1},A_{t+1})|S_t=s,A_t=a]$​
    - 状态价值函数和动作价值函数的关系
      ![image-20240828120045375](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240828120045375.png)
      - $v_\pi(s)=\sum_{a \in \mathcal{A}}\pi(a|s)q_\pi(s,a)$
        - 状态价值函数 $v_\pi(s)$ 是动作价值函数 $q_\pi(s, a)$ 依据策略 π 在状态 s 下选择每个动作的概率 $\pi(a|s)$ 的加权平均。换句话说，状态 s 的价值是所有可能动作的价值的期望值。
      - $q_\pi(s,a)=\mathcal{R}_s^a+\gamma\sum_{s^{\prime}\in\mathcal{S}}\mathcal{P}_{ss^{\prime}}^av_\pi(s^{\prime})$​
        - 动作价值函数 $q_\pi(s, a)$ 是当前即时奖励 $\mathcal{R}_s^a$ 和未来所有可能状态 s' 的折扣价值 $\gamma v_\pi(s')$ 的加权和。加权的依据是从状态 s 执行动作 a 转移到状态 s′ 的概率 $\mathcal{P}_{ss^{\prime}}^a$。
    - 两个函数关系相互代入得递推函数
      - 状态价值递推函数：$v_{\pi}(s)=\sum_{a\in A}\pi(a|s)\left(\mathcal{R}_{s}^{a}+\gamma\sum_{s'\in S}\mathcal{P}_{ss'}^{a}v_{\pi}(s')\right)$
      - 动作价值递推函数：$q_{\pi}(s,a)=\mathcal{R}_{s}^{a}+\gamma\sum_{s'\in S}\mathcal{P}_{ss'}^{a}\sum_{a'\in A}\pi(a'|s')q_{\pi}(s',a')$​
    - 在一个 MDP 和策略 π 确定时，**贝尔曼期望方程的解存在且唯一**
      - 因为它们形成了一个线性系统，能够通过矩阵求解或迭代方法收敛到唯一解。这些方程的线性结构和 MDP 中的折扣因子 $\gamma < 1$ 的性质保证了我们总是能求解出策略下每个状态的价值函数和每个状态下每个动作的动作价值函数。
  
  - 回溯图
  
  - 最优价值函数
  
    - 最优状态价值函数是所有策略产生的状态价值函数中，使状态 s 价值最大的函数：
      - $v_*(s)=\max_\pi v_\pi(s)$
    - 最优动作价值函数是指所有策略产生的动作价值函数中，使状态-行为 <s,α> 对价值最大的函数：
      - $q_*(s,a)=\max_\pi q_\pi(s,a)$
    - 最优价值函数明确了 MDP 的最优可能表现
    - 一旦最优价值函数知晓，则认为 MDP 已完成求解
  
  - 最优策略
  
    - 策略之前的偏序：当且仅当对于任意的状态 s 都有 $v_{\pi}(s) ≥ v_{\pi^{\prime}}(s)$​时，记做 π ≥ π'。
    - 最优策略：**在有限状态和动作的 MDP 中，至少存在一个策略不劣于其他所有的策略**，即 $\pi_{*}\geq\pi,\forall\pi $​，这就是最优策略。
    - 所有的最优策略具有相同的最优状态价值函数元 $v_{\pi_*}(s)=v_*(s)$
      - 无论最优策略 $\pi_*$ 是哪一种，只要策略是最优的，从状态 s 出发的期望累计奖励 $v_{\pi_*}(s)$​ 总是相同的。
      - 换句话说，在所有最优策略下，达到的状态价值是一样的。最优策略通过不同的路径或方式可能会做出不同的决策，但其带来的总期望回报 $v_*(s)$ 必然相同。
    - 所有的最优策略有相同的动作价值函数 $q_{\pi_*}(s,a)=q_*(s,a)$​
      - ……
  
  - 寻找最优策略
  
    - 可以通过最大化 $q_*(s,a)$ 来找到最佳策略
      $\pi_*(a|s)=\begin{cases}1&\quad\text{当}a=arg\max_{a\in A}q_*(s,a)\\0\end{cases}$
    - 任何 MDP 始终都有确定性的最佳策略
    - 如果我们知道 $q_*(s,a)$，我们将立即获得最优策略
  
  - **贝尔曼最优方程**
    ![image-20240827231731040](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240827231731040.png)
  
    - 根据“可以通过最大化 $q_*(s,a)$ 来找到最佳策略”，确定最佳策略时其他动作的概率被置为 0， ，代入贝尔曼期望方程得：
      - 贝尔曼最优状态价值方程 1 为：$v_*(s)=\max q_*(s,a)$
      - 贝尔曼最优动作价值方程 1 为：$q_{*}(s,a)=\mathcal{R}_{s}^{a}+\gamma\sum_{s^{\prime}\in\mathcal{S}}\mathcal{P}_{ss^{\prime}}^{a}v_{*}(s^{\prime})$​
    - 将两者的方程 1 相互代入得：
      ![image-20240827232707698](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240827232707698.png)
      - 贝尔曼最优状态价值方程为：$v_*(s) = \max_{a \in \mathcal{A}} \left( \mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a v_*(s') \right)$
      - 贝尔曼最优动作价值方程为：$q_*(s, a) = \mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a \max_{a' \in \mathcal{A}} q_*(s', a')$​
    - 求解贝尔曼最优方程
      - Bellman 最优方程是非线性的
      - 不能使用与策略优化相同的直接矩阵解决方案
      - 通过一些迭代方法来解决：
        - 价值迭代（Value Iteration）
        - 策略迭代（Policy Iteration）
        - Q 学习（Q-learning）
        - Sarsa


***

# 动态规划

- 动态规划
  - 动态规划（Dynamic programming, DP）是一类优化算法
  - 动态规划将待求解问题分解成若干子问题，先求解子问题，然后从这些子问题的解得到目标问题的解。
  - 核心特点：最优子结构-子问题的最优解是可以得到的）重复子问题子问题的解决方案可以存储和重用
- 动态规划与强化学习
  - 在完备的马尔可夫决策过程中，DP 可用于计算最优策略
  - 对于强化学习问题，传统的 DP 算法作用有限：
    - 完备的环境模型只是一个假设
    - 计算复杂度极高

  - 但是，DP 提供了必要的基础，所有其他方法都是对 DP 的近似
    - 降低计算复杂度
    - 减弱对环境模型完备性的假设
- 基于动态规划的强化学习
  - 策略迭代（Policy iteration）：使用贝尔曼期望方程，求解最优策略。包含两个核心步骤：
    - 策略评估（Policy evaluation）：输入 MDP $(\mathcal{S},\mathcal{A},\mathcal{P},\mathcal{R},\gamma)$ 和策略 π，输出价值函数 $v_{\pi}$
    - 策略提升（Policy Improvement）：输入 MDP $(\mathcal{S},\mathcal{A},\mathcal{P},\mathcal{R},\gamma)$，输出最优价值函数 $v_*$ 和最优策略 π 
  - 价值迭代（Value iteration）：使用贝尔曼最优方程，求解最优策略
- 策略迭代
  - 迭代策略评估
    - 问题：评估一个给定的策略 π，目标是计算在策略 π\piπ 下的每个状态 sss 的状态价值函数 $v_\pi(s)$。这被称为 **预测问题**，即在已知策略的情况下，估计每个状态的长期回报。
    - 解决方案：迭代应用贝尔曼期望方程进行回溯
      - $v_1→v_2→\dots →v_π$
        $\forall s\colon v_{k+1}(s)\leftarrow\mathbb{E}_\pi[R_{t+1}+\gamma v_k(S_{t+1})| S_t=s ]$​
      - 伪代码
        ![image-20240828141840313](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240828141840313.png)
    - 通过反复应用贝尔曼期望方程，迭代策略评估方法能够在每次迭代中将当前状态价值的估计值向真实的状态价值函数靠近。随着迭代次数 k 的增加，估计值逐步逼近策略 π 下的准确状态价值函数 $v_\pi(s)$​​。
    - 同步更新和异步更新
      - 同步更新：是指在每一轮迭代中，使用当前迭代计算出的所有新值同时更新所有状态的价值函数或动作价值函数。
      - 异步更新：是指在每一轮迭代中，不需要等到所有状态的价值函数更新完毕，而是根据更新顺序实时更新状态的价值函数或动作价值函数。
  - 如何获得最优策略？
    **通过交替进行策略评估和策略改进两个步骤，策略迭代方法逐步逼近最优策略**
    - 策略评估：迭代应用贝尔曼期望方程，计算当前策略 π 下所有状态的状态价值函数 $v_\pi(s)$。
    - 策略改进：基于当前的状态价值函数 $v_\pi(s)$，使用 **贪心策略**（greedy policy），选择能够最大化状态价值的动作 a 作为新的策略，更新策略以提高期望回报
      $\pi'=\mathrm{greedy}(v_\pi)$
      $\pi'(s)=\arg\max_{a\in\mathcal{A}}\left(\mathcal{R}_s^a+\gamma\sum_{s'\in\mathcal{S}}\mathcal{P}_{ss'}^av_\pi(s')\right)$​
    - 伪代码
      ![image-20240828145412998](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240828145412998.png)
    - 如果改进停止
      - $q_\pi\big(s,\pi'(s)\big)=\max_{a\in A}q_\pi(s,a)=q_\pi\big(s,\pi(s)\big)=v_\pi(s)$
      - 满足贝尔曼最优方程
        $v_\pi(s)=\max_{a\in A}q_\pi(s,a)$
      - 此时，对于所有的 $s\in S ,v_{\pi} (s)=v_{*} (s)$
      - 所以，π 是最优策略
- 价值迭代
  - 价值迭代与策略迭代
    - 可以理解为，价值迭代就是将策略迭代中策略评估的循环次数改为 1
    - 价值迭代：结合策略评估和改进，单一迭代更新
    - 策略迭代：交替进行策略评估和策略改进

  - 价值迭代
    - 迭代更新：
      - 在每一轮迭代中，更新所有状态的价值函数 v(s) 直到收敛。更新公式基于贝尔曼最优方程：
        $V(s) = \max_{a \in \mathcal{A}} \left( \mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a V(s') \right)$
      - 该公式表示，在状态 s 下，选择使未来回报最大的动作 a，并更新状态 s 的价值。
    - 提取最优策略：
      - 当价值函数 V(s) 收敛时，从价值函数中提取最优策略 $\pi_*(s)$，公式如下：
        $\pi(s)=\mathrm{~argmax}_a\left(\mathcal{R}_s^a+\gamma\sum_{s^{\prime}\in\mathcal{S}}\mathcal{P}_{ss^{\prime}}^a\vee(s^{\prime})\right)$
  - 伪代码
    <img src="https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240828151607945.png" alt="image-20240828151607945" style="zoom: 67%;" />
    - **算法收敛到最优状态价值函数时，得到了最优策略。**

- 总结
  | 问题 | 贝尔曼方程 | 算法 |
  | --- | --- | --- |
  | 预测 | 贝尔曼期望方程 | 迭代策略评估 |
  | 控制 | 贝尔曼期望方程 + 贪婪策略改进 | 策略迭代 |
  | 控制 | 贝尔曼最优方程 | 价值迭代 |

***

# 无模型预测与控制

-  上一章：用动态规划来进行规划，解决一个 **已知的 MDP**
-  本章：Model-free 方法，评估和优化一个 **未知 MDP** 的价值函数
-  无模型预测（评估）：估计一个未知 MDP 的价值函数
   -  蒙特卡罗强化学习（Monte-Carlo Reinforcement Learning）

      -  MC 方法可直接从 **完整的分幕**（episodes）的经验中学习
         -  **MC 只能应用于分幕的 MDPs 中（所有的 episodes 必须终止）**

      -  MC 是无模型（model-free）的算法：**不了解 MDP 转换/奖励**
      -  MC 使用最简单的想法：价值（value）= 平均回报（meanreturn）
      -  目标：在给定策略 π 下，从一系列 episodes 经验中学习价值函数 $V_π$
        $S_1,A_1,R_2,..,S_k \sim \pi$
      -  回顾：回报是带折扣总奖励：
        $Gt= Rt+1+Rt+2+..+T-1RT$
      -  回顾：价值函数是回报的期望
        $V_π(s)=\mathbb{E} [G_t|S_t=s]$
      -  **蒙特卡罗策略评估使用每个状态的平均回报来代替回报的期望**
      -  首次访问型（First-Visit）蒙特卡罗策略评估
         -  目标：评估状态 S
         -  每幕中，状态 s 第一次出现时，进行如下操作一次
            ![image-20240828162631518](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240828162631518.png)
         -  增加计数个数 N(s) ← N(s) + 1
         -  增加回报总和 S(s) ← S(s) + Gt
         -  价值由平均回报估算 V(s)= S(s) / N(s)
         -  根据大数定律：$V(s)\to V_{\pi}(s)\:as\:N(s)\to\infty $

      -  每次访问型（Every-Visit）蒙特卡罗策略评估
         -  目标：评估状态 S
         -  每幕中，状态 s 每出现一次时，进行如下操作一次
            ![image-20240828162809066](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240828162809066.png)
         -  增加计数个数 $N(s) \leftarrow N(s) + 1$
         -  增加回报总和 $S(s) \leftarrow S(s) + G_t$
         -  价值由平均回报估算 $V(s)= S(s) / N(s)$
         -  根据大数定律：$V(s)\to V_{\pi}(s)\:as\:N(s)\to\infty $​

      -  累进式均值更新： 均值 $\mu_{k}=\mu_{k-1}+\frac1k\left(x_k-\mu_{k-1}\right)$
        $\begin{aligned}
        \mu_{k}& =\frac1k\sum_{j = 1}^kx_j \\
        &=\frac1k\left(x_k+\sum_{j = 1}^{k-1}x_j\right) \\
        &=\frac1k\left(x_k+(k-1)\mu_{k-1}\right) \\
        &=\mu_{k-1}+\frac1k\left(x_k-\mu_{k-1}\right)
        \end{aligned}$​
      -  累进式蒙特卡罗更新
         -  在 episode $S_1,A_1,R_2...R_T$ 后逐步更新 $V(s)$
         -  对于每个具有回报 $G_t$ 的状态 $S_t$
           $\begin{aligned}&N(S_{t})\leftarrow N(S_{t})+1\\&V(S_{t})\leftarrow V(S_{t})+\frac{1}{N(S_{t})}(G_{t}-V(S_{t}))\end{aligned}$
         -  上面的计算方式适用于平稳环境。
         -  在非平稳问题中，环境（如奖励函数、状态转移函数等）和数据分布会随着时间而变化。这使得旧的经验可能不再有用，甚至可能是误导性的。
         -  因此，在非平稳问题中算法需要不断适应环境的变化，以维持或提高其表现。
           -  跟踪连续平均值（即忘掉旧 episodes），使用一个常数学习率 α 来对新经验给予更大的权重，同时逐渐淡化旧经验的影响。
             $V(S_t)\leftarrow V(S_t)+\alpha\left(G_t-V(S_t)\right)$

   -  **时序差分学习**（Temporal-Difference Learning）
      -  时序差分方法的核心是通过当前状态的估计值来更新状态值函数。这种方法 **利用了当前的估计值和下一步的估计值之间的差异**，称为 **TD 误差**，来调整当前状态的估计值。
      -  TD 方法可直接从经验中学习
      -  TD 是无模型的：不了解 MDP 转换/奖励
      -  TD 通过自举（Bootstrapping）从不完整的 episodes 中学习
         -  自举：猜测 episode 的结果，同时持续更新这个猜测

      -  蒙特卡罗学习 和 时序差分学习
         -  目标：根据策略 π 得到的经验学习价值函数 $v_π$
         -  增量式 every-visit 蒙特卡罗
           -  朝着 **实际回报 $G_t$ ** 的方向更新价值 $V(S_t)$
             $V(S_t)\leftarrow V(S_t)+\alpha\left(G_t-V(S_t)\right)$

         -  最简单的时序差分算法：TD(0)
           -  朝着 **估计回报** $R_{t+1}+\gamma V(S_{t+1})$ 的方向更新 $V(S_t)$
             $V(S_t)\leftarrow V(S_t)+\alpha\left(R_{t+1}+\gamma V(S_{t+1})-V(S_t)\right)$
           -  $R_{t+1}+\gamma V(S_{t+1})$ 被称为 TD target
           -  $\delta_{t}=R_{t+1}+\gamma V(S_{t+1})-V(S_{t})$​ 被称为 TD error

   -  MC 和 TD 的优点和缺点
     -  TD 可以在知道最终结果之前学习，在每一步之后在线学习
        -  MC 必须等到 episode 结束才能知道回报

     -  TD 可以在没有最终结果的情况下学习
        -  TD 可以从不完整的序列中学习
        -  MC 只能从完整序列中学习
        -  TD 在连续（非终止）环境中工作
        -  MC 仅适用于 episode（终止）环境

     -  偏差 / 方差的平衡
        - 回报 $G_t=R_{t+1}+\gamma R_{t+2}+…+\gamma^{T-1}R^{T}$ 是 vπ(St) 的无偏估计
        - 真实的 TDtarget $R_{t+1}+\gamma v_π(St+1)$ 是 $v_π(S_t)$ 的无偏估计
        - TD target $R_{t+1}+\gamma V(S_{t+1})$ 是 $v_π(S_t)$ 的有偏估计
        - TD target $R_{t+1}+\gamma V(S_{t+1})$ 的方差比回报 $G_t$ 低得多
          - 回报取决于一序列随机的动作、转移与奖励
          - TD target 取决于一个动作及其对应的转移与奖励
        - MC 具有高方差，零偏差
          - 良好的收敛性
          - 对初始值不太敏感
          - 容易理解和使用
        - TD 方差低，但存在偏差
          - 通常比 MC 更高效
          - TD(0) 收敛至 $v_\pi(S_t)$​ 对初始值更敏感
     -  马尔科夫性
        - **TD 利用了马尔科夫性**，通常在马尔可夫环境中效率更高
        - **MC 没有利用马尔科夫性**，通常在非马尔可夫环境中更有效

   -  MC、TD、DP 的比较
     ![image-20240829134327195](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240829134327195.png)
     -  MC
       ![image-20240828231809595](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240828231809595.png)
     -  TD
       ![image-20240828231822657](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240828231822657.png)
     -  DP
       ![image-20240828231859638](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240828231859638.png)
     -  Bootstrapping：更新涉及估计
        -  MC 不自举
        -  DP 自举
        -  TD 自举

     -  Sampling：更新采样
        -  MC 采样
        -  DP 不采样
        -  TD 采样

   -  TD (λ)
     -  n 步 TD
       -  让 TD target 看更多步未来的状态
         ![image-20240828232130995](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240828232130995.png)
       -  考虑 n 步回报，其中 n = 1, 2, ∞：
         $\begin{aligned}&n=1\quad(TD)\quad G_t^{(1)}=R_{t+1}+\gamma V(S_{t+1})\\&n=2\quad G_t^{(2)}=R_{t+1}+\gamma R_{t+2}+\gamma^2V(S_{t+2})\\&n=\infty\quad(MC)\quad G_t^{(\infty)}=R_{t+1}+\gamma R_{t+2}+...+\gamma^{T-1}R_T\end{aligned}$
       -  定义 n 步回报为
         $G_t^{(n)}=R_{t+1}+\gamma R_{t+2}+...+\gamma^{n-1}R_{t+n}+\gamma^nV(S_{t+n})$
       -  n 步时序差分算法：
         $V(S_t)\leftarrow V(S_t)+\alpha\left(G_t^{(n)}-V(S_t)\right)$​
       -  可以对不同 n 的求 n 步回报的的平均值，例如：
       -  平均 2 步回报和 4 步回报：$\begin{aligned}\frac{1}{2}G^{(2)}+\frac{1}{2}G^{(4)}\end{aligned}$​

     -  TD (λ)
        ![image-20240828233407807](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240828233407807.png)
        -  $G_t^\lambda$ 整合了所有的 n 步回报 $G_t^{(n)}$
        -  加和时，使用权重 $(1－\lambda )\lambda^{n-1}$
          $G_{t}^{\lambda}=(1-\lambda)\sum_{n=1}^{\infty}\lambda^{n-1}G_{t}^{(n)}$
        -  得到 $TD(\lambda)$
          $V(S_t)\leftarrow V(S_t)+\alpha\left(G_t^\lambda-V(S_t)\right)$​
        -  TD (0) 只考虑一步
        -  TD (1) 等价于 MC
-  无模型控制（优化）：优化一个未知 MDP 的价值函数
   -  对于大多数这些问题，会有下列之一的情况：
      -  MDP 模型是未知的，但可以采样得到经验
      -  MDP 模型是已知的，但过于复杂，过于繁琐
   -  无模型控制可以解决这些问题
   -  在轨 / 离轨学习
      -  在轨（On-policy）学习
         -  “在工作中学习”
         -  从 π 中得到的经验学习策略 π

      -  离轨（Off-policy）学习
         -  “站在巨人的肩膀上”
         -  从 μ 中得到的经验学习策略 π
   -  在轨蒙特卡罗控制（On-Policy Monte-Carlo Learning）
      -  回顾：策略迭代算法
         ![image-20240829140039401](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240829140039401.png)
         -  广义策略迭代与蒙特卡罗评估
            -  策略评估：蒙特卡罗策略评估，$V=V_π$？
            -  策略优化：贪婪策略优化？
               -  对 V(S) 的贪婪策略优化需要 MDP 模型（需要 $\mathcal{P}_{ss'}^a$ 已知）
                  ![image-20240829140331029](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240829140331029.png)

            -  对 Q(s, a) 的贪婪策略优化是无模型的
               $\pi'(s)=\underset{a\in\mathcal{A}}{\operatorname*{argmax}}Q(s,a)$

         -  基于动作价值函数的广义策略迭代
            ![image-20240829140602514](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240829140602514.png)
            -  策略评估：蒙特卡罗策略评估，$Q=q_π$​
               -  采样策略 π 的第 k 轮 episode: $\{S_1,A_1,R_2,\dots,S_T\}～π$​
               -  对于 episode 中的每个状态 $S_t$ 和动作 $A_t$
                  $\begin{aligned}&N(S_{t},A_{t})\leftarrow N(S_{t},A_{t})+1\\&Q(S_{t},A_{t})\leftarrow Q(S_{t},A_{t})+\frac{1}{N(S_{t},A_{t})}\left(G_{t}-Q(S_{t},A_{t})\right)\end{aligned}$

            -  策略优化：贪婪策略优化？
               $\pi'(s)=\underset{a\in\mathcal{A}}{\operatorname*{argmax}}Q(s,a)$
               -  贪婪策略优化的缺点：**缺乏探索**，贪婪策略总是选择看似最优的动作，这意味着它完全忽略了探索潜在更好选择的可能性。因此，贪婪策略容易陷入局部最优，而不能找到全局最优解。
               -  ε - Greedy 探索
                  -  确保持续探索的最简单想法
                  -  所有 m 个动作都以非零概率进行尝试。
                  -  平衡探索与利用的策略
                     $\pi(a|s)=\left\{\begin{array}{ll}\epsilon/m+1-\epsilon&\text{if}a^*=\underset{a\in\mathcal{A}}{\operatorname*{argmax}}Q(s,a)\\\epsilon/m&\text{otherwise}\end{array}\right.$
                     -  以 $1-\varepsilon $ 的概率选择贪婪动作
                     -  以 $\varepsilon $ 的概率随机选择动作

                  -  GLIE（greedy in the Limit with Infinite Exploration）
                     -  有限的时间里进行无限可能的探索(GLIE)
                        -  所有的状态-动作对都被探索了无数次 $\lim_{k\to\infty}N_k(s,a)=\infty $
                        -  策略趋同于贪婪的策略 $\lim_{k\to\infty}\pi_{k}(a|s)=\mathbf{1}(a=\underset{a^{\prime}\in\mathcal{A}}{\operatorname*{\arg\max}}Q_{k}(s,a^{\prime}))$

                     -  例如，如果 $ε_k=\frac{1}{k}$（k 为探索的 episode 数目），则 ε - Greedy 为 GLIE

         -  GLIE 蒙特卡罗控制
            -  策略评估：​
               -  采样策略 π 的第 k 轮 episode: $\{S_1,A_1,R_2,\dots,S_T\}～π$​
               -  对于 episode 中的每个状态 $S_t$ 和动作 $A_t$
                  $\begin{aligned}&N(S_{t},A_{t})\leftarrow N(S_{t},A_{t})+1\\&Q(S_{t},A_{t})\leftarrow Q(S_{t},A_{t})+\frac{1}{N(S_{t},A_{t})}\left(G_{t}-Q(S_{t},A_{t})\right)\end{aligned}$
            -  策略优化：基于新的动作价值函数优化策略
               $\begin{aligned}&\epsilon\leftarrow1/k\\&\pi\leftarrow\epsilon\mathrm{-greedy}(Q)\end{aligned}$
   -  在轨时序差分学习
      -  MC vs. TD 控制（与评估一致）
         -  与蒙特卡罗（MC）相比，时序差分（TD）学习有几个优点
            -  更低的方差
            -  在线
            -  不完整的序列

         -  自然的想法是：在我们的控制循环中使用 TD 而不是 MC
            -  将 TD 应用于 Q(S, A)
            -  使用 ε - Greedy 策略改进
            -  更新每一个时间步
      -  在轨策略控制中的 Sarsa 算法
         -  Sarsa（State-Action-Reward-State-Action）是一种基于 on-policy 策略的强化学习算法，用于求解马尔可夫决策过程（MDP）中的最优策略。Sarsa 算法的名字来源于它更新公式中的五元组：状态  - 动作 - 奖励 - 新状态 - 新动作。
         -  算法步骤
            -  初始化 Q 表：对于每个状态-动作对，初始化 Q 值表 Q(s, a)（通常初始化为零或小的随机值）。
            -  选择动作：根据当前的策略（如 ε-greedy 策略），在当前状态 s 下选择一个动作 a。
            -  执行动作，观察奖励和下一个状态：执行动作 a，获得奖励 r 并观察到下一个状态 s'。
            -  选择下一个动作：根据当前的策略，在新的状态 s′ 下选择一个新的动作 a′。
            -  更新 Q 值：使用以下更新公式更新 Q 值：
               -  $Q(S,A)\leftarrow Q(S,A)+\alpha\left(R+\gamma Q(S^{\prime},A^{\prime})-Q(S,A)\right)$
         -  伪代码
            ![image-20240831194615087](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240831194615087.png)
         -  使用 $\varepsilon-greedy$ 的 Sarsa 不一定会收敛到最优，只有满足一下条件时：
            -  任何时候的策略 $π(a|s)$ 符合 GLIE 特性
            -  步长系数 $α_t$ 满足：
               $\begin{aligned}&\sum_{t=1}^\infty\alpha_t=\infty\\&\sum_{t=1}^\infty\alpha_t^2<\infty\end{aligned}$
   -  **离轨学习 Q-learning**
      -  离轨学习
         -  **目标策略：用来学习的策略** $\pi $
         -  **行为策略：生成行动样本的策略** $\mu$
         -  评估目标策略 $π(a|s)$ 以计算 $V_{\pi}(s)$ 或 $q_π(S,α)$ 
         -  同时遵循行为策略 $\mu(a|s)$+
            ${S1,A, R2, ..,ST} \sim \mu$
         -  为什么这很重要？
            -  通过观察人类或其他智能体来学习
            -  重用从旧策略 $\pi_1,\pi_2,…,\pi_{t-1}$ 生成的经验
            -  在遵循探索性策略的同时学习最优策略
      -  Q-learning
         -  现在考虑基于动作价值 Q(s, a) 的离轨学习
         -  使用行为策略 $A_t~μ(·|S_t)$ 选择下一个动作，产生 $R_{t+1}$ 与 $S_{t+1}$
         -  考虑基于替代策略 π 的后续动作 $A'～π(·|S_t)$
         -  并将 $Q(S_t, A_t)$ 更新为替代策略动作的价值
            $Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\alpha\left(R_{t+1}+\gamma Q(S_{t+1},A')-Q(S_t,A_t)\right)$
         -  **目标策略 π 是贪婪的 $w. r. t. Q(s,a)$ （找到一个最优的策略）**
            **$\pi(S_{t+1})=\arg\max_{a^{\prime}}Q(S_{t+1},a^{\prime})$**
         -  **行为策略 μ 是 $\varepsilon-greedy$ 的 $w.r.t.Q(s,a)$​​（以探索环境并收集经验数据）**
         -  Q-learning 一定可以收敛到最优策略
      -  Sarsa VS. Q-learning
         ![image-20240831202901553](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240831202901553.png)
         -  示例：悬崖行走
            ![image-20240831203059839](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240831203059839.png)
            -  训练过程中 Sarsa 收益和高、Q-学习收益和低
            -  Q-学习走近路，Sarsa 走安全的远路
   -  DP 和 TD 的关系
      ![image-20240831203536180](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240831203536180.png)
      ![image-20240831203602991](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240831203602991.png)



***

# 价值函数近似（DQN）

- 大规模强化学习
  - 我们希望用强化学习来解决一些大型问题，例如：
    - Backgammon：10^20 个状态
    - Computer Go：10^170 个状态
    - Helicopter:  连续状态空间

  - 如何将强化学习应用到这类大型问题中，实现预测和控制呢？
  - 到目前为止，我们都是通过一个查找表来存储价值函数
    - 每个状态 s 都有一个对应的 V(s)
    - 或者每个 state-action 对(s, a)都有一个对应的 Q(s, a)

  - 大型 MDPs 的问题：
    - 状态或者行动太多，无法全部存储在内存中
    - 针对每一个状态学习得到价值也是一个很慢的过程

  - 解决大型 MDPs 的方法：
    - 用函数逼近来评估每个价值函数
      $\hat{v}(s,\mathbf{w})\approx v_{\pi}(s)\\\mathrm{or}\: \hat{q}(s,a,\mathbf{w})\approx q_{\pi}(s,a)$
    - 可以用已知状态学到的价值函数插值出未见过状态的价值
    - 用 MC 或 TD 学习来更新函数参数 W

- 价值函数逼近
  - 价值函数逼近的类型
    ![image-20240831210022808](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240831210022808.png)
  - 梯度下降
    - 假定 $J(\mathbf{w})$ 是一个可微的函数
    - 定义 $J(w)$ 的梯度如下：
      $\nabla_\mathbf{w}J(\mathbf{w})=\begin{pmatrix}\frac{\partial J(\mathbf{w})}{\partial\mathbf{w}_1}\\\vdots\\\frac{\partial J(\mathbf{w})}{\partial\mathbf{w}_n}\end{pmatrix}$
    - 为寻找 $J(\mathbf{w})$ 的局部最小值
    - 朝着负梯度方向调整参数 w
      $\Delta w =-\frac12\alpha\nabla_\mathbf{w}J(\mathbf{w})$
      - α 是一个步长参数
  - 用随机梯度下降进行价值函数逼近
    - 目标：找到参数向量 w，最小化近似价值函数 $\hat{v}(S,w)$ 与真实价值函数 $v_π(s)$ 的均方差 $J(\mathbf{w})=\mathbb{E}_\pi[(v_π(S)-\hat{v}(S,w))^2]$
    - 通过梯度下降能够找到局部最小值
      $\begin{aligned}
      \Delta w& =-\frac12\alpha\nabla_\mathbf{w}J(\mathbf{w}) \\
      &=\alpha\mathbb{E}_\pi\left [(v_\pi(S)-\hat{v}(S,\mathbf{w}))\nabla_\mathbf{w}\hat{v}(S,\mathbf{w})\right]
      \end{aligned}$
    - 使用随机梯度下降对梯度进行采样
      $\Delta\mathbf{w}=\alpha(v_{\pi}(S)-\hat{v}(S,\mathbf{w}))\nabla_{\mathbf{w}}\hat{v}(S,\mathbf{w})$
    - 期望更新等于全部梯度更新
  - 特征向量
    - 用特征向量表示状态
      $\mathbf{x}(S)=\begin{pmatrix}\mathbf{x}_1(S)\\ \vdots \\\mathbf{x}_n(S)\end{pmatrix}$
    - 以直升机控制问题为例：
      - 3D 位置
      - 3D 速度(位置的变化量)
      - 3D 加速度(速度的变化量)
  - 线性价值函数逼近
    - 通过特征的线性组合表示值函数
      $\hat{v}(S,\mathbf{w})=\mathbf{x}(S)^\top\mathbf{w}=\sum_{j=1}^n\mathbf{x}_j(S)\mathbf{w}_j$
    - 参数为 w 的目标函数是二次函数
      $J(\mathbf{w})=\mathbb{E}_\pi\left[(v_\pi(S)-\mathbf{x}(S)^\top\mathbf{w})^2\right]$
    - 随机梯度下降收敛于全局最优
    - 更新规则：更新 = 步长 × 预测误差 × 特征
      $\begin{aligned}\nabla_{\mathbf{w}}\hat{v}(S,\mathbf{w})&=\mathbf{x}(S)\\\Delta\mathbf{w}&=\alpha(v_{\pi}(S)-\hat{v}(S,\mathbf{w}))\mathbf{x}(S)\end{aligned}$
  - 查表法
    - 表格法是线性值函数逼近的一种特殊情况
    - 使用表查找特征
      $\mathbf{x}^{table}(S)=\begin{pmatrix}\mathbf{1}(S=s_1)\\\vdots\\\mathbf{1}(S=s_n)\end{pmatrix}$
    - 参数向量 w 给出每个状态的值
      $\hat{v}(S,\mathbf{w})=\begin{pmatrix}\mathbf{1}(S=s_{1})\\\vdots\\\mathbf{1}(S=s_{n})\end{pmatrix}\cdot\begin{pmatrix}\mathbf{w}_{1}\\\vdots\\\mathbf{w}_{n}\end{pmatrix}$

- 增量式预测方法
  -  给定了真正的值函数 $V_π(s)$，该问题可建模为一个典型的有监督学习问题
  -  但是在 RL 中没有监督，只有奖励
  -  实际计算时，使用 target 代替 $V_\pi(s)$
    - 在 MC 中，target 是回报 $G_t$
      $\Delta\mathbf{w}=\alpha(G_t-\hat{v}(S_t,\mathbf{w}))\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w})$
    - 在 TD(0)中，target 是 TD target
      $\Delta\mathbf{w}=\alpha(R_{t+1}+\gamma\hat{v}(S_{t+1},\mathbf{w})-\hat{v}(S_{t},\mathbf{w}))\nabla_{\mathbf{w}}\hat{v}(S_{t},\mathbf{w})$​
      - 半梯度更新方法，因为真值 y 中包含参数 w，最终更新结果不是最小二乘结果
  -  蒙特卡罗学习的价值函数逼近
    -  回报 Gt 是对真实价值 $V_\pi(S_t)$ 的无偏估计
    -  因此，可以采用监督学习的方式使用“训练数据”：
      $\langle S_1,G_1\rangle,\langle S_2,G_2\rangle,...,\langle S_T,G_T\rangle $
    -  例如，使用线性蒙特卡洛策略评估
      $\begin{aligned}\Delta\mathbf{w}&=\alpha(G_t-\hat{v}(S_t,\mathbf{w}))\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w})\\&=\alpha(G_t+\hat{v}(S_t,\mathbf{w}))\mathbf{x}(S_t)\end{aligned}$
    -  蒙特卡洛评估收敛到局部最优(即使使用非线性值函数逼近）
  -  TD 学习的价值函数逼近
    -  TD-target $R_{t+1}+\gamma\hat{V}(S_{t+1},w)$ 是对真实价值 $V_\pi (S_t)$ 的有偏采样
    -  仍然可以将监督学习应用于“训练数据”：
      $\langle S_{1},R_{2}+\gamma\hat{v}(S_{2},\mathbf{w})\rangle,\langle S_{2},R_{3}+\gamma\hat{v}(S_{3},\mathbf{w})\rangle,...,\langle S_{T-1},R_{T}\rangle $
    -  例如，使用线性 TD(0)
      $\begin{aligned}
      \Delta w& =\alpha(R+\gamma\hat{v}(S^{\prime},\mathbf{w})-\hat{v}(S,\mathbf{w}))\nabla_\mathbf{w}\hat{v}(S,\mathbf{w}) \\
      &=\alpha\delta\mathbf{x}(S)\:\:\:\delta\rightarrow TD\: error
      \end{aligned}$
    -  线性 TD(0)收敛到全局最优
  -  价值函数近似的控制
     ![image-20240903112150146](https://raw.githubusercontent.com/Duangboss/Typora-Image/main/img/image-20240903112150146.png)
     -  策略评估：近似策略评估，$\hat{q}(\cdot,\cdot,\mathbf{w})\approx q_\pi $
     -  策略优化：ε－greedy 策略优化
     -  Action-value 函数逼近
        -  近似 action-value 函数 $\hat{q}(S,A,\mathbf{w})\approx q_π(S, A)$
        -  最小化估计的动作价值函数 $\hat{q}(S,A,\mathbf{w})$ 与真实的动作价值函数 $q_π(S,A)$ 之间的均方误差
          $J(\mathbf{w})=\mathbb{E}_\pi\left[(q_\pi(S,A)-\hat{q}(S,A,\mathbf{w}))^2\right]$
        -  用随机梯度下降方法找到局部最小值：
          $\Delta w=-\frac12\alpha\nabla_{\mathbf{w}}J(\mathbf{w}) =(q_\pi(S,A)-\hat{q}(S,A,\mathbf{w}))\nabla_\mathbf{w}\hat{q}(S,A,\mathbf{w})$​
        -  线性 Action-Value 函数逼近
          -  状态行为可以用特征向量表示：
            $\mathbf{x}(S,A)=\begin{pmatrix}\mathbf{x}_1(S,A)\\\vdots\\\mathbf{x}_n(S,A)\end{pmatrix}$
          -  通过特征的线性组合表示动作价值函数
            $\hat{q}(S,A,\mathbf{w})=\mathbf{x}(S,A)^\top\mathbf{w}=\sum_{j=1}^n\mathbf{x}_j(S,A)\mathbf{w}_j$
          -  用随机梯度下降方法进行更新
            $\begin{aligned}
            \nabla_{\mathbf{w}}\hat{q}(S, A,\mathbf{w})& =\mathbf{x}(S, A) \\
            \Delta w& =\alpha(q_\pi(S, A)-\hat{q}(S, A,\mathbf{w}))\mathbf{x}(S, A) 
            \end{aligned}$

- 批量方法
  - 批量强化学习
    - 梯度下降很简单，但是样本使用效率不高
    - 批量方法找寻满足这批数据的最佳价值函数
    - 根据智能体的经验（“训练数据”）

  - 最小二乘预测
    - 假设存在一个价值函数的近似 $\hat{v}(s,\mathbf{w})\approx v_\pi$
    - 以及一段时期的包含 <状态、价值> 的经验 $\mathcal{D}$：
      $\mathcal{D}=\{\langle s_1,v_1^\pi\rangle,\langle s_2,v_2^\pi\rangle,...,\langle s_T,v_T^\pi\rangle\}$
    - 最小二乘算法要求找到参数 w，使得目标值为 v 和近似值(s, w）之间的平方和误差最小：
      $\begin{aligned}
      LS(\mathbf{w})& =\sum_{t = 1}^T(v_t^\pi-\hat{v}(s_t,\mathbf{w}))^2 \\
      &=\mathbb{E}_{\mathcal{D}}\left [(v^\pi-\hat{v}(s,\mathbf{w}))^2\right]
      \end{aligned}$​

  - 带有经验回放的随机梯度下降
    - 给出包含 <状态、价值> 的经验 $\mathcal{D}$：
      $\mathcal{D}=\{\langle s_1,v_1^\pi\rangle,\langle s_2,v_2^\pi\rangle,...,\langle s_T,v_T^\pi\rangle\}$​
    - Repeat：
      - 从经验中采样状态、价值
        $\langle s,v^\pi\rangle \sim \mathcal{D}$
      - 应用随机梯度下降更新
        $\Delta\mathbf{w}=\alpha(v^{\pi}-\hat{v}(s,\mathbf{w}))\nabla_{\mathbf{w}}\hat{v}(s,\mathbf{w})$
    - 收敛至针对这段经历最小平方差的参数：
      $\mathbf{w}^{\pi}=\underset{{\mathbf{w}}}{\operatorname*{\operatorname*{\operatorname*{argmin}}}}LS(\mathbf{w})$​

  - DQN (Deep Q-Network)

    - DQN 将深度学习与 Q-learning 结合，利用深度神经网络来近似状态-动作值函数（Q 函数）。这种方法能够在高维输入（如图像）下有效地学习策略，在 Atari 游戏中的表现尤为出色

    - DQN 的关键特性和技术：

      - Q-learning: DQN 基于 Q-learning 算法，该算法的核心是学习一个 Q 函数 $Q(s, a)$，表示在状态 $s$ 选择动作 $a$​ 后的预期回报。目标是使得这个 Q 函数逼近真实的 Q 值。
      - 经验回放：为了减少样本间的相关性和提高数据利用率，DQN 使用经验回放机制。训练过程中，代理在与环境交互时，会将其经历的（状态，动作，奖励，下一个状态）四元组存储在经验回放缓冲区中，然后从中随机抽取小批量样本用于更新神经网络参数。

      - 目标网络：DQN 使用一个目标网络来计算目标 Q 值$(r+\gamma\max_{a^{\prime}}Q(s^{\prime},a^{\prime};w_{i}^{-})$，这个目标网络的参数是定期从主 Q 网络复制而来。目标网络的存在减少了训练过程中 Q 值目标的快速变化，从而提高了训练的稳定性。

    - DQN步骤
      - 根据ε－greedy 执行行为$a_t$
      - 将经验以$(S_t,a_t,r_{t+1}, S_{t+1})$的形式存储到replay memery D
      - 从D中随机抽样一个mini-batch的经验$(s,a,r,s')$
      - 用旧的、固定的参数$\mathbf{w}^-$计算Q-learning target
      - 在Q-network和Q-learning targets之间优化MSE
        $\mathcal{L}_{i}(w_{i})=\mathbb{E}_{{s,a,r,s^{\prime}\sim\mathcal{D}_{i}}}\left[\left(r+\gamma\max_{a^{\prime}}Q(s^{\prime},a^{\prime};w_{i}^{-})-Q(s,a;w_{i})\right)^{2}\right]$
      - 使用随机梯度下降的方式更新参数。

***

# 策略梯度方法

- 

***

# 执行者/评论者方法

- 

***

# 连续动作空间的确定性策略（DDPG）

- 

***